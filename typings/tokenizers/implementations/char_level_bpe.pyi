from .. import AddedToken as AddedToken, Tokenizer as Tokenizer, decoders as decoders, pre_tokenizers as pre_tokenizers, trainers as trainers
from ..models import BPE as BPE
from ..normalizers import BertNormalizer as BertNormalizer, Lowercase as Lowercase, Sequence as Sequence, unicode_normalizer_from_str as unicode_normalizer_from_str
from .base_tokenizer import BaseTokenizer as BaseTokenizer
from typing import Iterator

class CharBPETokenizer(BaseTokenizer):
    def __init__(self, vocab: str | dict[str, int] | None = None, merges: str | list[tuple[str, str]] | None = None, unk_token: str | AddedToken = '<unk>', suffix: str = '</w>', dropout: float | None = None, lowercase: bool = False, unicode_normalizer: str | None = None, bert_normalizer: bool = True, split_on_whitespace_only: bool = False) -> None: ...
    @staticmethod
    def from_file(vocab_filename: str, merges_filename: str, **kwargs): ...
    def train(self, files: str | list[str], vocab_size: int = 30000, min_frequency: int = 2, special_tokens: list[str | AddedToken] = ['<unk>'], limit_alphabet: int = 1000, initial_alphabet: list[str] = [], suffix: str | None = '</w>', show_progress: bool = True): ...
    def train_from_iterator(self, iterator: Iterator[str] | Iterator[Iterator[str]], vocab_size: int = 30000, min_frequency: int = 2, special_tokens: list[str | AddedToken] = ['<unk>'], limit_alphabet: int = 1000, initial_alphabet: list[str] = [], suffix: str | None = '</w>', show_progress: bool = True, length: int | None = None): ...
