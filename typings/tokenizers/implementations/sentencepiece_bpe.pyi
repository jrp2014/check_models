from .base_tokenizer import BaseTokenizer as BaseTokenizer
from tokenizers import AddedToken as AddedToken, Tokenizer as Tokenizer, decoders as decoders, pre_tokenizers as pre_tokenizers, trainers as trainers
from tokenizers.models import BPE as BPE
from tokenizers.normalizers import NFKC as NFKC
from typing import Iterator

class SentencePieceBPETokenizer(BaseTokenizer):
    def __init__(self, vocab: str | dict[str, int] | None = None, merges: str | list[tuple[str, str]] | None = None, unk_token: str | AddedToken = '<unk>', replacement: str = 'â–', add_prefix_space: bool = True, dropout: float | None = None, fuse_unk: bool | None = False) -> None: ...
    @staticmethod
    def from_file(vocab_filename: str, merges_filename: str, **kwargs): ...
    def train(self, files: str | list[str], vocab_size: int = 30000, min_frequency: int = 2, special_tokens: list[str | AddedToken] = ['<unk>'], limit_alphabet: int = 1000, initial_alphabet: list[str] = [], show_progress: bool = True): ...
    def train_from_iterator(self, iterator: Iterator[str] | Iterator[Iterator[str]], vocab_size: int = 30000, min_frequency: int = 2, special_tokens: list[str | AddedToken] = ['<unk>'], limit_alphabet: int = 1000, initial_alphabet: list[str] = [], show_progress: bool = True, length: int | None = None): ...
