from .base_tokenizer import BaseTokenizer as BaseTokenizer
from tokenizers import AddedToken as AddedToken, Tokenizer as Tokenizer, decoders as decoders, pre_tokenizers as pre_tokenizers, processors as processors, trainers as trainers
from tokenizers.models import BPE as BPE
from tokenizers.normalizers import Lowercase as Lowercase, Sequence as Sequence, unicode_normalizer_from_str as unicode_normalizer_from_str
from typing import Iterator

class ByteLevelBPETokenizer(BaseTokenizer):
    def __init__(self, vocab: str | dict[str, int] | None = None, merges: str | list[tuple[str, str]] | None = None, add_prefix_space: bool = False, lowercase: bool = False, dropout: float | None = None, unicode_normalizer: str | None = None, continuing_subword_prefix: str | None = None, end_of_word_suffix: str | None = None, trim_offsets: bool = False) -> None: ...
    @staticmethod
    def from_file(vocab_filename: str, merges_filename: str, **kwargs): ...
    def train(self, files: str | list[str], vocab_size: int = 30000, min_frequency: int = 2, show_progress: bool = True, special_tokens: list[str | AddedToken] = []): ...
    def train_from_iterator(self, iterator: Iterator[str] | Iterator[Iterator[str]], vocab_size: int = 30000, min_frequency: int = 2, show_progress: bool = True, special_tokens: list[str | AddedToken] = [], length: int | None = None): ...
