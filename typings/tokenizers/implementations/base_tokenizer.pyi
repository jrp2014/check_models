from tokenizers import AddedToken as AddedToken, EncodeInput as EncodeInput, Encoding as Encoding, InputSequence as InputSequence, Tokenizer as Tokenizer
from tokenizers.decoders import Decoder as Decoder
from tokenizers.models import Model as Model
from tokenizers.normalizers import Normalizer as Normalizer
from tokenizers.pre_tokenizers import PreTokenizer as PreTokenizer
from tokenizers.processors import PostProcessor as PostProcessor

Offsets = tuple[int, int]

class BaseTokenizer:
    def __init__(self, tokenizer: Tokenizer, parameters=None) -> None: ...
    def num_special_tokens_to_add(self, is_pair: bool) -> int: ...
    def get_vocab(self, with_added_tokens: bool = True) -> dict[str, int]: ...
    def get_added_tokens_decoder(self) -> dict[int, AddedToken]: ...
    def get_vocab_size(self, with_added_tokens: bool = True) -> int: ...
    def enable_padding(self, direction: str | None = 'right', pad_to_multiple_of: int | None = None, pad_id: int | None = 0, pad_type_id: int | None = 0, pad_token: str | None = '[PAD]', length: int | None = None): ...
    def no_padding(self): ...
    @property
    def padding(self) -> dict | None: ...
    def enable_truncation(self, max_length: int, stride: int | None = 0, strategy: str | None = 'longest_first'): ...
    def no_truncation(self): ...
    @property
    def truncation(self) -> dict | None: ...
    def add_tokens(self, tokens: list[str | AddedToken]) -> int: ...
    def add_special_tokens(self, special_tokens: list[str | AddedToken]) -> int: ...
    def normalize(self, sequence: str) -> str: ...
    def encode(self, sequence: InputSequence, pair: InputSequence | None = None, is_pretokenized: bool = False, add_special_tokens: bool = True) -> Encoding: ...
    def encode_batch(self, inputs: list[EncodeInput], is_pretokenized: bool = False, add_special_tokens: bool = True) -> list[Encoding]: ...
    async def async_encode_batch(self, inputs: list[EncodeInput], is_pretokenized: bool = False, add_special_tokens: bool = True) -> list[Encoding]: ...
    async def async_encode_batch_fast(self, inputs: list[EncodeInput], is_pretokenized: bool = False, add_special_tokens: bool = True) -> list[Encoding]: ...
    def decode(self, ids: list[int], skip_special_tokens: bool | None = True) -> str: ...
    def decode_batch(self, sequences: list[list[int]], skip_special_tokens: bool | None = True) -> str: ...
    def token_to_id(self, token: str) -> int | None: ...
    def id_to_token(self, id: int) -> str | None: ...
    def save_model(self, directory: str, prefix: str | None = None): ...
    def save(self, path: str, pretty: bool = True): ...
    def to_str(self, pretty: bool = False): ...
    def post_process(self, encoding: Encoding, pair: Encoding | None = None, add_special_tokens: bool = True) -> Encoding: ...
    @property
    def model(self) -> Model: ...
    @model.setter
    def model(self, model: Model): ...
    @property
    def normalizer(self) -> Normalizer: ...
    @normalizer.setter
    def normalizer(self, normalizer: Normalizer): ...
    @property
    def pre_tokenizer(self) -> PreTokenizer: ...
    @pre_tokenizer.setter
    def pre_tokenizer(self, pre_tokenizer: PreTokenizer): ...
    @property
    def post_processor(self) -> PostProcessor: ...
    @post_processor.setter
    def post_processor(self, post_processor: PostProcessor): ...
    @property
    def decoder(self) -> Decoder: ...
    @decoder.setter
    def decoder(self, decoder: Decoder): ...
