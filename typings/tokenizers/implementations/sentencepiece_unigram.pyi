from .base_tokenizer import BaseTokenizer as BaseTokenizer
from tokenizers import AddedToken as AddedToken, Regex as Regex, Tokenizer as Tokenizer, decoders as decoders, normalizers as normalizers, pre_tokenizers as pre_tokenizers, trainers as trainers
from tokenizers.models import Unigram as Unigram
from typing import Iterator

class SentencePieceUnigramTokenizer(BaseTokenizer):
    def __init__(self, vocab: list[tuple[str, float]] | None = None, replacement: str = 'â–', add_prefix_space: bool = True) -> None: ...
    def train(self, files: str | list[str], vocab_size: int = 8000, show_progress: bool = True, special_tokens: list[str | AddedToken] | None = None, initial_alphabet: list[str] | None = None, unk_token: str | None = None): ...
    def train_from_iterator(self, iterator: Iterator[str] | Iterator[Iterator[str]], vocab_size: int = 8000, show_progress: bool = True, special_tokens: list[str | AddedToken] | None = None, initial_alphabet: list[str] | None = None, unk_token: str | None = None, length: int | None = None): ...
    @staticmethod
    def from_spm(filename: str): ...
