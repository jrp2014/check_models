import mlx.core as mx
import mlx.nn as nn
import numpy as np
from .models.base import BaseImageProcessor as BaseImageProcessor
from .tokenizer_utils import load_tokenizer as load_tokenizer
from .trainer import apply_lora_layers as apply_lora_layers
from _typeshed import Incomplete
from io import BytesIO
from pathlib import Path
from transformers import PreTrainedTokenizer as PreTrainedTokenizer, PreTrainedTokenizerFast as PreTrainedTokenizerFast
from typing import Any

MODEL_REMAPPING: Incomplete
MAX_FILE_SIZE_GB: int
MODEL_CONVERSION_DTYPES: Incomplete

def skip_multimodal_module(path: str) -> bool: ...
def get_model_and_args(config: dict): ...
def get_model_path(path_or_hf_repo: str, revision: str | None = None, force_download: bool = False) -> Path: ...
def load_model(model_path: Path, lazy: bool = False, **kwargs) -> nn.Module: ...
def sanitize_weights(model_obj, weights, config=None): ...
def update_module_configs(model_config, model_class, config, modules): ...
def load(path_or_hf_repo: str, adapter_path: str | None = None, lazy: bool = False, revision: str | None = None, **kwargs) -> tuple[nn.Module, PreTrainedTokenizer | PreTrainedTokenizerFast]: ...
def load_config(model_path: str | Path, **kwargs) -> dict: ...
def load_image_processor(model_path: str | Path, **kwargs) -> BaseImageProcessor: ...
def load_processor(model_path, add_detokenizer: bool = True, eos_token_ids=None, **kwargs) -> PreTrainedTokenizer | PreTrainedTokenizerFast: ...
def fetch_from_hub(model_path: Path, lazy: bool = False, **kwargs) -> tuple[nn.Module, dict, PreTrainedTokenizer]: ...
def make_shards(weights: dict, max_file_size_gb: int = ...) -> list: ...
def upload_to_hub(path: str, upload_repo: str, hf_path: str): ...
def apply_repetition_penalty(logits: mx.array, generated_tokens: Any, penalty: float): ...
def save_weights(save_path: str | Path, model: nn.Module, *, donate_weights: bool = False) -> None: ...
def save_config(config: dict, config_path: str | Path) -> None: ...
def load_image(image_source: str | Path | BytesIO, timeout: int = 10): ...
def resize_image(img, max_size): ...
def process_image(img, resize_shape, image_processor): ...
def resample_audio(audio: np.ndarray, orig_sr: int, target_sr: int) -> np.ndarray: ...
def load_audio(file: str, sr: int, timeout: int = 10): ...
def process_inputs(processor, prompts, images=None, audio=None, add_special_tokens: bool = False, return_tensors: str = 'mlx'): ...
def process_inputs_with_fallback(processor, prompts, images, audio, add_special_tokens: bool = False, return_tensors: str = 'mlx'): ...
def prepare_inputs(processor, images=None, audio=None, prompts=None, image_token_index=None, resize_shape=None, add_special_tokens: bool = False): ...

class StoppingCriteria:
    eos_token_ids: Incomplete
    tokenizer: Incomplete
    def __init__(self, eos_token_ids: list[int], tokenizer=None) -> None: ...
    def add_eos_token_ids(self, new_eos_token_ids: int | list[int] = None): ...
    def reset(self, eos_token_ids: list[int] = None): ...
    def __call__(self, input_ids: mx.array) -> bool: ...

def print_array_report(t: mx.array, label: str | None) -> dict: ...
