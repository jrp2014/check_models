"""
This type stub file was generated by pyright.
"""

import contextlib
import mlx.core as mx
import mlx.nn as nn
from dataclasses import dataclass
from io import BytesIO
from pathlib import Path
from typing import Any, Dict, Generator, List, Optional, Tuple, Union
from transformers import PreTrainedTokenizer, PreTrainedTokenizerFast
from .models.base import BaseImageProcessor

MODEL_REMAPPING = ...
MAX_FILE_SIZE_GB = ...
generation_stream = ...
@contextlib.contextmanager
def wired_limit(model: nn.Module, streams: Optional[List[mx.Stream]] = ...): # -> Generator[None, Any, None]:
    """
    A context manager to temporarily change the wired limit.

    Note, the wired limit should not be changed during an async eval.  If an
    async eval could be running pass in the streams to synchronize with prior
    to exiting the context manager.
    """
    ...

@dataclass
class GenerationResult:
    text: str
    token: Optional[int]
    logprobs: Optional[List[float]]
    prompt_tokens: int
    generation_tokens: int
    prompt_tps: float
    generation_tps: float
    peak_memory: float
    ...


def get_model_and_args(config: dict): # -> tuple[ModuleType, str | None]:
    """
    Retrieve the model object based on the configuration.

    Args:
        config (dict): The model configuration.

    Returns:
        A tuple containing the Model class and the ModelArgs class.
    """
    ...

def get_model_path(path_or_hf_repo: str, revision: Optional[str] = ...) -> Path:
    """
    Ensures the model is available locally. If the path does not exist locally,
    it is downloaded from the Hugging Face Hub.

    Args:
        path_or_hf_repo (str): The local path or Hugging Face repository ID of the model.
        revision (str, optional): A revision id which can be a branch name, a tag, or a commit hash.

    Returns:
        Path: The path to the model.
    """
    ...

def load_model(model_path: Path, lazy: bool = ..., **kwargs) -> nn.Module:
    """
    Load and initialize the model from a given path.

    Args:
        model_path (Path): The path to load the model from.
        lazy (bool): If False eval the model parameters to make sure they are
            loaded in memory before returning, otherwise they will be loaded
            when needed. Default: ``False``

    Returns:
        nn.Module: The loaded and initialized model.

    Raises:
        FileNotFoundError: If the weight files (.safetensors) are not found.
        ValueError: If the model class or args class are not found or cannot be instantiated.
    """
    ...

def sanitize_weights(model_obj, weights, config=...):
    """Helper function to sanitize weights if the model has a sanitize method"""
    ...

def update_module_configs(model_config, model_class, config, modules):
    """Updates configuration for model modules like text and vision modules.

    Args:
        model_config: The model configuration object that will be updated
        model_class: The model class containing component config classes
        config: Dictionary containing configuration parameters
        modules: List of module names to update configs for (e.g. ["text", "vision"])

    Returns:
        The updated model_config object
    """
    ...

def get_class_predicate(skip_vision, weights=...): # -> Callable[..., bool] | Callable[..., Any | bool] | Callable[..., Any | Literal[False]]:
    ...

def load(path_or_hf_repo: str, adapter_path: Optional[str] = ..., lazy: bool = ..., **kwargs) -> Tuple[nn.Module, Union[PreTrainedTokenizer, PreTrainedTokenizerFast]]:
    """
    Load the model and tokenizer from a given path or a huggingface repository.

    Args:
        path_or_hf_repo (Path): The path or the huggingface repository to load the model from.
        tokenizer_config (dict, optional): Configuration parameters specifically for the tokenizer.
            Defaults to an empty dictionary.
        adapter_path (str, optional): Path to the LoRA adapters. If provided, applies LoRA layers
            to the model. Default: ``None``.
        lazy (bool): If False eval the model parameters to make sure they are
            loaded in memory before returning, otherwise they will be loaded
            when needed. Default: ``False``
    Returns:
        Tuple[nn.Module, TokenizerWrapper]: A tuple containing the loaded model and tokenizer.

    Raises:
        FileNotFoundError: If config file or safetensors are not found.
        ValueError: If model class or args class are not found.
    """
    ...

def load_config(model_path: Union[str, Path], **kwargs) -> dict:
    """Load model configuration from a path or Hugging Face repo.

    Args:
        model_path: Local path or Hugging Face repo ID to load config from
        **kwargs: Additional keyword arguments to pass to the config loader

    Returns:
        dict: Model configuration

    Raises:
        FileNotFoundError: If config.json is not found at the path
    """
    ...

def load_image_processor(model_path: Union[str, Path], **kwargs) -> BaseImageProcessor:
    ...

def load_processor(model_path, add_detokenizer=..., eos_token_ids=..., **kwargs) -> Union[PreTrainedTokenizer, PreTrainedTokenizerFast]:
    ...

def fetch_from_hub(model_path: Path, lazy: bool = ..., **kwargs) -> Tuple[nn.Module, dict, PreTrainedTokenizer]:
    ...

def make_shards(weights: dict, max_file_size_gb: int = ...) -> list:
    """
    Splits the weights into smaller shards.

    Args:
        weights (dict): Model weights.
        max_file_size_gb (int): Maximum size of each shard in gigabytes.

    Returns:
        list: List of weight shards.
    """
    ...

def upload_to_hub(path: str, upload_repo: str, hf_path: str): # -> None:
    """
    Uploads the model to Hugging Face hub.

    Args:
        path (str): Local path to the model.
        upload_repo (str): Name of the HF repo to upload to.
        hf_path (str): Path to the original Hugging Face model.
    """
    ...

def get_model_path(path_or_hf_repo: str, revision: Optional[str] = ...) -> Path:
    """
    Ensures the model is available locally. If the path does not exist locally,
    it is downloaded from the Hugging Face Hub.

    Args:
        path_or_hf_repo (str): The local path or Hugging Face repository ID of the model.
        revision (str, optional): A revision id which can be a branch name, a tag, or a commit hash.

    Returns:
        Path: The path to the model.
    """
    ...

def apply_repetition_penalty(logits: mx.array, generated_tokens: Any, penalty: float):
    """
    Apply repetition penalty to specific logits based on the given context.

    Paper: https://arxiv.org/abs/1909.05858

    Args:
        logits (mx.array): The logits produced by the language model.
        generated_tokens (any): A list of N previous tokens.
        penalty (float): The repetition penalty factor to be applied.

    Returns:
        logits (mx.array): Logits with repetition penalty applied to generated tokens.
    """
    ...

def save_weights(save_path: Union[str, Path], weights: Dict[str, Any], *, donate_weights: bool = ...) -> None:
    """Save model weights into specified directory."""
    ...

def quantize_model(model: nn.Module, config: dict, q_group_size: int, q_bits: int, skip_vision: bool = ...) -> Tuple[dict, dict]:
    """
    Applies quantization to the model weights.

    Args:
        model (nn.Module): The model to be quantized.
        config (dict): Model configuration.
        q_group_size (int): Group size for quantization.
        q_bits (int): Bits per weight for quantization.
        skip_vision (bool): Whether to skip quantizing vision model weights.

    Returns:
        Tuple[dict, dict]: Tuple containing quantized weights and updated config.
    """
    ...

def save_config(config: dict, config_path: Union[str, Path]) -> None:
    """Save the model configuration to the ``config_path``.

    The final configuration will be sorted before saving for better readability.

    Args:
        config (dict): The model configuration.
        config_path (Union[str, Path]): Model configuration file path.
    """
    ...

def dequantize_model(model: nn.Module) -> nn.Module:
    """
    Dequantize the quantized linear layers in the model.

    Args:
        model (nn.Module): The model with quantized linear layers.

    Returns:
        nn.Module: The model with dequantized layers.
    """
    ...

def convert(hf_path: str, mlx_path: str = ..., quantize: bool = ..., q_group_size: int = ..., q_bits: int = ..., dtype: str = ..., upload_repo: str = ..., revision: Optional[str] = ..., dequantize: bool = ..., skip_vision: bool = ..., trust_remote_code: bool = ...): # -> None:
    ...

def load_image(image_source: Union[str, Path, BytesIO], timeout: int = ...): # -> Image:
    """
    Helper function to load an image from either a URL or file.
    """
    ...

def resize_image(img, max_size):
    ...

def process_image(img, resize_shape, image_processor): # -> Image:
    ...

def process_inputs(processor, images, prompts, return_tensors=...):
    ...

def process_inputs_with_fallback(processor, images, prompts, return_tensors=...):
    ...

def prepare_inputs(processor, images, prompts, image_token_index, resize_shape=...): # -> dict[Any, Any]:
    ...

def generate_step(input_ids: mx.array, model: nn.Module, pixel_values, mask, *, max_tokens: int = ..., temperature: float = ..., repetition_penalty: Optional[float] = ..., repetition_context_size: Optional[int] = ..., top_p: float = ..., logit_bias: Optional[Dict[int, float]] = ..., **kwargs) -> Generator[Tuple[mx.array, mx.array], None, None]:
    """
    A generator producing token ids based on the given prompt from the model.

    Args:
        prompt (mx.array): The input prompt.
        model (nn.Module): The model to use for generation.
        temperature (float): The temperature for sampling, if 0 the argmax is used.
          Default: ``0``.
        repetition_penalty (float, optional): The penalty factor for repeating
          tokens.
        repetition_context_size (int, optional): The number of tokens to
          consider for repetition penalty. Default: ``20``.
        top_p (float, optional): Nulceus sampling, higher means model considers
          more less likely words.
        logit_bias (dictionary, optional): Additive logit bias.

    Yields:
        Generator[Tuple[mx.array, mx.array], None, None]: A generator producing
          one token and a vector of log probabilities.
    """
    ...

class StoppingCriteria:
    def __init__(self, eos_token_ids: List[int], tokenizer=...) -> None:
        ...
    
    def add_eos_token_ids(self, new_eos_token_ids: Union[int, List[int]] = ...): # -> None:
        """
        Add new token IDs to the list of EOS token IDs.

        Args:
            new_eos_token_ids: Integer, string, or list of integers/strings representing token IDs to add.
                               If strings are provided, they will be converted to integers if possible.
        """
        ...
    
    def reset(self, eos_token_ids: List[int] = ...): # -> None:
        ...
    
    def __call__(self, input_ids: mx.array) -> bool:
        ...
    


def stream_generate(model: nn.Module, processor: PreTrainedTokenizer, prompt: str, image: Union[str, List[str]] = ..., **kwargs) -> Union[str, Generator[str, None, None]]:
    """
    A generator producing text based on the given prompt from the model.

    Args:
        prompt (mx.array): The input prompt.
        model (nn.Module): The model to use for generation.
        max_tokens (int): The ma
        kwargs: The remaining options get passed to :func:`generate_step`.
          See :func:`generate_step` for more details.

    Yields:
        Generator[Tuple[mx.array, mx.array]]: A generator producing text.
    """
    ...

def generate(model: nn.Module, processor: PreTrainedTokenizer, prompt: str, image: Union[str, List[str]] = ..., verbose: bool = ..., **kwargs) -> str:
    """
    Generate text from the model.

    Args:
       model (nn.Module): The language model.
       tokenizer (PreTrainedTokenizer): The tokenizer.
       prompt (str): The string prompt.
       temperature (float): The temperature for sampling (default 0).
       max_tokens (int): The maximum number of tokens (default 100).
       verbose (bool): If ``True``, print tokens and timing information
           (default ``False``).
       formatter (Optional[Callable]): A function which takes a token and a
           probability and displays it.
       repetition_penalty (float, optional): The penalty factor for repeating tokens.
       repetition_context_size (int, optional): The number of tokens to consider for repetition penalty.
    """
    ...

