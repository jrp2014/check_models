from _typeshed import Incomplete

REPLACEMENT_CHAR: str

class StreamingDetokenizer:
    def reset(self) -> None: ...
    def add_token(self, token, skip_special_token_ids: list[int] = []): ...
    def finalize(self) -> None: ...
    offset: Incomplete
    @property
    def last_segment(self): ...

class NaiveStreamingDetokenizer(StreamingDetokenizer):
    def __init__(self, tokenizer) -> None: ...
    offset: int
    def reset(self) -> None: ...
    def add_token(self, token, skip_special_token_ids: list[int] = []): ...
    def finalize(self) -> None: ...
    @property
    def text(self): ...
    @property
    def tokens(self): ...

class SPMStreamingDetokenizer(StreamingDetokenizer):
    trim_space: Incomplete
    tokenmap: Incomplete
    def __init__(self, tokenizer, trim_space: bool = True) -> None: ...
    offset: int
    text: str
    tokens: Incomplete
    def reset(self) -> None: ...
    def add_token(self, token, skip_special_token_ids: list[int] = []): ...
    def finalize(self) -> None: ...

class BPEStreamingDetokenizer(StreamingDetokenizer):
    trim_space: Incomplete
    tokenmap: Incomplete
    def __init__(self, tokenizer, trim_space: bool = False) -> None: ...
    offset: int
    text: str
    tokens: Incomplete
    def reset(self) -> None: ...
    def add_token(self, token, skip_special_token_ids: list[int] = []): ...
    def finalize(self) -> None: ...
    @classmethod
    def make_byte_decoder(cls) -> None: ...

class TokenizerWrapper:
    def __init__(self, tokenizer, detokenizer_class=...) -> None: ...
    def __getattr__(self, attr): ...

def load_tokenizer(model_path, return_tokenizer: bool = True, tokenizer_config_extra={}): ...
