"""
This type stub file was generated by pyright.
"""

from typing import List

REPLACEMENT_CHAR = ...
class StreamingDetokenizer:
    """The streaming detokenizer interface so that we can detokenize one token at a time.

    Example usage is as follows:

        detokenizer = ...

        # Reset the tokenizer state
        detokenizer.reset()

        for token in generate(...):
            detokenizer.add_token(token.item())

            # Contains the whole text so far. Some tokens may not be included
            # since it contains whole words usually.
            detokenizer.text

            # Contains the printable segment (usually a word) since the last
            # time it was accessed
            detokenizer.last_segment

            # Contains all the tokens added so far
            detokenizer.tokens

        # Make sure that we detokenize any remaining tokens
        detokenizer.finalize()

        # Now detokenizer.text should match tokenizer.decode(detokenizer.tokens)
    """
    __slots__ = ...
    def reset(self):
        ...
    
    def add_token(self, token, skip_special_token_ids: List[int] = ...):
        ...
    
    def finalize(self):
        ...
    
    @property
    def last_segment(self): # -> Literal['']:
        """Return the last segment of readable text since last time this property was accessed."""
        ...
    


class NaiveStreamingDetokenizer(StreamingDetokenizer):
    """NaiveStreamingDetokenizer relies on the underlying tokenizer
    implementation and should work with every tokenizer.

    Its complexity is O(T^2) where T is the longest line since it will
    repeatedly detokenize the same tokens until a new line is generated.
    """
    def __init__(self, tokenizer) -> None:
        ...
    
    def reset(self): # -> None:
        ...
    
    def add_token(self, token, skip_special_token_ids: List[int] = ...): # -> None:
        ...
    
    def finalize(self): # -> None:
        ...
    
    @property
    def text(self): # -> str:
        ...
    
    @property
    def tokens(self): # -> list[Any]:
        ...
    


class SPMStreamingDetokenizer(StreamingDetokenizer):
    """A streaming detokenizer for SPM models.

    It adds tokens to the text if the next token starts with the special SPM
    underscore which results in linear complexity.
    """
    def __init__(self, tokenizer, trim_space=...) -> None:
        ...
    
    def reset(self): # -> None:
        ...
    
    def add_token(self, token, skip_special_token_ids: List[int] = ...): # -> None:
        ...
    
    def finalize(self): # -> None:
        ...
    


class BPEStreamingDetokenizer(StreamingDetokenizer):
    """A streaming detokenizer for OpenAI style BPE models.

    It adds tokens to the text if the next token starts with a space similar to
    the SPM detokenizer.
    """
    _byte_decoder = ...
    def __init__(self, tokenizer, trim_space=...) -> None:
        ...
    
    def reset(self): # -> None:
        ...
    
    def add_token(self, token, skip_special_token_ids: List[int] = ...): # -> None:
        ...
    
    def finalize(self): # -> None:
        ...
    
    @classmethod
    def make_byte_decoder(cls): # -> None:
        """See https://github.com/openai/gpt-2/blob/master/src/encoder.py for the rationale."""
        ...
    


class TokenizerWrapper:
    """A wrapper that combines an HF tokenizer and a detokenizer.

    Accessing any attribute other than the ``detokenizer`` is forwarded to the
    huggingface tokenizer.
    """
    def __init__(self, tokenizer, detokenizer_class=...) -> None:
        ...
    
    def __getattr__(self, attr): # -> NaiveStreamingDetokenizer | Any:
        ...
    


def load_tokenizer(model_path, return_tokenizer=..., tokenizer_config_extra=...): # -> TokenizerWrapper | type[SPMStreamingDetokenizer] | partial[SPMStreamingDetokenizer] | type[BPEStreamingDetokenizer] | type[NaiveStreamingDetokenizer]:
    """Load a huggingface tokenizer and try to infer the type of streaming
    detokenizer to use.

    Note, to use a fast streaming tokenizer, pass a local file path rather than
    a Hugging Face repo ID.
    """
    ...

