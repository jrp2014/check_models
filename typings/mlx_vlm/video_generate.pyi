import numpy as np
from .generate import generate as generate
from .utils import load as load, load_image as load_image, process_inputs_with_fallback as process_inputs_with_fallback
from PIL import Image
from _typeshed import Incomplete

logger: Incomplete
IMAGE_FACTOR: int
MIN_PIXELS: Incomplete
MAX_PIXELS: Incomplete
MAX_RATIO: int
VIDEO_MIN_PIXELS: Incomplete
VIDEO_MAX_PIXELS: Incomplete
FRAME_FACTOR: int
FPS: float
FPS_MIN_FRAMES: int
FPS_MAX_FRAMES: int
VIDEO_TOTAL_PIXELS: Incomplete

def round_by_factor(number: int, factor: int) -> int: ...
def ceil_by_factor(number: int, factor: int) -> int: ...
def floor_by_factor(number: int, factor: int) -> int: ...
def smart_resize(height: int, width: int, factor: int = ..., min_pixels: int = ..., max_pixels: int = ...) -> tuple[int, int]: ...
def to_rgb(pil_image: Image.Image) -> Image.Image: ...
def fetch_image(ele: dict[str, str | Image.Image], size_factor: int = ...) -> Image.Image: ...
def smart_nframes(ele: dict, total_frames: int, video_fps: int | float) -> int: ...
def load_video(ele: dict) -> tuple[np.ndarray, float]: ...
def fetch_video(ele: dict, image_factor: int = ..., return_video_sample_fps: bool = False) -> np.ndarray | list[Image.Image]: ...
def extract_vision_info(conversations: list[dict] | list[list[dict]]) -> list[dict]: ...
def process_vision_info(conversations: list[dict] | list[list[dict]], return_video_kwargs: bool = False) -> tuple[list[Image.Image] | None, list[np.ndarray | list[Image.Image]] | None, dict | None]: ...

class VideoFrameExtractor:
    max_frames: Incomplete
    def __init__(self, max_frames: int = 50) -> None: ...
    def resize_and_center_crop(self, image: Image.Image, target_size: int) -> Image.Image: ...
    def extract_frames(self, video_path: str) -> list[Image.Image]: ...

def is_video_model(model): ...
def is_video_file(video_path: list[str]) -> bool: ...
def main() -> None: ...
