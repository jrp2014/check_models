import mlx.nn as nn
from .lora import LoRaLayer as LoRaLayer

def get_module_by_name(model, name): ...
def set_module_by_name(model, name, new_module) -> None: ...
def get_peft_model(model, linear_layers, rank: int = 10, alpha: float = 0.1, dropout: float = 0.1, freeze: bool = True, verbose: bool = True): ...
def freeze_model(model) -> None: ...
def find_all_linear_names(model): ...
def count_parameters(model): ...
def print_trainable_parameters(model): ...
def apply_lora_layers(model: nn.Module, adapter_path: str) -> nn.Module: ...
