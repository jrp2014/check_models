from ..base import BaseModelConfig as BaseModelConfig
from dataclasses import dataclass

@dataclass
class AudioConfig(BaseModelConfig):
    input_feat_size: int = ...
    hidden_size: int = ...
    conf_attention_chunk_size: int = ...
    conf_attention_context_left: int = ...
    conf_attention_context_right: int = ...
    conf_attention_invalid_logits_value: float = ...
    conf_attention_logit_cap: float = ...
    conf_num_attention_heads: int = ...
    conf_num_hidden_layers: int = ...
    conf_conv_kernel_size: int = ...
    conf_positional_bias_size: int = ...
    conf_reduction_factor: int = ...
    conf_residual_weight: float = ...
    sscp_conv_channel_size: tuple[int, int] = ...
    sscp_conv_group_norm_eps: float = ...
    sscp_conv_kernel_size: tuple[tuple[int, int], tuple[int, int]] = ...
    sscp_conv_stride_size: tuple[tuple[int, int], tuple[int, int]] = ...
    vocab_size: int = ...
    sscp_conv_eps: float = ...
    rms_norm_eps: float = ...
    gradient_clipping: float = ...
    vocab_offset: int = ...

@dataclass
class VisionConfig(BaseModelConfig):
    model_type: str = ...
    num_hidden_layers: int = ...
    hidden_size: int = ...
    intermediate_size: int = ...
    num_attention_heads: int = ...
    patch_size: int = ...
    image_size: int = ...
    num_channels: int = ...
    rms_norm_eps: float = ...
    vocab_size: int = ...
    vocab_offset: int = ...

@dataclass
class TextConfig(BaseModelConfig):
    model_type: str
    hidden_size: int
    num_hidden_layers: int
    intermediate_size: int
    num_attention_heads: int = ...
    head_dim: int = ...
    rms_norm_eps: float = ...
    vocab_size: int = ...
    vocab_size_per_layer_input: int = ...
    num_key_value_heads: int = ...
    laurel_rank: int = ...
    frac_shared_layers: float = ...
    altup_active_idx: int = ...
    pad_token_id: int = ...
    altup_num_inputs: int = ...
    altup_coef_clip: float | None = ...
    altup_correct_scale: bool = ...
    hidden_size_per_layer_input: int = ...
    rope_local_base_freq: float = ...
    rope_traditional: bool = ...
    rope_theta: float = ...
    query_pre_attn_scalar: float = ...
    sliding_window: int = ...
    rope_scaling: dict[str, float | list[float]] | None = ...
    mm_tokens_per_image: int = ...
    sliding_window_pattern: int = ...
    activation_sparsity_pattern: list[float] | None = ...
    final_logit_softcapping: float = ...
    query_rescale_scalar: float = ...
    num_kv_shared_layers: int = ...
    max_position_embeddings: int = ...
    attn_logit_softcapping: float = ...
    layer_types: list[str] = ...

@dataclass
class ModelConfig(BaseModelConfig):
    text_config: TextConfig
    vision_config: VisionConfig
    audio_config: AudioConfig
    model_type: str
    vocab_size: int = ...
    ignore_index: int = ...
    image_token_index: int = ...
    audio_token_id: int = ...
    image_token_id: int = ...
    hidden_size: int = ...
    pad_token_id: int = ...
    vision_soft_tokens_per_image: int = ...
    audio_soft_tokens_per_image: int = ...
    eos_token_id: list[int] | None = ...

@dataclass
class MultiQueryAttentionBlockConfig(BaseModelConfig):
    num_heads: int = ...
    kv_dim: int = ...
    kv_strides: int = ...
    mmqa_avg_pool_kv: bool = ...
    mmqa_dropout: float = ...
    mmqa_dw_kernel_size: int = ...
    is_multiscale: bool = ...

@dataclass
class UniversalInvertedResidualConfig(BaseModelConfig):
    start_dw_kernel_size: int = ...
    mid_dw_kernel_size: int = ...
    filters: int = ...
    strides: int = ...
    expand_ratio: float = ...
    is_multiscale: bool = ...

@dataclass
class EdgeResidualConfig(BaseModelConfig):
    kernel_size: int = ...
    filters: int = ...
    strides: int = ...
    expand_ratio: float = ...
    is_multiscale: bool = ...
