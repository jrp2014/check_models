{"model": "HuggingFaceTB/SmolVLM-Instruct", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 1437, "generation_tokens": 19, "generation_tps": 118.20147126768954, "peak_memory_gb": 5.487353354, "active_memory_gb": 4.184753889217973, "cache_memory_gb": 0.11166672594845295}}
{"model": "Qwen/Qwen3-VL-2B-Instruct", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["verbose", "bullets(54)"], "metrics": {"prompt_tokens": 16477, "generation_tokens": 500, "generation_tps": 70.29528136660697, "peak_memory_gb": 12.380306772, "active_memory_gb": 3.9632408153265715, "cache_memory_gb": 0.7020423728972673}}
{"model": "meta-llama/Llama-3.2-11B-Vision-Instruct", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 238, "generation_tokens": 189, "generation_tps": 3.7982453590512772, "peak_memory_gb": 25.16080423, "active_memory_gb": 19.875752087682486, "cache_memory_gb": 3.0080921668559313}}
{"model": "microsoft/Florence-2-large-ft", "success": false, "error_stage": "Model Error", "error_message": "Model loading failed: Missing 1 parameters: \nlanguage_model.lm_head.weight.", "quality_issues": [], "metrics": {}}
{"model": "microsoft/Phi-3.5-vision-instruct", "success": false, "error_stage": "Error", "error_message": "Model generation failed for microsoft/Phi-3.5-vision-instruct: Failed to process inputs with error: Phi3VProcessor.__call__() got an unexpected keyword argument 'padding_side'", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Apriel-1.5-15b-Thinker-6bit-MLX", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 3169, "generation_tokens": 500, "generation_tps": 35.311992472957684, "peak_memory_gb": 14.740799694, "active_memory_gb": 11.69638067111373, "cache_memory_gb": 0.1422637216746807}}
{"model": "mlx-community/Devstral-Small-2-24B-Instruct-2512-5bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 2345, "generation_tokens": 194, "generation_tps": 27.72672692649656, "peak_memory_gb": 21.704214869, "active_memory_gb": 15.871880184859037, "cache_memory_gb": 0.07900684885680676}}
{"model": "mlx-community/GLM-4.6V-Flash-6bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["formatting"], "metrics": {"prompt_tokens": 6382, "generation_tokens": 500, "generation_tps": 47.270568796954365, "peak_memory_gb": 12.765250922, "active_memory_gb": 8.77819937467575, "cache_memory_gb": 0.16765889152884483}}
{"model": "mlx-community/Idefics3-8B-Llama3-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["formatting"], "metrics": {"prompt_tokens": 2549, "generation_tokens": 364, "generation_tps": 29.033171847164358, "peak_memory_gb": 18.482057982, "active_memory_gb": 15.764423873275518, "cache_memory_gb": 0.06270590610802174}}
{"model": "mlx-community/InternVL3-14B-8bit", "success": false, "error_stage": "Error", "error_message": "Model loading failed: Received a InternVLImageProcessor for argument image_processor, but a ImageProcessingMixin was expected.", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Kimi-VL-A3B-Thinking-2506-bf16", "success": false, "error_stage": "Lib Version", "error_message": "Model loading failed: cannot import name '_validate_images_text_input_order' from 'transformers.processing_utils' (/opt/homebrew/Caskroom/miniconda/base/envs/mlx-vlm/lib/python3.13/site-packages/transformers/processing_utils.py)", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Kimi-VL-A3B-Thinking-8bit", "success": false, "error_stage": "Lib Version", "error_message": "Model loading failed: cannot import name '_validate_images_text_input_order' from 'transformers.processing_utils' (/opt/homebrew/Caskroom/miniconda/base/envs/mlx-vlm/lib/python3.13/site-packages/transformers/processing_utils.py)", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/LFM2-VL-1.6B-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 2036, "generation_tokens": 15, "generation_tps": 312.1017451007053, "peak_memory_gb": 52.70399446, "active_memory_gb": 1.9179614800959826, "cache_memory_gb": 0.07530060224235058}}
{"model": "mlx-community/Llama-3.2-11B-Vision-Instruct-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 237, "generation_tokens": 222, "generation_tps": 8.611093198053418, "peak_memory_gb": 15.000277319, "active_memory_gb": 10.569727189838886, "cache_memory_gb": 0.4426043052226305}}
{"model": "mlx-community/Ministral-3-3B-Instruct-2512-4bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 2910, "generation_tokens": 305, "generation_tps": 156.0967687131649, "peak_memory_gb": 7.413111024, "active_memory_gb": 2.562248768284917, "cache_memory_gb": 0.27531589940190315}}
{"model": "mlx-community/Molmo-7B-D-0924-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 1452, "generation_tokens": 187, "generation_tps": 44.55545140480264, "peak_memory_gb": 40.683170198, "active_memory_gb": 8.419287744909525, "cache_memory_gb": 0.050875457003712654}}
{"model": "mlx-community/Molmo-7B-D-0924-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 1452, "generation_tokens": 289, "generation_tps": 27.888667890046133, "peak_memory_gb": 47.498111352, "active_memory_gb": 14.943686550483108, "cache_memory_gb": 0.032658616080880165}}
{"model": "mlx-community/Phi-3.5-vision-instruct-bf16", "success": false, "error_stage": "Error", "error_message": "Model loading failed: /Users/jrp/.cache/huggingface/hub/models--mlx-community--Phi-3.5-vision-instruct-bf16/snapshots/d8da684308c275a86659e2b36a9189b2f4aec8ea does not appear to have a file named image_processing_phi3_v.py. Checkout 'https://huggingface.co//Users/jrp/.cache/huggingface/hub/models--mlx-community--Phi-3.5-vision-instruct-bf16/snapshots/d8da684308c275a86659e2b36a9189b2f4aec8ea/tree/main' for available files.", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Qwen2-VL-2B-Instruct-4bit", "success": false, "error_stage": "OOM", "error_message": "Model runtime error during generation for mlx-community/Qwen2-VL-2B-Instruct-4bit: [metal::malloc] Attempting to allocate 134767706112 bytes which is greater than the maximum allowed buffer size of 86586540032 bytes.", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Qwen3-VL-2B-Thinking-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 16479, "generation_tokens": 500, "generation_tps": 71.81459625357628, "peak_memory_gb": 12.38445196, "active_memory_gb": 3.966475712135434, "cache_memory_gb": 0.7020423132926226}}
{"model": "mlx-community/SmolVLM-Instruct-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 1437, "generation_tokens": 409, "generation_tps": 112.93930654708085, "peak_memory_gb": 5.524005039, "active_memory_gb": 4.188385529443622, "cache_memory_gb": 0.13896421901881695}}
{"model": "mlx-community/SmolVLM2-2.2B-Instruct-mlx", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 1337, "generation_tokens": 47, "generation_tps": 118.32176953523475, "peak_memory_gb": 5.483747164, "active_memory_gb": 4.189438385888934, "cache_memory_gb": 0.11166523583233356}}
{"model": "mlx-community/deepseek-vl2-8bit", "success": false, "error_stage": "Error", "error_message": "Model runtime error during generation for mlx-community/deepseek-vl2-8bit: std::bad_cast", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/gemma-3-12b-pt-8bit", "success": false, "error_stage": "Error", "error_message": "Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/gemma-3-27b-it-qat-4bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["bullets(21)"], "metrics": {"prompt_tokens": 508, "generation_tokens": 305, "generation_tps": 27.896688576530167, "peak_memory_gb": 19.383715786, "active_memory_gb": 15.685489945113659, "cache_memory_gb": 0.08380711078643799}}
{"model": "mlx-community/gemma-3-27b-it-qat-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["bullets(25)"], "metrics": {"prompt_tokens": 508, "generation_tokens": 360, "generation_tps": 15.392572873152405, "peak_memory_gb": 33.593051092, "active_memory_gb": 28.91896467655897, "cache_memory_gb": 0.04402640275657177}}
{"model": "mlx-community/gemma-3n-E4B-it-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 508, "generation_tokens": 230, "generation_tps": 41.58907391609894, "peak_memory_gb": 17.19849592, "active_memory_gb": 14.627118539065123, "cache_memory_gb": 0.013714537024497986}}
{"model": "mlx-community/llava-v1.6-mistral-7b-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 2447, "generation_tokens": 8, "generation_tps": 47.57081592093167, "peak_memory_gb": 11.945494792, "active_memory_gb": 7.49412739649415, "cache_memory_gb": 0.47387705370783806}}
{"model": "mlx-community/paligemma2-10b-ft-docci-448-6bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 1263, "generation_tokens": 87, "generation_tps": 41.27818450558566, "peak_memory_gb": 11.293720638, "active_memory_gb": 7.326100518926978, "cache_memory_gb": 0.487768879160285}}
{"model": "mlx-community/paligemma2-10b-ft-docci-448-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 1263, "generation_tokens": 128, "generation_tps": 4.844337595477522, "peak_memory_gb": 26.920024294, "active_memory_gb": 18.007738729938865, "cache_memory_gb": 4.775907088071108}}
{"model": "mlx-community/paligemma2-3b-ft-docci-448-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 1263, "generation_tokens": 128, "generation_tps": 17.577255070126693, "peak_memory_gb": 11.305319702, "active_memory_gb": 5.6586243119090796, "cache_memory_gb": 3.0543227940797806}}
{"model": "mlx-community/paligemma2-3b-pt-896-4bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 4335, "generation_tokens": 22, "generation_tps": 110.16352398527604, "peak_memory_gb": 8.601962802, "active_memory_gb": 1.608608415350318, "cache_memory_gb": 0.5266583058983088}}
{"model": "mlx-community/pixtral-12b-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["bullets(19)"], "metrics": {"prompt_tokens": 3071, "generation_tokens": 281, "generation_tps": 34.31962406325559, "peak_memory_gb": 15.47188667, "active_memory_gb": 12.561599906533957, "cache_memory_gb": 0.09218678809702396}}
{"model": "mlx-community/pixtral-12b-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["bullets(20)"], "metrics": {"prompt_tokens": 3071, "generation_tokens": 277, "generation_tps": 19.237859441154434, "peak_memory_gb": 27.361034638, "active_memory_gb": 23.634231742471457, "cache_memory_gb": 0.037048401311039925}}
{"model": "prince-canuma/Florence-2-large-ft", "success": false, "error_stage": "Error", "error_message": "Model loading failed: RobertaTokenizer has no attribute additional_special_tokens", "quality_issues": [], "metrics": {}}
{"model": "qnguyen3/nanoLLaVA", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 260, "generation_tokens": 65, "generation_tps": 101.09175864711406, "peak_memory_gb": 4.47636775, "active_memory_gb": 1.967399774119258, "cache_memory_gb": 1.382887527346611}}
