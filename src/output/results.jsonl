{"model": "HuggingFaceTB/SmolVLM-Instruct", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 1442, "generation_tokens": 164, "generation_tps": 116.34786530877084, "peak_memory_gb": 5.546143432, "active_memory_gb": 4.184753889217973, "cache_memory_gb": 0.13254034332931042}}
{"model": "Qwen/Qwen3-VL-2B-Instruct", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["verbose", "bullets(66)"], "metrics": {"prompt_tokens": 16519, "generation_tokens": 500, "generation_tps": 71.56026446828585, "peak_memory_gb": 12.599082336, "active_memory_gb": 3.9632408265024424, "cache_memory_gb": 0.7030150014907122}}
{"model": "meta-llama/Llama-3.2-11B-Vision-Instruct", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 240, "generation_tokens": 202, "generation_tps": 3.749533825768683, "peak_memory_gb": 25.161820134, "active_memory_gb": 19.875752087682486, "cache_memory_gb": 3.0080922562628984}}
{"model": "microsoft/Florence-2-large-ft", "success": false, "error_stage": "Model Error", "error_message": "Model loading failed: Missing 1 parameters: \nlanguage_model.lm_head.weight.", "quality_issues": [], "metrics": {}}
{"model": "microsoft/Phi-3.5-vision-instruct", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["refusal(uncertainty)", "lang_mixing"], "metrics": {"prompt_tokens": 1054, "generation_tokens": 500, "generation_tps": 10.966783798910505, "peak_memory_gb": 11.50677336, "active_memory_gb": 7.724683305248618, "cache_memory_gb": 1.3939073774963617}}
{"model": "mlx-community/Apriel-1.5-15b-Thinker-6bit-MLX", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["hallucination"], "metrics": {"prompt_tokens": 3271, "generation_tokens": 500, "generation_tps": 35.72846236480137, "peak_memory_gb": 14.850490578, "active_memory_gb": 11.69644170999527, "cache_memory_gb": 0.14292890671640635}}
{"model": "mlx-community/Idefics3-8B-Llama3-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["formatting"], "metrics": {"prompt_tokens": 2550, "generation_tokens": 395, "generation_tps": 29.301471012741274, "peak_memory_gb": 18.520183522, "active_memory_gb": 15.763950854539871, "cache_memory_gb": 0.06920356303453445}}
{"model": "mlx-community/InternVL3-14B-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 2045, "generation_tokens": 158, "generation_tps": 29.2119039713503, "peak_memory_gb": 17.873930594, "active_memory_gb": 15.226991884410381, "cache_memory_gb": 0.08001143299043179}}
{"model": "mlx-community/Kimi-VL-A3B-Thinking-2506-bf16", "success": false, "error_stage": "Lib Version", "error_message": "Model loading failed: cannot import name '_validate_images_text_input_order' from 'transformers.processing_utils' (/opt/homebrew/Caskroom/miniconda/base/envs/mlx-vlm/lib/python3.13/site-packages/transformers/processing_utils.py)", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Kimi-VL-A3B-Thinking-8bit", "success": false, "error_stage": "Lib Version", "error_message": "Model loading failed: cannot import name '_validate_images_text_input_order' from 'transformers.processing_utils' (/opt/homebrew/Caskroom/miniconda/base/envs/mlx-vlm/lib/python3.13/site-packages/transformers/processing_utils.py)", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/LFM2-VL-1.6B-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 2042, "generation_tokens": 24, "generation_tps": 307.737351711573, "peak_memory_gb": 21.206139964, "active_memory_gb": 18.52197322808206, "cache_memory_gb": 0.07549235969781876}}
{"model": "mlx-community/Llama-3.2-11B-Vision-Instruct-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 239, "generation_tokens": 207, "generation_tps": 8.88667941441939, "peak_memory_gb": 17.830612142, "active_memory_gb": 10.569638829678297, "cache_memory_gb": 0.4426251519471407}}
{"model": "mlx-community/Molmo-7B-D-0924-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 1446, "generation_tokens": 182, "generation_tps": 44.77538180578599, "peak_memory_gb": 40.67979505, "active_memory_gb": 8.418860502541065, "cache_memory_gb": 0.05087541230022907}}
{"model": "mlx-community/Molmo-7B-D-0924-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 1446, "generation_tokens": 223, "generation_tps": 28.271191742627128, "peak_memory_gb": 47.49553902, "active_memory_gb": 14.943259308114648, "cache_memory_gb": 0.03265886940062046}}
{"model": "mlx-community/Phi-3.5-vision-instruct-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 1054, "generation_tokens": 184, "generation_tps": 11.005297901882871, "peak_memory_gb": 11.096434472, "active_memory_gb": 7.72669274546206, "cache_memory_gb": 1.370496580377221}}
{"model": "mlx-community/Qwen2-VL-2B-Instruct-4bit", "success": false, "error_stage": "OOM", "error_message": "Model runtime error during generation for mlx-community/Qwen2-VL-2B-Instruct-4bit: [metal::malloc] Attempting to allocate 135433060352 bytes which is greater than the maximum allowed buffer size of 86586540032 bytes.", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Qwen3-VL-2B-Thinking-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 16521, "generation_tokens": 500, "generation_tps": 71.52228647858772, "peak_memory_gb": 12.602916232, "active_memory_gb": 3.9661095160990953, "cache_memory_gb": 0.7030150834470987}}
{"model": "mlx-community/SmolVLM-Instruct-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 1442, "generation_tokens": 152, "generation_tps": 116.8137924061855, "peak_memory_gb": 5.549649664, "active_memory_gb": 4.188019322231412, "cache_memory_gb": 0.13254034332931042}}
{"model": "mlx-community/SmolVLM2-2.2B-Instruct-mlx", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 1342, "generation_tokens": 20, "generation_tps": 123.73095187013048, "peak_memory_gb": 5.586770196, "active_memory_gb": 4.189072178676724, "cache_memory_gb": 0.11892849393188953}}
{"model": "mlx-community/deepseek-vl2-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 2253, "generation_tokens": 7, "generation_tps": 70.4474887426563, "peak_memory_gb": 32.140585012, "active_memory_gb": 27.32607844658196, "cache_memory_gb": 0.13865155167877674}}
{"model": "mlx-community/gemma-3-12b-pt-8bit", "success": false, "error_stage": "Error", "error_message": "Cannot use apply_chat_template because this processor does not have a chat template.", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/gemma-3-27b-it-qat-4bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["bullets(27)"], "metrics": {"prompt_tokens": 513, "generation_tokens": 319, "generation_tps": 24.52471402133351, "peak_memory_gb": 18.111088598, "active_memory_gb": 15.028950031846762, "cache_memory_gb": 0.08443283289670944}}
{"model": "mlx-community/gemma-3-27b-it-qat-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["bullets(63)"], "metrics": {"prompt_tokens": 513, "generation_tokens": 500, "generation_tps": 14.404581060250273, "peak_memory_gb": 31.644158006, "active_memory_gb": 27.60602217540145, "cache_memory_gb": 0.04434933327138424}}
{"model": "mlx-community/gemma-3n-E4B-it-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 513, "generation_tokens": 287, "generation_tps": 41.966905254472046, "peak_memory_gb": 17.773060633, "active_memory_gb": 14.626996003091335, "cache_memory_gb": 0.014902211725711823}}
{"model": "mlx-community/llava-v1.6-mistral-7b-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 2531, "generation_tokens": 15, "generation_tps": 44.421712640027394, "peak_memory_gb": 12.293015816, "active_memory_gb": 7.49400532618165, "cache_memory_gb": 0.47430429980158806}}
{"model": "mlx-community/paligemma2-10b-ft-docci-448-6bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 1266, "generation_tokens": 143, "generation_tps": 39.86074384392174, "peak_memory_gb": 11.263624514, "active_memory_gb": 7.325978448614478, "cache_memory_gb": 0.48823838494718075}}
{"model": "mlx-community/paligemma2-10b-ft-docci-448-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 1266, "generation_tokens": 103, "generation_tps": 4.858148194685768, "peak_memory_gb": 26.923084326, "active_memory_gb": 18.007616659626365, "cache_memory_gb": 4.776010382920504}}
{"model": "mlx-community/paligemma2-3b-ft-docci-448-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["repetitive(phrase: \"side of the building....\")", "context-ignored"], "metrics": {"prompt_tokens": 1266, "generation_tokens": 500, "generation_tps": 17.059689984782736, "peak_memory_gb": 11.369312854, "active_memory_gb": 5.6585022415965796, "cache_memory_gb": 3.058378115296364}}
{"model": "mlx-community/paligemma2-3b-pt-896-4bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 4338, "generation_tokens": 44, "generation_tps": 105.3907677033023, "peak_memory_gb": 8.766310706, "active_memory_gb": 1.608486345037818, "cache_memory_gb": 0.5270235743373632}}
{"model": "mlx-community/pixtral-12b-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["bullets(17)"], "metrics": {"prompt_tokens": 3180, "generation_tokens": 215, "generation_tps": 31.872433572977073, "peak_memory_gb": 15.624864078, "active_memory_gb": 12.561477836221457, "cache_memory_gb": 0.09276519156992435}}
{"model": "mlx-community/pixtral-12b-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 3180, "generation_tokens": 200, "generation_tps": 19.63132911823385, "peak_memory_gb": 27.42283507, "active_memory_gb": 23.634109672158957, "cache_memory_gb": 0.03750474192202091}}
{"model": "prince-canuma/Florence-2-large-ft", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["lang_mixing", "formatting", "context-ignored"], "metrics": {"prompt_tokens": 232, "generation_tokens": 500, "generation_tps": 322.13159186587717, "peak_memory_gb": 5.150475444, "active_memory_gb": 3.076001353561878, "cache_memory_gb": 0.17414287850260735}}
{"model": "qnguyen3/nanoLLaVA", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 262, "generation_tokens": 55, "generation_tps": 105.95683713811971, "peak_memory_gb": 4.497917134, "active_memory_gb": 1.9669466707855463, "cache_memory_gb": 1.3829147145152092}}
