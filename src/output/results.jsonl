{"model": "HuggingFaceTB/SmolVLM-Instruct", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 1481, "generation_tokens": 13, "generation_tps": 125.80087255612247, "peak_memory_gb": 5.487354064, "active_memory_gb": 4.184753889217973, "cache_memory_gb": 0.11166738159954548}}
{"model": "Qwen/Qwen3-VL-2B-Instruct", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["verbose", "bullets(32)"], "metrics": {"prompt_tokens": 16618, "generation_tokens": 350, "generation_tps": 72.34335125976473, "peak_memory_gb": 12.432522588, "active_memory_gb": 3.963240822777152, "cache_memory_gb": 0.7043313402682543}}
{"model": "meta-llama/Llama-3.2-11B-Vision-Instruct", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 276, "generation_tokens": 356, "generation_tps": 3.891421121534343, "peak_memory_gb": 25.246363302, "active_memory_gb": 19.875752087682486, "cache_memory_gb": 3.0100450832396746}}
{"model": "microsoft/Florence-2-large-ft", "success": false, "error_stage": "Model Error", "error_message": "Model loading failed: Missing 1 parameters: \nlanguage_model.lm_head.weight.", "quality_issues": [], "metrics": {}}
{"model": "microsoft/Phi-3.5-vision-instruct", "success": false, "error_stage": "Error", "error_message": "Model generation failed for microsoft/Phi-3.5-vision-instruct: Failed to process inputs with error: Phi3VProcessor.__call__() got an unexpected keyword argument 'padding_side'", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Apriel-1.5-15b-Thinker-6bit-MLX", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["hallucination"], "metrics": {"prompt_tokens": 3259, "generation_tokens": 500, "generation_tps": 34.70938577548197, "peak_memory_gb": 14.78411899, "active_memory_gb": 11.69638067111373, "cache_memory_gb": 0.14244647324085236}}
{"model": "mlx-community/Devstral-Small-2-24B-Instruct-2512-5bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 2429, "generation_tokens": 252, "generation_tps": 26.911153306916194, "peak_memory_gb": 21.947828649, "active_memory_gb": 15.871880192309618, "cache_memory_gb": 0.08535466156899929}}
{"model": "mlx-community/GLM-4.6V-Flash-6bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["verbose", "formatting"], "metrics": {"prompt_tokens": 6386, "generation_tokens": 500, "generation_tps": 44.025586323204365, "peak_memory_gb": 12.766479722, "active_memory_gb": 8.77819937467575, "cache_memory_gb": 0.16706405952572823}}
{"model": "mlx-community/Idefics3-8B-Llama3-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["formatting"], "metrics": {"prompt_tokens": 2586, "generation_tokens": 364, "generation_tps": 29.144771502972993, "peak_memory_gb": 18.532864766, "active_memory_gb": 15.764423873275518, "cache_memory_gb": 0.06270590610802174}}
{"model": "mlx-community/InternVL3-14B-8bit", "success": false, "error_stage": "Error", "error_message": "Model loading failed: Received a InternVLImageProcessor for argument image_processor, but a ImageProcessingMixin was expected.", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Kimi-VL-A3B-Thinking-2506-bf16", "success": false, "error_stage": "Lib Version", "error_message": "Model loading failed: cannot import name '_validate_images_text_input_order' from 'transformers.processing_utils' (/opt/homebrew/Caskroom/miniconda/base/envs/mlx-vlm/lib/python3.13/site-packages/transformers/processing_utils.py)", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Kimi-VL-A3B-Thinking-8bit", "success": false, "error_stage": "Lib Version", "error_message": "Model loading failed: cannot import name '_validate_images_text_input_order' from 'transformers.processing_utils' (/opt/homebrew/Caskroom/miniconda/base/envs/mlx-vlm/lib/python3.13/site-packages/transformers/processing_utils.py)", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/LFM2-VL-1.6B-8bit", "success": false, "error_stage": "Error", "error_message": "Unsupported model: lfm2-vl", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Llama-3.2-11B-Vision-Instruct-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 275, "generation_tokens": 219, "generation_tps": 8.463870104999168, "peak_memory_gb": 14.956771115, "active_memory_gb": 10.56960511393845, "cache_memory_gb": 0.44260607473552227}}
{"model": "mlx-community/Ministral-3-3B-Instruct-2512-4bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 2994, "generation_tokens": 333, "generation_tps": 155.1586644653682, "peak_memory_gb": 7.656593726, "active_memory_gb": 2.5621266923844814, "cache_memory_gb": 0.27581196650862694}}
{"model": "mlx-community/Molmo-7B-D-0924-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 1485, "generation_tokens": 232, "generation_tps": 45.63266356248284, "peak_memory_gb": 40.698473112, "active_memory_gb": 8.41916566900909, "cache_memory_gb": 0.05087570287287235}}
{"model": "mlx-community/Molmo-7B-D-0924-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 1485, "generation_tokens": 233, "generation_tps": 28.483970034710797, "peak_memory_gb": 47.509187194, "active_memory_gb": 14.943564474582672, "cache_memory_gb": 0.03265915997326374}}
{"model": "mlx-community/Phi-3.5-vision-instruct-bf16", "success": false, "error_stage": "Error", "error_message": "Model loading failed: /Users/jrp/.cache/huggingface/hub/models--mlx-community--Phi-3.5-vision-instruct-bf16/snapshots/d8da684308c275a86659e2b36a9189b2f4aec8ea does not appear to have a file named image_processing_phi3_v.py. Checkout 'https://huggingface.co//Users/jrp/.cache/huggingface/hub/models--mlx-community--Phi-3.5-vision-instruct-bf16/snapshots/d8da684308c275a86659e2b36a9189b2f4aec8ea/tree/main' for available files.", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Qwen2-VL-2B-Instruct-4bit", "success": false, "error_stage": "OOM", "error_message": "Model runtime error during generation for mlx-community/Qwen2-VL-2B-Instruct-4bit: [metal::malloc] Attempting to allocate 136434163712 bytes which is greater than the maximum allowed buffer size of 86586540032 bytes.", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Qwen3-VL-2B-Thinking-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 16620, "generation_tokens": 500, "generation_tps": 72.38652522069079, "peak_memory_gb": 12.436520314, "active_memory_gb": 3.9663536436855793, "cache_memory_gb": 0.7043311465531588}}
{"model": "mlx-community/SmolVLM-Instruct-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 1481, "generation_tokens": 13, "generation_tps": 123.23881075980601, "peak_memory_gb": 5.49112243, "active_memory_gb": 4.188263453543186, "cache_memory_gb": 0.11166738159954548}}
{"model": "mlx-community/SmolVLM2-2.2B-Instruct-mlx", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 1381, "generation_tokens": 12, "generation_tps": 127.1513626666156, "peak_memory_gb": 5.48361679, "active_memory_gb": 4.189316309988499, "cache_memory_gb": 0.1116658914834261}}
{"model": "mlx-community/deepseek-vl2-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 2314, "generation_tokens": 59, "generation_tps": 64.16973288341573, "peak_memory_gb": 32.330854043, "active_memory_gb": 27.326322577893734, "cache_memory_gb": 0.1532322010025382}}
{"model": "mlx-community/gemma-3-12b-pt-8bit", "success": false, "error_stage": "Error", "error_message": "Cannot use apply_chat_template because this processor does not have a chat template.", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/gemma-3-27b-it-qat-4bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["bullets(33)"], "metrics": {"prompt_tokens": 559, "generation_tokens": 338, "generation_tps": 28.16234208317232, "peak_memory_gb": 18.67902478, "active_memory_gb": 15.029194163158536, "cache_memory_gb": 0.08402187377214432}}
{"model": "mlx-community/gemma-3-27b-it-qat-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["bullets(27)"], "metrics": {"prompt_tokens": 559, "generation_tokens": 375, "generation_tps": 15.48225151799974, "peak_memory_gb": 32.183553174, "active_memory_gb": 27.606266306713223, "cache_memory_gb": 0.044121479615569115}}
{"model": "mlx-community/gemma-3n-E4B-it-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["verbose"], "metrics": {"prompt_tokens": 559, "generation_tokens": 409, "generation_tps": 41.89509527326581, "peak_memory_gb": 17.823485301, "active_memory_gb": 14.62724013440311, "cache_memory_gb": 0.014208488166332245}}
{"model": "mlx-community/llava-v1.6-mistral-7b-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 2571, "generation_tokens": 15, "generation_tps": 45.96841663567368, "peak_memory_gb": 11.961305346, "active_memory_gb": 7.494249461218715, "cache_memory_gb": 0.47387705370783806}}
{"model": "mlx-community/paligemma2-10b-ft-docci-448-6bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["repetitive(phrase: \"there are gray stone...\")"], "metrics": {"prompt_tokens": 1315, "generation_tokens": 500, "generation_tps": 41.45880947042897, "peak_memory_gb": 11.577444024, "active_memory_gb": 7.326222583651543, "cache_memory_gb": 0.5505145695060492}}
{"model": "mlx-community/paligemma2-10b-ft-docci-448-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 1315, "generation_tokens": 120, "generation_tps": 5.069511388644305, "peak_memory_gb": 26.935902036, "active_memory_gb": 18.00786079466343, "cache_memory_gb": 4.775908637791872}}
{"model": "mlx-community/paligemma2-3b-ft-docci-448-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["repetitive(phrase: \"the squares alternate between...\")", "context-ignored"], "metrics": {"prompt_tokens": 1315, "generation_tokens": 500, "generation_tps": 18.032682731727576, "peak_memory_gb": 11.480647056, "active_memory_gb": 5.658746376633644, "cache_memory_gb": 3.079803664237261}}
{"model": "mlx-community/paligemma2-3b-pt-896-4bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 4387, "generation_tokens": 9, "generation_tps": 123.9055014091301, "peak_memory_gb": 8.736885036, "active_memory_gb": 1.6087304800748825, "cache_memory_gb": 0.526655844412744}}
{"model": "mlx-community/pixtral-12b-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["verbose", "bullets(30)"], "metrics": {"prompt_tokens": 3161, "generation_tokens": 319, "generation_tps": 34.19269133095562, "peak_memory_gb": 15.557263688, "active_memory_gb": 12.561721971258521, "cache_memory_gb": 0.09237013198435307}}
{"model": "mlx-community/pixtral-12b-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["bullets(24)"], "metrics": {"prompt_tokens": 3161, "generation_tokens": 299, "generation_tps": 19.415306423679013, "peak_memory_gb": 27.414036872, "active_memory_gb": 23.63435380719602, "cache_memory_gb": 0.037233131006360054}}
{"model": "prince-canuma/Florence-2-large-ft", "success": false, "error_stage": "Error", "error_message": "Model loading failed: RobertaTokenizer has no attribute additional_special_tokens", "quality_issues": [], "metrics": {}}
{"model": "qnguyen3/nanoLLaVA", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 301, "generation_tokens": 110, "generation_tps": 104.53673184372411, "peak_memory_gb": 7.820981016, "active_memory_gb": 5.033105902373791, "cache_memory_gb": 1.3827715180814266}}
