{"model": "HuggingFaceTB/SmolVLM-Instruct", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": [], "metrics": {"prompt_tokens": 1461, "generation_tokens": 268, "generation_tps": 111.76854877548406, "peak_memory_gb": 5.487353744, "active_memory_gb": 4.184753889217973, "cache_memory_gb": 0.1252927016466856}}
{"model": "Qwen/Qwen3-VL-2B-Instruct", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": ["verbose", "bullets(22)"], "metrics": {"prompt_tokens": 16541, "generation_tokens": 452, "generation_tps": 71.88007191290247, "peak_memory_gb": 12.40388336, "active_memory_gb": 3.9632408265024424, "cache_memory_gb": 0.7029577810317278}}
{"model": "meta-llama/Llama-3.2-11B-Vision-Instruct", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": [], "metrics": {"prompt_tokens": 262, "generation_tokens": 179, "generation_tps": 3.791581997756889, "peak_memory_gb": 25.168948222, "active_memory_gb": 19.875752087682486, "cache_memory_gb": 3.0081237573176622}}
{"model": "microsoft/Florence-2-large-ft", "success": false, "error_stage": "Weight Mismatch", "error_message": "Model loading failed: Missing 1 parameters: \nlanguage_model.lm_head.weight.", "error_package": "mlx", "quality_issues": [], "metrics": {}}
{"model": "microsoft/Phi-3.5-vision-instruct", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": ["lang_mixing", "hallucination"], "metrics": {"prompt_tokens": 1074, "generation_tokens": 500, "generation_tps": 10.819640768303525, "peak_memory_gb": 11.508068016, "active_memory_gb": 7.724683305248618, "cache_memory_gb": 1.393922934308648}}
{"model": "mlx-community/Apriel-1.5-15b-Thinker-6bit-MLX", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": [], "metrics": {"prompt_tokens": 3306, "generation_tokens": 500, "generation_tps": 33.24171132964669, "peak_memory_gb": 14.806941906, "active_memory_gb": 11.69644170999527, "cache_memory_gb": 0.14263049326837063}}
{"model": "mlx-community/Devstral-Small-2-24B-Instruct-2512-5bit", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": [], "metrics": {"prompt_tokens": 2479, "generation_tokens": 217, "generation_tps": 26.00258695777185, "peak_memory_gb": 22.197930463, "active_memory_gb": 15.871941238641739, "cache_memory_gb": 0.08582777716219425}}
{"model": "mlx-community/GLM-4.6V-Flash-6bit", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": ["formatting"], "metrics": {"prompt_tokens": 6306, "generation_tokens": 500, "generation_tps": 46.69799253611846, "peak_memory_gb": 12.728305006, "active_memory_gb": 8.778260413557291, "cache_memory_gb": 0.16593485698103905}}
{"model": "mlx-community/Idefics3-8B-Llama3-bf16", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": ["formatting"], "metrics": {"prompt_tokens": 2573, "generation_tokens": 195, "generation_tps": 29.33462912170881, "peak_memory_gb": 18.526900994, "active_memory_gb": 15.764484912157059, "cache_memory_gb": 0.059776218608021736}}
{"model": "mlx-community/InternVL3-14B-8bit", "success": false, "error_stage": "Processor Error", "error_message": "Model loading failed: Received a InternVLImageProcessor for argument image_processor, but a ImageProcessingMixin was expected.", "error_package": "mlx-vlm", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Kimi-VL-A3B-Thinking-2506-bf16", "success": false, "error_stage": "Lib Version", "error_message": "Model loading failed: cannot import name '_validate_images_text_input_order' from 'transformers.processing_utils' (/opt/homebrew/Caskroom/miniconda/base/envs/mlx-vlm/lib/python3.13/site-packages/transformers/processing_utils.py)", "error_package": "mlx-vlm", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Kimi-VL-A3B-Thinking-8bit", "success": false, "error_stage": "Lib Version", "error_message": "Model loading failed: cannot import name '_validate_images_text_input_order' from 'transformers.processing_utils' (/opt/homebrew/Caskroom/miniconda/base/envs/mlx-vlm/lib/python3.13/site-packages/transformers/processing_utils.py)", "error_package": "mlx-vlm", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/LFM2-VL-1.6B-8bit", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": [], "metrics": {"prompt_tokens": 2064, "generation_tokens": 14, "generation_tps": 316.78747106285203, "peak_memory_gb": 52.70406, "active_memory_gb": 1.9180225189775229, "cache_memory_gb": 0.07537874393165112}}
{"model": "mlx-community/LFM2.5-VL-1.6B-bf16", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": [], "metrics": {"prompt_tokens": 2064, "generation_tokens": 12, "generation_tps": 196.55952689134574, "peak_memory_gb": 4.440787588, "active_memory_gb": 2.9767780657857656, "cache_memory_gb": 0.03958147205412388}}
{"model": "mlx-community/Llama-3.2-11B-Vision-Instruct-8bit", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": [], "metrics": {"prompt_tokens": 261, "generation_tokens": 159, "generation_tps": 8.251576717705383, "peak_memory_gb": 14.948971263, "active_memory_gb": 10.569925557821989, "cache_memory_gb": 0.44262065552175045}}
{"model": "mlx-community/Ministral-3-3B-Instruct-2512-4bit", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": [], "metrics": {"prompt_tokens": 3003, "generation_tokens": 406, "generation_tps": 155.865298063365, "peak_memory_gb": 7.906974033, "active_memory_gb": 2.5624471362680197, "cache_memory_gb": 0.2958162557333708}}
{"model": "mlx-community/Molmo-7B-D-0924-8bit", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": [], "metrics": {"prompt_tokens": 1476, "generation_tokens": 364, "generation_tps": 43.677809777958096, "peak_memory_gb": 40.694573658, "active_memory_gb": 8.419486112892628, "cache_memory_gb": 0.05527016706764698}}
{"model": "mlx-community/Molmo-7B-D-0924-bf16", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": ["bullets(18)"], "metrics": {"prompt_tokens": 1476, "generation_tokens": 218, "generation_tps": 28.47614529186379, "peak_memory_gb": 47.506451004, "active_memory_gb": 14.94388491846621, "cache_memory_gb": 0.03265909291803837}}
{"model": "mlx-community/Phi-3.5-vision-instruct-bf16", "success": false, "error_stage": "Config Missing", "error_message": "Model loading failed: /Users/jrp/.cache/huggingface/hub/models--mlx-community--Phi-3.5-vision-instruct-bf16/snapshots/d8da684308c275a86659e2b36a9189b2f4aec8ea does not appear to have a file named image_processing_phi3_v.py. Checkout 'https://huggingface.co//Users/jrp/.cache/huggingface/hub/models--mlx-community--Phi-3.5-vision-instruct-bf16/snapshots/d8da684308c275a86659e2b36a9189b2f4aec8ea/tree/main' for available files.", "error_package": "mlx-vlm", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Qwen2-VL-2B-Instruct-4bit", "success": false, "error_stage": "OOM", "error_message": "Model runtime error during generation for mlx-community/Qwen2-VL-2B-Instruct-4bit: [metal::malloc] Attempting to allocate 135433060352 bytes which is greater than the maximum allowed buffer size of 86586540032 bytes.", "error_package": "mlx", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Qwen3-VL-2B-Thinking-bf16", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": [], "metrics": {"prompt_tokens": 16543, "generation_tokens": 500, "generation_tps": 71.75263065334356, "peak_memory_gb": 12.408241544, "active_memory_gb": 3.966674091294408, "cache_memory_gb": 0.7029578629881144}}
{"model": "mlx-community/SmolVLM-Instruct-bf16", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": ["verbose", "formatting"], "metrics": {"prompt_tokens": 1461, "generation_tokens": 500, "generation_tps": 112.75649991240256, "peak_memory_gb": 5.524218403, "active_memory_gb": 4.188583897426724, "cache_memory_gb": 0.1389645766466856}}
{"model": "mlx-community/SmolVLM2-2.2B-Instruct-mlx", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": [], "metrics": {"prompt_tokens": 1361, "generation_tokens": 117, "generation_tps": 116.1110057217572, "peak_memory_gb": 5.483960544, "active_memory_gb": 4.189636753872037, "cache_memory_gb": 0.11166559346020222}}
{"model": "mlx-community/deepseek-vl2-8bit", "success": false, "error_stage": "Type Cast Error", "error_message": "Model runtime error during generation for mlx-community/deepseek-vl2-8bit: std::bad_cast", "error_package": "mlx", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/gemma-3-27b-it-qat-4bit", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": ["bullets(25)"], "metrics": {"prompt_tokens": 531, "generation_tokens": 367, "generation_tps": 27.803934194178648, "peak_memory_gb": 19.383929334, "active_memory_gb": 15.685688313096762, "cache_memory_gb": 0.08402124792337418}}
{"model": "mlx-community/gemma-3-27b-it-qat-8bit", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": ["bullets(26)"], "metrics": {"prompt_tokens": 531, "generation_tokens": 325, "generation_tps": 15.265955551050375, "peak_memory_gb": 33.593264644, "active_memory_gb": 28.919163044542074, "cache_memory_gb": 0.04412085376679897}}
{"model": "mlx-community/gemma-3n-E4B-it-bf16", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": ["verbose"], "metrics": {"prompt_tokens": 531, "generation_tokens": 429, "generation_tps": 41.45721181481064, "peak_memory_gb": 17.198700275, "active_memory_gb": 14.62733169645071, "cache_memory_gb": 0.014255546033382416}}
{"model": "mlx-community/llava-v1.6-mistral-7b-8bit", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 2565, "generation_tokens": 8, "generation_tps": 48.0132875800901, "peak_memory_gb": 11.953948936, "active_memory_gb": 7.494341019541025, "cache_memory_gb": 0.47387705370783806}}
{"model": "mlx-community/paligemma2-10b-ft-docci-448-6bit", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 1286, "generation_tokens": 110, "generation_tps": 41.38890634338565, "peak_memory_gb": 11.517444894, "active_memory_gb": 7.326314141973853, "cache_memory_gb": 0.4877695646136999}}
{"model": "mlx-community/paligemma2-10b-ft-docci-448-bf16", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 1286, "generation_tokens": 83, "generation_tps": 4.946504866063631, "peak_memory_gb": 26.906164154, "active_memory_gb": 18.00795235298574, "cache_memory_gb": 4.775907773524523}}
{"model": "mlx-community/paligemma2-3b-ft-docci-448-bf16", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 1286, "generation_tokens": 94, "generation_tps": 17.67236802340434, "peak_memory_gb": 11.206524894, "active_memory_gb": 5.6588379349559546, "cache_memory_gb": 3.0543234795331955}}
{"model": "mlx-community/paligemma2-3b-pt-896-4bit", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 4358, "generation_tokens": 9, "generation_tps": 126.93092072927101, "peak_memory_gb": 8.692205874, "active_memory_gb": 1.608822038397193, "cache_memory_gb": 0.5266574379056692}}
{"model": "mlx-community/pixtral-12b-8bit", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": ["verbose", "bullets(22)"], "metrics": {"prompt_tokens": 3215, "generation_tokens": 302, "generation_tps": 34.03762524167009, "peak_memory_gb": 15.58346171, "active_memory_gb": 12.561813529580832, "cache_memory_gb": 0.09255347587168217}}
{"model": "mlx-community/pixtral-12b-bf16", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": ["bullets(20)"], "metrics": {"prompt_tokens": 3215, "generation_tokens": 265, "generation_tps": 19.15251886852787, "peak_memory_gb": 27.439677838, "active_memory_gb": 23.63444536551833, "cache_memory_gb": 0.03741687722504139}}
{"model": "prince-canuma/Florence-2-large-ft", "success": false, "error_stage": "Model Error", "error_message": "Model loading failed: RobertaTokenizer has no attribute additional_special_tokens", "error_package": "mlx-vlm", "quality_issues": [], "metrics": {}}
{"model": "qnguyen3/nanoLLaVA", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": [], "metrics": {"prompt_tokens": 284, "generation_tokens": 139, "generation_tps": 100.01160206473631, "peak_memory_gb": 7.839962042, "active_memory_gb": 5.033197460696101, "cache_memory_gb": 1.3827709779143333}}
