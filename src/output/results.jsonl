{"model": "HuggingFaceTB/SmolVLM-Instruct", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": [], "metrics": {"prompt_tokens": 1461, "generation_tokens": 291, "generation_tps": 112.41892002714145, "peak_memory_gb": 5.487353744, "active_memory_gb": 4.184753889217973, "cache_memory_gb": 0.1252927016466856}}
{"model": "Qwen/Qwen3-VL-2B-Instruct", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": ["verbose", "bullets(57)"], "metrics": {"prompt_tokens": 16541, "generation_tokens": 500, "generation_tps": 72.17360190019255, "peak_memory_gb": 12.40388336, "active_memory_gb": 3.9632408265024424, "cache_memory_gb": 0.7029579598456621}}
{"model": "meta-llama/Llama-3.2-11B-Vision-Instruct", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": [], "metrics": {"prompt_tokens": 262, "generation_tokens": 237, "generation_tps": 3.846889507084676, "peak_memory_gb": 25.168948222, "active_memory_gb": 19.875767346471548, "cache_memory_gb": 3.0081237573176622}}
{"model": "microsoft/Florence-2-large-ft", "success": false, "error_stage": "Weight Mismatch", "error_message": "Model loading failed: Missing 1 parameters: \nlanguage_model.lm_head.weight.", "error_package": "mlx", "quality_issues": [], "metrics": {}}
{"model": "microsoft/Phi-3.5-vision-instruct", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": ["lang_mixing"], "metrics": {"prompt_tokens": 1072, "generation_tokens": 500, "generation_tps": 10.822329123096884, "peak_memory_gb": 11.507915444, "active_memory_gb": 7.724678570404649, "cache_memory_gb": 1.3939076457172632}}
{"model": "mlx-community/Apriel-1.5-15b-Thinker-6bit-MLX", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": [], "metrics": {"prompt_tokens": 3321, "generation_tokens": 500, "generation_tps": 35.71931405992963, "peak_memory_gb": 14.814118098, "active_memory_gb": 11.696456968784332, "cache_memory_gb": 0.14263063296675682}}
{"model": "mlx-community/Devstral-Small-2-24B-Instruct-2512-5bit", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": [], "metrics": {"prompt_tokens": 2479, "generation_tokens": 217, "generation_tps": 27.931828016407856, "peak_memory_gb": 22.197946847, "active_memory_gb": 15.871956497430801, "cache_memory_gb": 0.08582777716219425}}
{"model": "mlx-community/GLM-4.6V-Flash-6bit", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": ["formatting"], "metrics": {"prompt_tokens": 6306, "generation_tokens": 500, "generation_tps": 47.65519030029347, "peak_memory_gb": 12.72832139, "active_memory_gb": 8.778275672346354, "cache_memory_gb": 0.16593509539961815}}
{"model": "mlx-community/Idefics3-8B-Llama3-bf16", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": ["formatting"], "metrics": {"prompt_tokens": 2573, "generation_tokens": 204, "generation_tps": 29.300394375753065, "peak_memory_gb": 18.526917378, "active_memory_gb": 15.764500170946121, "cache_memory_gb": 0.059776218608021736}}
{"model": "mlx-community/InternVL3-14B-8bit", "success": false, "error_stage": "Processor Error", "error_message": "Model loading failed: Received a InternVLImageProcessor for argument image_processor, but a ImageProcessingMixin was expected.", "error_package": "mlx-vlm", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Kimi-VL-A3B-Thinking-2506-bf16", "success": false, "error_stage": "Lib Version", "error_message": "Model loading failed: cannot import name '_validate_images_text_input_order' from 'transformers.processing_utils' (/opt/homebrew/Caskroom/miniconda/base/envs/mlx-vlm/lib/python3.13/site-packages/transformers/processing_utils.py)", "error_package": "mlx-vlm", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Kimi-VL-A3B-Thinking-8bit", "success": false, "error_stage": "Lib Version", "error_message": "Model loading failed: cannot import name '_validate_images_text_input_order' from 'transformers.processing_utils' (/opt/homebrew/Caskroom/miniconda/base/envs/mlx-vlm/lib/python3.13/site-packages/transformers/processing_utils.py)", "error_package": "mlx-vlm", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/LFM2-VL-1.6B-8bit", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": ["repetitive(phrase: \"photography", "natural scenery", "...\")"], "metrics": {"prompt_tokens": 2064, "generation_tokens": 500, "generation_tps": 290.6067753235955, "peak_memory_gb": 21.132608706, "active_memory_gb": 18.522247886285186, "cache_memory_gb": 0.07087930850684643}}
{"model": "mlx-community/LFM2.5-VL-1.6B-bf16", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": [], "metrics": {"prompt_tokens": 2064, "generation_tokens": 163, "generation_tps": 180.9945455934979, "peak_memory_gb": 17.83090349, "active_memory_gb": 2.976828372105956, "cache_memory_gb": 0.03969402052462101}}
{"model": "mlx-community/Llama-3.2-11B-Vision-Instruct-8bit", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 261, "generation_tokens": 151, "generation_tps": 8.662964656621883, "peak_memory_gb": 14.948987647, "active_memory_gb": 10.569940816611052, "cache_memory_gb": 0.44262065552175045}}
{"model": "mlx-community/Ministral-3-3B-Instruct-2512-4bit", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": ["bullets(18)"], "metrics": {"prompt_tokens": 3044, "generation_tokens": 292, "generation_tps": 156.7685807834038, "peak_memory_gb": 7.906990458, "active_memory_gb": 2.562462395057082, "cache_memory_gb": 0.29582205414772034}}
{"model": "mlx-community/Molmo-7B-D-0924-8bit", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": [], "metrics": {"prompt_tokens": 1476, "generation_tokens": 332, "generation_tps": 44.94543689755211, "peak_memory_gb": 40.694590042, "active_memory_gb": 8.41950137168169, "cache_memory_gb": 0.05527016706764698}}
{"model": "mlx-community/Molmo-7B-D-0924-bf16", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": [], "metrics": {"prompt_tokens": 1476, "generation_tokens": 357, "generation_tps": 28.105846248824225, "peak_memory_gb": 47.506467388, "active_memory_gb": 14.943900177255273, "cache_memory_gb": 0.03448984958231449}}
{"model": "mlx-community/Phi-3.5-vision-instruct-bf16", "success": false, "error_stage": "Config Missing", "error_message": "Model loading failed: /Users/jrp/.cache/huggingface/hub/models--mlx-community--Phi-3.5-vision-instruct-bf16/snapshots/d8da684308c275a86659e2b36a9189b2f4aec8ea does not appear to have a file named image_processing_phi3_v.py. Checkout 'https://huggingface.co//Users/jrp/.cache/huggingface/hub/models--mlx-community--Phi-3.5-vision-instruct-bf16/snapshots/d8da684308c275a86659e2b36a9189b2f4aec8ea/tree/main' for available files.", "error_package": "mlx-vlm", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Qwen2-VL-2B-Instruct-4bit", "success": false, "error_stage": "OOM", "error_message": "Model runtime error during generation for mlx-community/Qwen2-VL-2B-Instruct-4bit: [metal::malloc] Attempting to allocate 135433060352 bytes which is greater than the maximum allowed buffer size of 86586540032 bytes.", "error_package": "mlx", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Qwen3-VL-2B-Thinking-bf16", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": [], "metrics": {"prompt_tokens": 16543, "generation_tokens": 500, "generation_tps": 72.40874665058232, "peak_memory_gb": 12.408257928, "active_memory_gb": 3.9666893500834703, "cache_memory_gb": 0.7029580418020487}}
{"model": "mlx-community/SmolVLM-Instruct-bf16", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": ["verbose"], "metrics": {"prompt_tokens": 1461, "generation_tokens": 331, "generation_tps": 115.21640293072787, "peak_memory_gb": 5.524234803, "active_memory_gb": 4.188599156215787, "cache_memory_gb": 0.1252927016466856}}
{"model": "mlx-community/SmolVLM2-2.2B-Instruct-mlx", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": [], "metrics": {"prompt_tokens": 1361, "generation_tokens": 102, "generation_tps": 117.97431167305464, "peak_memory_gb": 5.483976928, "active_memory_gb": 4.189652012661099, "cache_memory_gb": 0.11166559346020222}}
{"model": "mlx-community/deepseek-vl2-8bit", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 2302, "generation_tokens": 24, "generation_tps": 62.97081887674801, "peak_memory_gb": 32.164851041, "active_memory_gb": 27.326658280566335, "cache_memory_gb": 0.15323188435286283}}
{"model": "mlx-community/gemma-3-12b-pt-8bit", "success": false, "error_stage": "No Chat Template", "error_message": "Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating", "error_package": "mlx-vlm", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/gemma-3-27b-it-qat-4bit", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": ["bullets(25)"], "metrics": {"prompt_tokens": 531, "generation_tokens": 376, "generation_tps": 28.081668937639066, "peak_memory_gb": 19.384207862, "active_memory_gb": 15.685947712510824, "cache_memory_gb": 0.08402124792337418}}
{"model": "mlx-community/gemma-3-27b-it-qat-8bit", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": ["bullets(23)"], "metrics": {"prompt_tokens": 531, "generation_tokens": 281, "generation_tps": 15.38279935440752, "peak_memory_gb": 33.593543172, "active_memory_gb": 28.919422443956137, "cache_memory_gb": 0.04412085376679897}}
{"model": "mlx-community/gemma-3n-E4B-it-bf16", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": ["verbose"], "metrics": {"prompt_tokens": 531, "generation_tokens": 325, "generation_tps": 41.302282794308496, "peak_memory_gb": 17.198993139, "active_memory_gb": 14.62757583707571, "cache_memory_gb": 0.014256499707698822}}
{"model": "mlx-community/llava-v1.6-mistral-7b-8bit", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 2565, "generation_tokens": 8, "generation_tps": 47.843491977059855, "peak_memory_gb": 11.95421108, "active_memory_gb": 7.494585160166025, "cache_memory_gb": 0.47387705370783806}}
{"model": "mlx-community/paligemma2-10b-ft-docci-448-6bit", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 1286, "generation_tokens": 117, "generation_tps": 41.23293524771622, "peak_memory_gb": 11.517707038, "active_memory_gb": 7.326558282598853, "cache_memory_gb": 0.4877695646136999}}
{"model": "mlx-community/paligemma2-10b-ft-docci-448-bf16", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 1286, "generation_tokens": 78, "generation_tps": 4.9460113401780275, "peak_memory_gb": 26.906426298, "active_memory_gb": 18.00819649361074, "cache_memory_gb": 4.775907773524523}}
{"model": "mlx-community/paligemma2-3b-ft-docci-448-bf16", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 1286, "generation_tokens": 59, "generation_tps": 17.74884460205268, "peak_memory_gb": 11.206787038, "active_memory_gb": 5.6590820755809546, "cache_memory_gb": 3.0543234795331955}}
{"model": "mlx-community/paligemma2-3b-pt-896-4bit", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 4358, "generation_tokens": 9, "generation_tps": 121.21776660610418, "peak_memory_gb": 8.692468018, "active_memory_gb": 1.609066179022193, "cache_memory_gb": 0.526656961068511}}
{"model": "mlx-community/pixtral-12b-8bit", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": ["verbose", "bullets(19)"], "metrics": {"prompt_tokens": 3223, "generation_tokens": 339, "generation_tps": 33.53746323531053, "peak_memory_gb": 15.58755771, "active_memory_gb": 12.562057670205832, "cache_memory_gb": 0.09255347587168217}}
{"model": "mlx-community/pixtral-12b-bf16", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": ["verbose", "bullets(22)"], "metrics": {"prompt_tokens": 3223, "generation_tokens": 307, "generation_tps": 19.029458071955524, "peak_memory_gb": 27.443691918, "active_memory_gb": 23.63468950614333, "cache_memory_gb": 0.03741574473679066}}
{"model": "prince-canuma/Florence-2-large-ft", "success": false, "error_stage": "Model Error", "error_message": "Model loading failed: RobertaTokenizer has no attribute additional_special_tokens", "error_package": "mlx-vlm", "quality_issues": [], "metrics": {}}
{"model": "qnguyen3/nanoLLaVA", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 284, "generation_tokens": 161, "generation_tps": 99.45218677872049, "peak_memory_gb": 5.403994172, "active_memory_gb": 1.967328129336238, "cache_memory_gb": 1.3828107416629791}}
