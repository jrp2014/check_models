{"model": "HuggingFaceTB/SmolVLM-Instruct", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 1430, "generation_tokens": 9, "generation_tps": 129.92160733440434, "peak_memory_gb": 5.54614324, "active_memory_gb": 0.00010683573782444, "cache_memory_gb": 4.303576966747642}}
{"model": "meta-llama/Llama-3.2-11B-Vision-Instruct", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["bullets(18)"], "metrics": {"prompt_tokens": 237, "generation_tokens": 234, "generation_tps": 3.8648133410523493, "peak_memory_gb": 25.159984966, "active_memory_gb": 0.0005951281636953354, "cache_memory_gb": 22.88295914977789}}
{"model": "microsoft/Florence-2-large-ft", "success": false, "error_stage": "Model Error", "error_message": "Model loading failed: Missing 1 parameters: \nlanguage_model.lm_head.weight.", "quality_issues": [], "metrics": {}}
{"model": "microsoft/Phi-3.5-vision-instruct", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["repetitive(phrase: \"# instruction # solution...\")", "lang_mixing"], "metrics": {"prompt_tokens": 1042, "generation_tokens": 500, "generation_tps": 10.833555978708237, "peak_memory_gb": 11.505708192, "active_memory_gb": 0.0006761606782674789, "cache_memory_gb": 9.117639670148492}}
{"model": "mlx-community/Apriel-1.5-15b-Thinker-6bit-MLX", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 2933, "generation_tokens": 500, "generation_tps": 35.727518082018264, "peak_memory_gb": 14.714749122, "active_memory_gb": 0.0009003132581710815, "cache_memory_gb": 11.828470936976373}}
{"model": "mlx-community/Idefics3-8B-Llama3-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["formatting", "context-ignored"], "metrics": {"prompt_tokens": 2547, "generation_tokens": 278, "generation_tps": 29.28226791274592, "peak_memory_gb": 18.518545106, "active_memory_gb": 0.0011444538831710815, "cache_memory_gb": 15.831720029003918}}
{"model": "mlx-community/InternVL3-14B-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["bullets(17)"], "metrics": {"prompt_tokens": 2032, "generation_tokens": 264, "generation_tps": 28.773811124454514, "peak_memory_gb": 17.86645937, "active_memory_gb": 0.0014343708753585815, "cache_memory_gb": 15.305263658985496}}
{"model": "mlx-community/Kimi-VL-A3B-Thinking-2506-bf16", "success": false, "error_stage": "Lib Version", "error_message": "Model loading failed: cannot import name '_validate_images_text_input_order' from 'transformers.processing_utils' (/opt/homebrew/Caskroom/miniconda/base/envs/mlx-vlm/lib/python3.13/site-packages/transformers/processing_utils.py)", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Kimi-VL-A3B-Thinking-8bit", "success": false, "error_stage": "Lib Version", "error_message": "Model loading failed: cannot import name '_validate_images_text_input_order' from 'transformers.processing_utils' (/opt/homebrew/Caskroom/miniconda/base/envs/mlx-vlm/lib/python3.13/site-packages/transformers/processing_utils.py)", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/LFM2-VL-1.6B-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 2029, "generation_tokens": 247, "generation_tps": 291.87189923834063, "peak_memory_gb": 52.703355696, "active_memory_gb": 30.563820144161582, "cache_memory_gb": 1.9915000032633543}}
{"model": "mlx-community/Llama-3.2-11B-Vision-Instruct-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 236, "generation_tokens": 164, "generation_tps": 8.43930822375458, "peak_memory_gb": 47.842673348, "active_memory_gb": 30.564308425411582, "cache_memory_gb": 11.009811781346798}}
{"model": "mlx-community/Molmo-7B-D-0924-8bit", "success": false, "error_stage": "Missing Dep", "error_message": "Model loading failed: This modeling file requires the following packages that were not found in your environment: tensorflow. Run `pip install tensorflow`", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Molmo-7B-D-0924-bf16", "success": false, "error_stage": "Missing Dep", "error_message": "Model loading failed: This modeling file requires the following packages that were not found in your environment: tensorflow. Run `pip install tensorflow`", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Phi-3.5-vision-instruct-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 1042, "generation_tokens": 205, "generation_tps": 10.94292351715848, "peak_memory_gb": 68.981997268, "active_memory_gb": 53.92095019482076, "cache_memory_gb": 9.094213614240289}}
{"model": "mlx-community/Qwen2-VL-2B-Instruct-4bit", "success": false, "error_stage": "OOM", "error_message": "Model runtime error during generation for mlx-community/Qwen2-VL-2B-Instruct-4bit: [metal::malloc] Attempting to allocate 135383101952 bytes which is greater than the maximum allowed buffer size of 86586540032 bytes.", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Qwen3-VL-2B-Thinking-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["verbose"], "metrics": {"prompt_tokens": 16505, "generation_tokens": 500, "generation_tps": 71.93333103450438, "peak_memory_gb": 54.448929346, "active_memory_gb": 38.980895314365625, "cache_memory_gb": 4.665798036381602}}
{"model": "mlx-community/SmolVLM-Instruct-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 1430, "generation_tokens": 9, "generation_tps": 130.0656356017218, "peak_memory_gb": 47.40156085, "active_memory_gb": 38.98100212588906, "cache_memory_gb": 4.303576966747642}}
{"model": "mlx-community/SmolVLM2-2.2B-Instruct-mlx", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 1330, "generation_tokens": 23, "generation_tps": 121.84061459750254, "peak_memory_gb": 47.43199671, "active_memory_gb": 38.9811089374125, "cache_memory_gb": 4.3045214135199785}}
{"model": "mlx-community/deepseek-vl2-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["formatting"], "metrics": {"prompt_tokens": 2041, "generation_tokens": 180, "generation_tps": 63.787100386304644, "peak_memory_gb": 73.689065416, "active_memory_gb": 38.98135309666395, "cache_memory_gb": 27.472022691741586}}
{"model": "mlx-community/gemma-3-12b-pt-8bit", "success": false, "error_stage": "Error", "error_message": "Cannot use apply_chat_template because this processor does not have a chat template.", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/gemma-3-27b-it-qat-4bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["bullets(31)"], "metrics": {"prompt_tokens": 504, "generation_tokens": 334, "generation_tps": 26.906705630325327, "peak_memory_gb": 59.96299978, "active_memory_gb": 38.98185664601624, "cache_memory_gb": 15.108726263046265}}
{"model": "mlx-community/gemma-3-27b-it-qat-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["bullets(26)"], "metrics": {"prompt_tokens": 504, "generation_tokens": 331, "generation_tps": 15.380917929197363, "peak_memory_gb": 41.856472486, "active_memory_gb": 0.0038605649024248123, "cache_memory_gb": 27.645676938816905}}
{"model": "mlx-community/gemma-3n-E4B-it-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 504, "generation_tokens": 214, "generation_tps": 42.16121196134787, "peak_memory_gb": 17.772126526, "active_memory_gb": 0.004364142194390297, "cache_memory_gb": 14.636168908327818}}
{"model": "mlx-community/llava-v1.6-mistral-7b-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 2321, "generation_tokens": 8, "generation_tps": 47.13001508048002, "peak_memory_gb": 12.286036238, "active_memory_gb": 0.004486212506890297, "cache_memory_gb": 7.962953668087721}}
{"model": "mlx-community/paligemma2-10b-ft-docci-448-6bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 1262, "generation_tokens": 126, "generation_tps": 40.606931279344266, "peak_memory_gb": 11.252778228, "active_memory_gb": 0.005447544157505035, "cache_memory_gb": 7.8078994657844305}}
{"model": "mlx-community/paligemma2-10b-ft-docci-448-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 1262, "generation_tokens": 162, "generation_tps": 4.98523309334624, "peak_memory_gb": 26.918037976, "active_memory_gb": 0.006408847868442535, "cache_memory_gb": 22.776348371058702}}
{"model": "mlx-community/paligemma2-3b-ft-docci-448-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 1262, "generation_tokens": 180, "generation_tps": 17.588963082020005, "peak_memory_gb": 11.303397896, "active_memory_gb": 0.007370151579380035, "cache_memory_gb": 8.704703614115715}}
{"model": "mlx-community/paligemma2-3b-pt-896-4bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 4334, "generation_tokens": 22, "generation_tps": 103.42821419179235, "peak_memory_gb": 8.759003428, "active_memory_gb": 0.008331455290317535, "cache_memory_gb": 2.1261413134634495}}
{"model": "mlx-community/pixtral-12b-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["verbose", "bullets(19)"], "metrics": {"prompt_tokens": 2842, "generation_tokens": 315, "generation_tps": 33.998086617060764, "peak_memory_gb": 15.494365504, "active_memory_gb": 0.008575601503252983, "cache_memory_gb": 12.638021567836404}}
{"model": "mlx-community/pixtral-12b-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["verbose", "bullets(16)"], "metrics": {"prompt_tokens": 2842, "generation_tokens": 313, "generation_tps": 19.201833903957173, "peak_memory_gb": 27.302019456, "active_memory_gb": 0.008819742128252983, "cache_memory_gb": 23.659053871408105}}
{"model": "prince-canuma/Florence-2-large-ft", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["lang_mixing", "formatting", "context-ignored"], "metrics": {"prompt_tokens": 233, "generation_tokens": 500, "generation_tps": 326.53477056587343, "peak_memory_gb": 5.14954155, "active_memory_gb": 0.009018117561936378, "cache_memory_gb": 3.2402563579380512}}
{"model": "qnguyen3/nanoLLaVA", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 249, "generation_tokens": 64, "generation_tps": 102.77039832843434, "peak_memory_gb": 4.481959052, "active_memory_gb": 0.009597953408956528, "cache_memory_gb": 3.3393966667354107}}
