{"model": "HuggingFaceTB/SmolVLM-Instruct", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 1414, "generation_tokens": 174, "generation_tps": 114.238155646515, "peak_memory_gb": 5.487352992, "active_memory_gb": 4.184753889217973, "cache_memory_gb": 0.1252920012921095}}
{"model": "Qwen/Qwen3-VL-2B-Instruct", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["verbose", "bullets(31)"], "metrics": {"prompt_tokens": 16493, "generation_tokens": 425, "generation_tps": 72.45313434306325, "peak_memory_gb": 12.386942304, "active_memory_gb": 3.9632408265024424, "cache_memory_gb": 0.7029578629881144}}
{"model": "meta-llama/Llama-3.2-11B-Vision-Instruct", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["repetitive(phrase: \"seasons", "seasons", "seasons", "sea...\")"], "metrics": {"prompt_tokens": 224, "generation_tokens": 500, "generation_tps": 3.9104896020890845, "peak_memory_gb": 25.219669222, "active_memory_gb": 19.875752087682486, "cache_memory_gb": 3.0100427586585283}}
{"model": "microsoft/Florence-2-large-ft", "success": false, "error_stage": "Model Error", "error_message": "Model loading failed: Missing 1 parameters: \nlanguage_model.lm_head.weight.", "quality_issues": [], "metrics": {}}
{"model": "microsoft/Phi-3.5-vision-instruct", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["lang_mixing"], "metrics": {"prompt_tokens": 1019, "generation_tokens": 500, "generation_tps": 11.27863231303963, "peak_memory_gb": 11.290609508, "active_memory_gb": 7.724663311615586, "cache_memory_gb": 1.3822033647447824}}
{"model": "mlx-community/Apriel-1.5-15b-Thinker-6bit-MLX", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["hallucination"], "metrics": {"prompt_tokens": 3335, "generation_tokens": 500, "generation_tps": 36.001224149608284, "peak_memory_gb": 14.837203154, "active_memory_gb": 11.69644170999527, "cache_memory_gb": 0.14281434565782547}}
{"model": "mlx-community/Devstral-Small-2-24B-Instruct-2512-5bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 2429, "generation_tokens": 219, "generation_tps": 28.09691451430885, "peak_memory_gb": 22.197930413, "active_memory_gb": 15.871941238641739, "cache_memory_gb": 0.08582768402993679}}
{"model": "mlx-community/GLM-4.6V-Flash-6bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["verbose", "formatting"], "metrics": {"prompt_tokens": 6265, "generation_tokens": 500, "generation_tps": 48.109926754971916, "peak_memory_gb": 12.709283182, "active_memory_gb": 8.778260413557291, "cache_memory_gb": 0.16593474242836237}}
{"model": "mlx-community/Idefics3-8B-Llama3-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["formatting"], "metrics": {"prompt_tokens": 2535, "generation_tokens": 175, "generation_tps": 29.487818289198646, "peak_memory_gb": 18.47560269, "active_memory_gb": 15.764484912157059, "cache_memory_gb": 0.059776218608021736}}
{"model": "mlx-community/InternVL3-14B-8bit", "success": false, "error_stage": "Error", "error_message": "Model loading failed: Received a InternVLImageProcessor for argument image_processor, but a ImageProcessingMixin was expected.", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Kimi-VL-A3B-Thinking-2506-bf16", "success": false, "error_stage": "Lib Version", "error_message": "Model loading failed: cannot import name '_validate_images_text_input_order' from 'transformers.processing_utils' (/opt/homebrew/Caskroom/miniconda/base/envs/mlx-vlm/lib/python3.13/site-packages/transformers/processing_utils.py)", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Kimi-VL-A3B-Thinking-8bit", "success": false, "error_stage": "Lib Version", "error_message": "Model loading failed: cannot import name '_validate_images_text_input_order' from 'transformers.processing_utils' (/opt/homebrew/Caskroom/miniconda/base/envs/mlx-vlm/lib/python3.13/site-packages/transformers/processing_utils.py)", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/LFM2-VL-1.6B-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 2022, "generation_tokens": 14, "generation_tps": 314.1243805172751, "peak_memory_gb": 52.70406, "active_memory_gb": 1.9180225189775229, "cache_memory_gb": 0.06949084810912609}}
{"model": "mlx-community/LFM2.5-VL-1.6B-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 2022, "generation_tokens": 500, "generation_tps": 179.34829492605076, "peak_memory_gb": 4.440754406, "active_memory_gb": 2.9767780657857656, "cache_memory_gb": 0.041512250900268555}}
{"model": "mlx-community/Llama-3.2-11B-Vision-Instruct-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 223, "generation_tokens": 248, "generation_tps": 8.913938840772769, "peak_memory_gb": 15.099704979, "active_memory_gb": 10.569910299032927, "cache_memory_gb": 0.44260367937386036}}
{"model": "mlx-community/Ministral-3-3B-Instruct-2512-4bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 2994, "generation_tokens": 281, "generation_tps": 157.27757193153812, "peak_memory_gb": 7.90695764, "active_memory_gb": 2.562431877478957, "cache_memory_gb": 0.27628498896956444}}
{"model": "mlx-community/Molmo-7B-D-0924-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 1428, "generation_tokens": 172, "generation_tps": 45.77340234656111, "peak_memory_gb": 40.671996122, "active_memory_gb": 8.419470854103565, "cache_memory_gb": 0.05087527818977833}}
{"model": "mlx-community/Molmo-7B-D-0924-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 1428, "generation_tokens": 126, "generation_tps": 28.68901640557497, "peak_memory_gb": 47.49006662, "active_memory_gb": 14.943869659677148, "cache_memory_gb": 0.032658735290169716}}
{"model": "mlx-community/Phi-3.5-vision-instruct-bf16", "success": false, "error_stage": "Error", "error_message": "Model loading failed: /Users/jrp/.cache/huggingface/hub/models--mlx-community--Phi-3.5-vision-instruct-bf16/snapshots/d8da684308c275a86659e2b36a9189b2f4aec8ea does not appear to have a file named image_processing_phi3_v.py. Checkout 'https://huggingface.co//Users/jrp/.cache/huggingface/hub/models--mlx-community--Phi-3.5-vision-instruct-bf16/snapshots/d8da684308c275a86659e2b36a9189b2f4aec8ea/tree/main' for available files.", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Qwen2-VL-2B-Instruct-4bit", "success": false, "error_stage": "OOM", "error_message": "Model runtime error during generation for mlx-community/Qwen2-VL-2B-Instruct-4bit: [metal::malloc] Attempting to allocate 135433060352 bytes which is greater than the maximum allowed buffer size of 86586540032 bytes.", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Qwen3-VL-2B-Thinking-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["verbose"], "metrics": {"prompt_tokens": 16495, "generation_tokens": 500, "generation_tps": 73.18280508529689, "peak_memory_gb": 12.391284104, "active_memory_gb": 3.9666588325053453, "cache_memory_gb": 0.7029577810317278}}
{"model": "mlx-community/SmolVLM-Instruct-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 1414, "generation_tokens": 15, "generation_tps": 122.81772553121623, "peak_memory_gb": 5.524201283, "active_memory_gb": 4.188568638637662, "cache_memory_gb": 0.0645619947463274}}
{"model": "mlx-community/SmolVLM2-2.2B-Instruct-mlx", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 1314, "generation_tokens": 48, "generation_tps": 120.47485918448565, "peak_memory_gb": 5.483943408, "active_memory_gb": 4.189621495082974, "cache_memory_gb": 0.1116648931056261}}
{"model": "mlx-community/deepseek-vl2-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 2258, "generation_tokens": 3, "generation_tps": 95.78391154505853, "peak_memory_gb": 32.131492029, "active_memory_gb": 27.32662776298821, "cache_memory_gb": 0.13854107912629843}}
{"model": "mlx-community/gemma-3-12b-pt-8bit", "success": false, "error_stage": "Error", "error_message": "Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/gemma-3-27b-it-qat-4bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["bullets(23)"], "metrics": {"prompt_tokens": 487, "generation_tokens": 275, "generation_tps": 28.31649832606586, "peak_memory_gb": 19.384174038, "active_memory_gb": 15.6859171949327, "cache_memory_gb": 0.07883227616548538}}
{"model": "mlx-community/gemma-3-27b-it-qat-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["bullets(24)"], "metrics": {"prompt_tokens": 487, "generation_tokens": 266, "generation_tps": 15.557486821943419, "peak_memory_gb": 33.593509348, "active_memory_gb": 28.91939192637801, "cache_memory_gb": 0.04161742888391018}}
{"model": "mlx-community/gemma-3n-E4B-it-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["verbose"], "metrics": {"prompt_tokens": 487, "generation_tokens": 500, "generation_tps": 42.0794334727945, "peak_memory_gb": 17.198953383, "active_memory_gb": 14.627546395175159, "cache_memory_gb": 0.01420687884092331}}
{"model": "mlx-community/llava-v1.6-mistral-7b-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 2516, "generation_tokens": 8, "generation_tps": 49.11092617817805, "peak_memory_gb": 11.935844616, "active_memory_gb": 7.4945546425879, "cache_memory_gb": 0.47387705370783806}}
{"model": "mlx-community/paligemma2-10b-ft-docci-448-6bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 1243, "generation_tokens": 98, "generation_tps": 42.36630435757782, "peak_memory_gb": 11.252956606, "active_memory_gb": 7.326527765020728, "cache_memory_gb": 0.48776828311383724}}
{"model": "mlx-community/paligemma2-10b-ft-docci-448-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 1243, "generation_tokens": 70, "generation_tps": 5.092149445932505, "peak_memory_gb": 26.899904102, "active_memory_gb": 18.008165976032615, "cache_memory_gb": 4.77590649202466}}
{"model": "mlx-community/paligemma2-3b-ft-docci-448-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 1243, "generation_tokens": 151, "generation_tps": 18.24832688657862, "peak_memory_gb": 11.285198486, "active_memory_gb": 5.6590515580028296, "cache_memory_gb": 3.054322198033333}}
{"model": "mlx-community/paligemma2-3b-pt-896-4bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 4315, "generation_tokens": 15, "generation_tps": 119.07372547829233, "peak_memory_gb": 8.571636018, "active_memory_gb": 1.609035661444068, "cache_memory_gb": 0.5071406057104468}}
{"model": "mlx-community/pixtral-12b-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["verbose", "bullets(20)"], "metrics": {"prompt_tokens": 3237, "generation_tokens": 349, "generation_tps": 32.70497494563184, "peak_memory_gb": 15.59443899, "active_memory_gb": 12.562027152627707, "cache_memory_gb": 0.09859619475901127}}
{"model": "mlx-community/pixtral-12b-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["verbose", "bullets(19)"], "metrics": {"prompt_tokens": 3237, "generation_tokens": 333, "generation_tps": 18.865496015516833, "peak_memory_gb": 27.450425742, "active_memory_gb": 23.634658988565207, "cache_memory_gb": 0.03760038502514362}}
{"model": "prince-canuma/Florence-2-large-ft", "success": false, "error_stage": "Error", "error_message": "Model loading failed: RobertaTokenizer has no attribute additional_special_tokens", "quality_issues": [], "metrics": {}}
{"model": "qnguyen3/nanoLLaVA", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 236, "generation_tokens": 58, "generation_tps": 106.63809902862499, "peak_memory_gb": 4.45543057, "active_memory_gb": 1.967827020213008, "cache_memory_gb": 1.3555342257022858}}
