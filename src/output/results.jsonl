{"model": "HuggingFaceTB/SmolVLM-Instruct", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 1347, "generation_tokens": 13, "generation_tps": 121.3245373362674, "peak_memory_gb": 5.584378156, "active_memory_gb": 4.184753889217973, "cache_memory_gb": 0.11892867647111416}}
{"model": "Qwen/Qwen3-VL-2B-Instruct", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["bullets(17)"], "metrics": {"prompt_tokens": 16390, "generation_tokens": 232, "generation_tps": 71.70734255427737, "peak_memory_gb": 12.550995284, "active_memory_gb": 3.9632408153265715, "cache_memory_gb": 0.6923341806977987}}
{"model": "meta-llama/Llama-3.2-11B-Vision-Instruct", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["bullets(34)"], "metrics": {"prompt_tokens": 161, "generation_tokens": 252, "generation_tps": 3.883233747506817, "peak_memory_gb": 25.12129871, "active_memory_gb": 19.875752087682486, "cache_memory_gb": 3.0080887246876955}}
{"model": "microsoft/Phi-3.5-vision-instruct", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["repetitive(phrase: \"# instruction # solution...\")", "lang_mixing"], "metrics": {"prompt_tokens": 943, "generation_tokens": 500, "generation_tps": 11.06002556993276, "peak_memory_gb": 11.28572586, "active_memory_gb": 7.724663311615586, "cache_memory_gb": 1.3822022322565317}}
{"model": "mlx-community/Apriel-1.5-15b-Thinker-6bit-MLX", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 3045, "generation_tokens": 500, "generation_tps": 36.101108988837204, "peak_memory_gb": 14.687289554, "active_memory_gb": 11.69644170999527, "cache_memory_gb": 0.13376865442842245}}
{"model": "mlx-community/Idefics3-8B-Llama3-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["formatting", "context-ignored"], "metrics": {"prompt_tokens": 2471, "generation_tokens": 161, "generation_tps": 29.630891574261607, "peak_memory_gb": 18.55323005, "active_memory_gb": 15.763950854539871, "cache_memory_gb": 0.06627380196005106}}
{"model": "mlx-community/InternVL3-14B-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 1956, "generation_tokens": 149, "generation_tps": 29.009582203092442, "peak_memory_gb": 17.825859226, "active_memory_gb": 15.226991884410381, "cache_memory_gb": 0.08001076988875866}}
{"model": "mlx-community/Kimi-VL-A3B-Thinking-2506-bf16", "success": false, "error_stage": "Lib Version", "error_message": "Model loading failed: cannot import name '_validate_images_text_input_order' from 'transformers.processing_utils' (/opt/homebrew/Caskroom/miniconda/base/envs/mlx-vlm/lib/python3.13/site-packages/transformers/processing_utils.py)", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Kimi-VL-A3B-Thinking-8bit", "success": false, "error_stage": "Lib Version", "error_message": "Model loading failed: cannot import name '_validate_images_text_input_order' from 'transformers.processing_utils' (/opt/homebrew/Caskroom/miniconda/base/envs/mlx-vlm/lib/python3.13/site-packages/transformers/processing_utils.py)", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/LFM2-VL-1.6B-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 1952, "generation_tokens": 90, "generation_tps": 291.315717282758, "peak_memory_gb": 50.646456584, "active_memory_gb": 1.9181767757982016, "cache_memory_gb": 0.06972726993262768}}
{"model": "mlx-community/Llama-3.2-11B-Vision-Instruct-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 160, "generation_tokens": 214, "generation_tps": 9.355165202237945, "peak_memory_gb": 15.120744626, "active_memory_gb": 10.569544088095427, "cache_memory_gb": 0.4425989557057619}}
{"model": "mlx-community/Molmo-7B-D-0924-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 1357, "generation_tokens": 162, "generation_tps": 45.96262222544381, "peak_memory_gb": 41.097422722, "active_memory_gb": 8.418860502541065, "cache_memory_gb": 0.046480217948555946}}
{"model": "mlx-community/Molmo-7B-D-0924-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 1357, "generation_tokens": 190, "generation_tps": 28.4976804699515, "peak_memory_gb": 47.704663844, "active_memory_gb": 14.943259308114648, "cache_memory_gb": 0.032658206298947334}}
{"model": "mlx-community/Phi-3.5-vision-instruct-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 943, "generation_tokens": 190, "generation_tps": 11.157613701232782, "peak_memory_gb": 11.073991348, "active_memory_gb": 7.72669274546206, "cache_memory_gb": 1.370479667559266}}
{"model": "mlx-community/Qwen2-VL-2B-Instruct-4bit", "success": false, "error_stage": "OOM", "error_message": "Model runtime error during generation for mlx-community/Qwen2-VL-2B-Instruct-4bit: [metal::malloc] Attempting to allocate 134767706112 bytes which is greater than the maximum allowed buffer size of 86586540032 bytes.", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Qwen3-VL-2B-Thinking-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 16392, "generation_tokens": 500, "generation_tps": 72.165570044561, "peak_memory_gb": 12.554730876, "active_memory_gb": 3.9661095049232244, "cache_memory_gb": 0.6972167212516069}}
{"model": "mlx-community/SmolVLM-Instruct-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 1347, "generation_tokens": 11, "generation_tps": 127.80276549057868, "peak_memory_gb": 5.587884388, "active_memory_gb": 4.188019322231412, "cache_memory_gb": 0.11892867647111416}}
{"model": "mlx-community/SmolVLM2-2.2B-Instruct-mlx", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 1247, "generation_tokens": 15, "generation_tps": 125.41321035787736, "peak_memory_gb": 5.550772672, "active_memory_gb": 4.189072178676724, "cache_memory_gb": 0.10525520332157612}}
{"model": "mlx-community/deepseek-vl2-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["repetitive(phrase: \"the image is a...\")", "hallucination", "context-ignored"], "metrics": {"prompt_tokens": 1568, "generation_tokens": 500, "generation_tps": 65.77048967292117, "peak_memory_gb": 31.456690673, "active_memory_gb": 27.32607844658196, "cache_memory_gb": 0.1377027416601777}}
{"model": "mlx-community/gemma-3-12b-pt-8bit", "success": false, "error_stage": "Error", "error_message": "Cannot use apply_chat_template because this processor does not have a chat template.", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/gemma-3-27b-it-qat-4bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["bullets(19)"], "metrics": {"prompt_tokens": 425, "generation_tokens": 268, "generation_tps": 28.34925576228632, "peak_memory_gb": 18.111086486, "active_memory_gb": 15.028950031846762, "cache_memory_gb": 0.07603853195905685}}
{"model": "mlx-community/gemma-3-27b-it-qat-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["bullets(21)"], "metrics": {"prompt_tokens": 425, "generation_tokens": 264, "generation_tps": 15.581430407701914, "peak_memory_gb": 31.644155894, "active_memory_gb": 27.60602217540145, "cache_memory_gb": 0.04047163389623165}}
{"model": "mlx-community/gemma-3n-E4B-it-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 425, "generation_tokens": 171, "generation_tps": 42.38243699103801, "peak_memory_gb": 17.773058433, "active_memory_gb": 14.62699647527188, "cache_memory_gb": 0.014387167990207672}}
{"model": "mlx-community/llava-v1.6-mistral-7b-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 2325, "generation_tokens": 13, "generation_tps": 45.628510854801284, "peak_memory_gb": 12.246567176, "active_memory_gb": 7.49400532618165, "cache_memory_gb": 0.47430429980158806}}
{"model": "mlx-community/paligemma2-10b-ft-docci-448-6bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 1181, "generation_tokens": 67, "generation_tps": 42.75948325228455, "peak_memory_gb": 11.193297948, "active_memory_gb": 7.325978448614478, "cache_memory_gb": 0.45686504803597927}}
{"model": "mlx-community/paligemma2-10b-ft-docci-448-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 1181, "generation_tokens": 87, "generation_tps": 5.037720304563254, "peak_memory_gb": 26.621699274, "active_memory_gb": 18.007616659626365, "cache_memory_gb": 4.77208760753274}}
{"model": "mlx-community/paligemma2-3b-ft-docci-448-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 1181, "generation_tokens": 114, "generation_tps": 17.82603281380532, "peak_memory_gb": 11.221001478, "active_memory_gb": 5.6585022415965796, "cache_memory_gb": 3.0544400811195374}}
{"model": "mlx-community/paligemma2-3b-pt-896-4bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["repetitive('Doggy'", ")", "context-ignored"], "metrics": {"prompt_tokens": 4253, "generation_tokens": 500, "generation_tps": 110.90995333469343, "peak_memory_gb": 8.632502602, "active_memory_gb": 1.608486345037818, "cache_memory_gb": 0.5467075724154711}}
{"model": "mlx-community/pixtral-12b-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 2954, "generation_tokens": 173, "generation_tps": 34.33399394866312, "peak_memory_gb": 15.470133582, "active_memory_gb": 12.561477836221457, "cache_memory_gb": 0.08653912879526615}}
{"model": "mlx-community/pixtral-12b-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 2954, "generation_tokens": 181, "generation_tps": 19.28154872161608, "peak_memory_gb": 27.35928155, "active_memory_gb": 23.634109672158957, "cache_memory_gb": 0.035184452310204506}}
{"model": "prince-canuma/Florence-2-large-ft", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["lang_mixing", "formatting", "context-ignored"], "metrics": {"prompt_tokens": 154, "generation_tokens": 500, "generation_tps": 325.75218418662854, "peak_memory_gb": 5.15047482, "active_memory_gb": 3.076001353561878, "cache_memory_gb": 0.16681807860732079}}
{"model": "qnguyen3/nanoLLaVA", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 173, "generation_tokens": 75, "generation_tps": 103.08369726562407, "peak_memory_gb": 4.512439818, "active_memory_gb": 1.9669466707855463, "cache_memory_gb": 1.3555732034146786}}
