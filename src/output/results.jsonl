{"model": "HuggingFaceTB/SmolVLM-Instruct", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 1347, "generation_tokens": 11, "generation_tps": 128.22685947671192, "peak_memory_gb": 5.584378156, "active_memory_gb": 4.184753889217973, "cache_memory_gb": 0.11892867647111416}}
{"model": "Qwen/Qwen3-VL-2B-Instruct", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["bullets(17)"], "metrics": {"prompt_tokens": 16390, "generation_tokens": 232, "generation_tps": 73.79189333195764, "peak_memory_gb": 12.550995284, "active_memory_gb": 3.9632408153265715, "cache_memory_gb": 0.6923342403024435}}
{"model": "meta-llama/Llama-3.2-11B-Vision-Instruct", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["repetitive(phrase: \"* dog * pet...\")", "bullets(138)"], "metrics": {"prompt_tokens": 161, "generation_tokens": 500, "generation_tps": 3.815301992330628, "peak_memory_gb": 25.18735695, "active_memory_gb": 19.875752087682486, "cache_memory_gb": 3.0100399423390627}}
{"model": "microsoft/Florence-2-large-ft", "success": false, "error_stage": "Model Error", "error_message": "Model loading failed: Missing 1 parameters: \nlanguage_model.lm_head.weight.", "quality_issues": [], "metrics": {}}
{"model": "microsoft/Phi-3.5-vision-instruct", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["lang_mixing"], "metrics": {"prompt_tokens": 943, "generation_tokens": 500, "generation_tps": 11.14838286980427, "peak_memory_gb": 11.28572586, "active_memory_gb": 7.724663311615586, "cache_memory_gb": 1.3822022322565317}}
{"model": "mlx-community/Apriel-1.5-15b-Thinker-6bit-MLX", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 3045, "generation_tokens": 500, "generation_tps": 32.18497926390493, "peak_memory_gb": 14.687289554, "active_memory_gb": 11.69644170999527, "cache_memory_gb": 0.13377032335847616}}
{"model": "mlx-community/Idefics3-8B-Llama3-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["formatting", "context-ignored"], "metrics": {"prompt_tokens": 2471, "generation_tokens": 141, "generation_tps": 29.697267164164852, "peak_memory_gb": 18.55323005, "active_memory_gb": 15.763950854539871, "cache_memory_gb": 0.06627380196005106}}
{"model": "mlx-community/InternVL3-14B-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 1956, "generation_tokens": 124, "generation_tps": 26.37650385286846, "peak_memory_gb": 17.87994281, "active_memory_gb": 15.226991884410381, "cache_memory_gb": 0.07999551109969616}}
{"model": "mlx-community/Kimi-VL-A3B-Thinking-2506-bf16", "success": false, "error_stage": "Lib Version", "error_message": "Model loading failed: cannot import name '_validate_images_text_input_order' from 'transformers.processing_utils' (/opt/homebrew/Caskroom/miniconda/base/envs/mlx-vlm/lib/python3.13/site-packages/transformers/processing_utils.py)", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Kimi-VL-A3B-Thinking-8bit", "success": false, "error_stage": "Lib Version", "error_message": "Model loading failed: cannot import name '_validate_images_text_input_order' from 'transformers.processing_utils' (/opt/homebrew/Caskroom/miniconda/base/envs/mlx-vlm/lib/python3.13/site-packages/transformers/processing_utils.py)", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/LFM2-VL-1.6B-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 1952, "generation_tokens": 16, "generation_tps": 301.8946247678898, "peak_memory_gb": 50.64646358, "active_memory_gb": 1.9181767757982016, "cache_memory_gb": 0.06972464732825756}}
{"model": "mlx-community/Llama-3.2-11B-Vision-Instruct-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 160, "generation_tokens": 186, "generation_tps": 8.064832625801275, "peak_memory_gb": 15.120744626, "active_memory_gb": 10.569544088095427, "cache_memory_gb": 0.4425989557057619}}
{"model": "mlx-community/Molmo-7B-D-0924-8bit", "success": false, "error_stage": "Missing Dep", "error_message": "Model loading failed: This modeling file requires the following packages that were not found in your environment: tensorflow. Run `pip install tensorflow`", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Molmo-7B-D-0924-bf16", "success": false, "error_stage": "Missing Dep", "error_message": "Model loading failed: This modeling file requires the following packages that were not found in your environment: tensorflow. Run `pip install tensorflow`", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Phi-3.5-vision-instruct-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 943, "generation_tokens": 229, "generation_tps": 11.221071028069426, "peak_memory_gb": 36.152322736, "active_memory_gb": 31.082693642005324, "cache_memory_gb": 1.3704949263483286}}
{"model": "mlx-community/Qwen2-VL-2B-Instruct-4bit", "success": false, "error_stage": "OOM", "error_message": "Model runtime error during generation for mlx-community/Qwen2-VL-2B-Instruct-4bit: [metal::malloc] Attempting to allocate 134767706112 bytes which is greater than the maximum allowed buffer size of 86586540032 bytes.", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Qwen3-VL-2B-Thinking-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 16392, "generation_tokens": 500, "generation_tps": 69.71255780911646, "peak_memory_gb": 12.55410828, "active_memory_gb": 3.965529667213559, "cache_memory_gb": 0.6972167212516069}}
{"model": "mlx-community/SmolVLM-Instruct-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 1347, "generation_tokens": 11, "generation_tps": 125.49126982203505, "peak_memory_gb": 5.587261792, "active_memory_gb": 4.187439484521747, "cache_memory_gb": 0.11892867647111416}}
{"model": "mlx-community/SmolVLM2-2.2B-Instruct-mlx", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 1247, "generation_tokens": 15, "generation_tps": 124.59657599252398, "peak_memory_gb": 5.550150076, "active_memory_gb": 4.188492340967059, "cache_memory_gb": 0.10525520332157612}}
{"model": "mlx-community/deepseek-vl2-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 1568, "generation_tokens": 30, "generation_tps": 69.15329752811036, "peak_memory_gb": 31.465480117, "active_memory_gb": 27.325498627498746, "cache_memory_gb": 0.11704859975725412}}
{"model": "mlx-community/gemma-3-12b-pt-8bit", "success": false, "error_stage": "Error", "error_message": "Cannot use apply_chat_template because this processor does not have a chat template.", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/gemma-3-27b-it-qat-4bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["bullets(17)"], "metrics": {"prompt_tokens": 425, "generation_tokens": 257, "generation_tps": 25.037576358086987, "peak_memory_gb": 18.11046391, "active_memory_gb": 15.028370212763548, "cache_memory_gb": 0.12626826483756304}}
{"model": "mlx-community/gemma-3-27b-it-qat-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["bullets(20)"], "metrics": {"prompt_tokens": 425, "generation_tokens": 251, "generation_tps": 14.876667169272675, "peak_memory_gb": 31.614992304, "active_memory_gb": 27.605442356318235, "cache_memory_gb": 0.034610032103955746}}
{"model": "mlx-community/gemma-3n-E4B-it-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 425, "generation_tokens": 167, "generation_tps": 42.0846735375917, "peak_memory_gb": 17.772435857, "active_memory_gb": 14.626416656188667, "cache_memory_gb": 0.014390029013156891}}
{"model": "mlx-community/llava-v1.6-mistral-7b-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 2325, "generation_tokens": 13, "generation_tps": 45.92266358043291, "peak_memory_gb": 12.2459446, "active_memory_gb": 7.493425507098436, "cache_memory_gb": 0.47430429980158806}}
{"model": "mlx-community/paligemma2-10b-ft-docci-448-6bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 1181, "generation_tokens": 79, "generation_tps": 43.24308741963504, "peak_memory_gb": 11.192675352, "active_memory_gb": 7.325398610904813, "cache_memory_gb": 0.45686504803597927}}
{"model": "mlx-community/paligemma2-10b-ft-docci-448-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 1181, "generation_tokens": 101, "generation_tps": 4.992062538271387, "peak_memory_gb": 26.835018962, "active_memory_gb": 18.0070368219167, "cache_memory_gb": 4.776009116321802}}
{"model": "mlx-community/paligemma2-3b-ft-docci-448-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 1181, "generation_tokens": 135, "generation_tps": 17.976045222802643, "peak_memory_gb": 11.220379906, "active_memory_gb": 5.657922403886914, "cache_memory_gb": 3.0544400811195374}}
{"model": "mlx-community/paligemma2-3b-pt-896-4bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["repetitive('Doggy'", ")", "context-ignored"], "metrics": {"prompt_tokens": 4253, "generation_tokens": 500, "generation_tps": 102.75887806556577, "peak_memory_gb": 8.631880006, "active_memory_gb": 1.6079065073281527, "cache_memory_gb": 0.5467090029269457}}
{"model": "mlx-community/pixtral-12b-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 2954, "generation_tokens": 163, "generation_tps": 33.76957110477055, "peak_memory_gb": 15.469510986, "active_memory_gb": 12.560897998511791, "cache_memory_gb": 0.08653912879526615}}
{"model": "mlx-community/pixtral-12b-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["bullets(17)"], "metrics": {"prompt_tokens": 2954, "generation_tokens": 177, "generation_tps": 18.37912267402009, "peak_memory_gb": 27.358658954, "active_memory_gb": 23.63352983444929, "cache_memory_gb": 0.035184452310204506}}
{"model": "prince-canuma/Florence-2-large-ft", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["lang_mixing", "formatting", "context-ignored"], "metrics": {"prompt_tokens": 154, "generation_tokens": 500, "generation_tps": 325.3938058282065, "peak_memory_gb": 5.149852224, "active_memory_gb": 3.075421515852213, "cache_memory_gb": 0.16681807860732079}}
{"model": "qnguyen3/nanoLLaVA", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 173, "generation_tokens": 73, "generation_tps": 104.58367826112631, "peak_memory_gb": 4.51552463, "active_memory_gb": 1.966366833075881, "cache_memory_gb": 1.3555727265775204}}
