{"model": "HuggingFaceTB/SmolVLM-Instruct", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 1514, "generation_tokens": 93, "generation_tps": 114.08810751443686, "peak_memory_gb": 5.48735459, "active_memory_gb": 4.184753889217973, "cache_memory_gb": 0.12529349140822887}}
{"model": "Qwen/Qwen3-VL-2B-Instruct", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["bullets(20)"], "metrics": {"prompt_tokens": 16546, "generation_tokens": 217, "generation_tps": 68.75019991800956, "peak_memory_gb": 12.404653396, "active_memory_gb": 3.9632408153265715, "cache_memory_gb": 0.6971600409597158}}
{"model": "meta-llama/Llama-3.2-11B-Vision-Instruct", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 306, "generation_tokens": 151, "generation_tps": 3.8099814278234514, "peak_memory_gb": 25.191494718, "active_memory_gb": 19.875752087682486, "cache_memory_gb": 3.008095206692815}}
{"model": "microsoft/Florence-2-large-ft", "success": false, "error_stage": "Model Error", "error_message": "Model loading failed: Missing 1 parameters: \nlanguage_model.lm_head.weight.", "quality_issues": [], "metrics": {}}
{"model": "microsoft/Phi-3.5-vision-instruct", "success": false, "error_stage": "Error", "error_message": "Model generation failed for microsoft/Phi-3.5-vision-instruct: Failed to process inputs with error: Phi3VProcessor.__call__() got an unexpected keyword argument 'padding_side'", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Apriel-1.5-15b-Thinker-6bit-MLX", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["verbose"], "metrics": {"prompt_tokens": 3225, "generation_tokens": 500, "generation_tps": 35.12108628220172, "peak_memory_gb": 14.767636686, "active_memory_gb": 11.69638067111373, "cache_memory_gb": 0.1422642432153225}}
{"model": "mlx-community/Devstral-Small-2-24B-Instruct-2512-5bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 2404, "generation_tokens": 255, "generation_tps": 27.413312240204018, "peak_memory_gb": 21.704214928, "active_memory_gb": 15.871880184859037, "cache_memory_gb": 0.08486633375287056}}
{"model": "mlx-community/GLM-4.6V-Flash-6bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["formatting"], "metrics": {"prompt_tokens": 6385, "generation_tokens": 500, "generation_tps": 47.31251677108053, "peak_memory_gb": 12.76552945, "active_memory_gb": 8.77819937467575, "cache_memory_gb": 0.1665454776957631}}
{"model": "mlx-community/Idefics3-8B-Llama3-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["formatting"], "metrics": {"prompt_tokens": 2616, "generation_tokens": 143, "generation_tps": 29.219165064895183, "peak_memory_gb": 18.54680755, "active_memory_gb": 15.764423873275518, "cache_memory_gb": 0.059776218608021736}}
{"model": "mlx-community/InternVL3-14B-8bit", "success": false, "error_stage": "Error", "error_message": "Model loading failed: Received a InternVLImageProcessor for argument image_processor, but a ImageProcessingMixin was expected.", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Kimi-VL-A3B-Thinking-2506-bf16", "success": false, "error_stage": "Lib Version", "error_message": "Model loading failed: cannot import name '_validate_images_text_input_order' from 'transformers.processing_utils' (/opt/homebrew/Caskroom/miniconda/base/lib/python3.13/site-packages/transformers/processing_utils.py)", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Kimi-VL-A3B-Thinking-8bit", "success": false, "error_stage": "Lib Version", "error_message": "Model loading failed: cannot import name '_validate_images_text_input_order' from 'transformers.processing_utils' (/opt/homebrew/Caskroom/miniconda/base/lib/python3.13/site-packages/transformers/processing_utils.py)", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/LFM2-VL-1.6B-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 2111, "generation_tokens": 171, "generation_tps": 291.0457274882382, "peak_memory_gb": 52.70399446, "active_memory_gb": 1.9179614800959826, "cache_memory_gb": 0.07536229304969311}}
{"model": "mlx-community/Llama-3.2-11B-Vision-Instruct-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 305, "generation_tokens": 124, "generation_tps": 8.777011070721933, "peak_memory_gb": 14.974353463, "active_memory_gb": 10.569727189838886, "cache_memory_gb": 0.44260752759873867}}
{"model": "mlx-community/Ministral-3-3B-Instruct-2512-4bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["verbose"], "metrics": {"prompt_tokens": 2969, "generation_tokens": 373, "generation_tps": 156.364960406891, "peak_memory_gb": 7.413111083, "active_memory_gb": 2.562248768284917, "cache_memory_gb": 0.2948396299034357}}
{"model": "mlx-community/Molmo-7B-D-0924-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 1513, "generation_tokens": 238, "generation_tps": 45.056712299738514, "peak_memory_gb": 40.711662462, "active_memory_gb": 8.419287744909525, "cache_memory_gb": 0.050875911489129066}}
{"model": "mlx-community/Molmo-7B-D-0924-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 1513, "generation_tokens": 245, "generation_tps": 28.136805434458815, "peak_memory_gb": 47.518788448, "active_memory_gb": 14.943686550483108, "cache_memory_gb": 0.032659368589520454}}
{"model": "mlx-community/Phi-3.5-vision-instruct-bf16", "success": false, "error_stage": "Error", "error_message": "Model loading failed: /Users/jrp/.cache/huggingface/hub/models--mlx-community--Phi-3.5-vision-instruct-bf16/snapshots/d8da684308c275a86659e2b36a9189b2f4aec8ea does not appear to have a file named image_processing_phi3_v.py. Checkout 'https://huggingface.co//Users/jrp/.cache/huggingface/hub/models--mlx-community--Phi-3.5-vision-instruct-bf16/snapshots/d8da684308c275a86659e2b36a9189b2f4aec8ea/tree/main' for available files.", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Qwen2-VL-2B-Instruct-4bit", "success": false, "error_stage": "OOM", "error_message": "Model runtime error during generation for mlx-community/Qwen2-VL-2B-Instruct-4bit: [metal::malloc] Attempting to allocate 134767706112 bytes which is greater than the maximum allowed buffer size of 86586540032 bytes.", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Qwen3-VL-2B-Thinking-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["verbose"], "metrics": {"prompt_tokens": 16548, "generation_tokens": 500, "generation_tps": 71.38864137498953, "peak_memory_gb": 12.408765816, "active_memory_gb": 3.966475712135434, "cache_memory_gb": 0.7020423132926226}}
{"model": "mlx-community/SmolVLM-Instruct-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["verbose"], "metrics": {"prompt_tokens": 1514, "generation_tokens": 500, "generation_tps": 112.82911086407087, "peak_memory_gb": 5.524006271, "active_memory_gb": 4.188385529443622, "cache_memory_gb": 0.13896536640822887}}
{"model": "mlx-community/SmolVLM2-2.2B-Instruct-mlx", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 1414, "generation_tokens": 167, "generation_tps": 115.29531931916299, "peak_memory_gb": 5.483748396, "active_memory_gb": 4.189438385888934, "cache_memory_gb": 0.1252920012921095}}
{"model": "mlx-community/deepseek-vl2-8bit", "success": false, "error_stage": "Error", "error_message": "Model runtime error during generation for mlx-community/deepseek-vl2-8bit: std::bad_cast", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/gemma-3-12b-pt-8bit", "success": false, "error_stage": "Error", "error_message": "Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/gemma-3-27b-it-qat-4bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["bullets(24)"], "metrics": {"prompt_tokens": 585, "generation_tokens": 373, "generation_tps": 27.91328999699479, "peak_memory_gb": 19.383717634, "active_memory_gb": 15.685489945113659, "cache_memory_gb": 0.08402245491743088}}
{"model": "mlx-community/gemma-3-27b-it-qat-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["bullets(27)"], "metrics": {"prompt_tokens": 585, "generation_tokens": 407, "generation_tps": 14.258845674660227, "peak_memory_gb": 33.59305294, "active_memory_gb": 28.91896467655897, "cache_memory_gb": 0.044122060760855675}}
{"model": "mlx-community/gemma-3n-E4B-it-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["verbose"], "metrics": {"prompt_tokens": 585, "generation_tokens": 474, "generation_tps": 41.56189104279973, "peak_memory_gb": 17.133418373, "active_memory_gb": 14.627118069678545, "cache_memory_gb": 0.014698304235935211}}
{"model": "mlx-community/llava-v1.6-mistral-7b-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 2517, "generation_tokens": 118, "generation_tps": 41.0291733503857, "peak_memory_gb": 11.977574664, "active_memory_gb": 7.49412739649415, "cache_memory_gb": 0.48559580370783806}}
{"model": "mlx-community/paligemma2-10b-ft-docci-448-6bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 1343, "generation_tokens": 90, "generation_tps": 41.16488936322286, "peak_memory_gb": 11.635623486, "active_memory_gb": 7.326100518926978, "cache_memory_gb": 0.487771263346076}}
{"model": "mlx-community/paligemma2-10b-ft-docci-448-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 1343, "generation_tokens": 102, "generation_tps": 4.886366265045025, "peak_memory_gb": 26.964591322, "active_memory_gb": 18.007738729938865, "cache_memory_gb": 4.775909472256899}}
{"model": "mlx-community/paligemma2-3b-ft-docci-448-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 1343, "generation_tokens": 99, "generation_tps": 17.686978665415282, "peak_memory_gb": 11.264952062, "active_memory_gb": 5.6586243119090796, "cache_memory_gb": 3.0543251782655716}}
{"model": "mlx-community/paligemma2-3b-pt-896-4bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 4415, "generation_tokens": 121, "generation_tps": 111.05454712491151, "peak_memory_gb": 8.78008965, "active_memory_gb": 1.608608415350318, "cache_memory_gb": 0.526657173410058}}
{"model": "mlx-community/pixtral-12b-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["verbose", "bullets(24)"], "metrics": {"prompt_tokens": 3127, "generation_tokens": 364, "generation_tps": 33.589761841606574, "peak_memory_gb": 15.540666702, "active_memory_gb": 12.561599906533957, "cache_memory_gb": 0.09218678809702396}}
{"model": "mlx-community/pixtral-12b-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["verbose", "bullets(24)"], "metrics": {"prompt_tokens": 3127, "generation_tokens": 350, "generation_tps": 18.897654376233803, "peak_memory_gb": 27.39778395, "active_memory_gb": 23.634231742471457, "cache_memory_gb": 0.03704953379929066}}
{"model": "prince-canuma/Florence-2-large-ft", "success": false, "error_stage": "Error", "error_message": "Model loading failed: RobertaTokenizer has no attribute additional_special_tokens", "quality_issues": [], "metrics": {}}
{"model": "qnguyen3/nanoLLaVA", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 329, "generation_tokens": 125, "generation_tps": 101.34588001593778, "peak_memory_gb": 4.536924782, "active_memory_gb": 1.967399774119258, "cache_memory_gb": 1.3828782476484776}}
