{"model": "HuggingFaceTB/SmolVLM-Instruct", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": [], "metrics": {"prompt_tokens": 1461, "generation_tokens": 361, "generation_tps": 112.6574844628629, "peak_memory_gb": 5.487353744, "active_memory_gb": 4.184753889217973, "cache_memory_gb": 0.1389645766466856}}
{"model": "Qwen/Qwen3-VL-2B-Instruct", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": ["verbose", "bullets(23)"], "metrics": {"prompt_tokens": 16541, "generation_tokens": 373, "generation_tps": 71.19870326577637, "peak_memory_gb": 12.40388336, "active_memory_gb": 3.9632408265024424, "cache_memory_gb": 0.7029578629881144}}
{"model": "meta-llama/Llama-3.2-11B-Vision-Instruct", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": ["bullets(20)"], "metrics": {"prompt_tokens": 262, "generation_tokens": 318, "generation_tps": 3.795232082089135, "peak_memory_gb": 25.239186438, "active_memory_gb": 19.875752087682486, "cache_memory_gb": 3.010059716179967}}
{"model": "microsoft/Florence-2-large-ft", "success": false, "error_stage": "Weight Mismatch", "error_message": "Model loading failed: Missing 1 parameters: \nlanguage_model.lm_head.weight.", "error_package": "mlx", "quality_issues": [], "metrics": {}}
{"model": "microsoft/Phi-3.5-vision-instruct", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": ["lang_mixing", "hallucination", "verbose"], "metrics": {"prompt_tokens": 1072, "generation_tokens": 500, "generation_tps": 10.92422471444291, "peak_memory_gb": 11.507915444, "active_memory_gb": 7.724663311615586, "cache_memory_gb": 1.3939229045063257}}
{"model": "mlx-community/Apriel-1.5-15b-Thinker-6bit-MLX", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": [], "metrics": {"prompt_tokens": 3321, "generation_tokens": 500, "generation_tps": 33.93566144245211, "peak_memory_gb": 14.814101714, "active_memory_gb": 11.69644170999527, "cache_memory_gb": 0.14263063296675682}}
{"model": "mlx-community/Devstral-Small-2-24B-Instruct-2512-5bit", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": [], "metrics": {"prompt_tokens": 2479, "generation_tokens": 251, "generation_tps": 25.402690200497055, "peak_memory_gb": 22.197930463, "active_memory_gb": 15.871941238641739, "cache_memory_gb": 0.08582777716219425}}
{"model": "mlx-community/GLM-4.6V-Flash-6bit", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": ["formatting"], "metrics": {"prompt_tokens": 6306, "generation_tokens": 500, "generation_tps": 46.07876930602365, "peak_memory_gb": 12.728305006, "active_memory_gb": 8.778260413557291, "cache_memory_gb": 0.16593487933278084}}
{"model": "mlx-community/Idefics3-8B-Llama3-bf16", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": ["formatting"], "metrics": {"prompt_tokens": 2573, "generation_tokens": 125, "generation_tps": 29.595395724321463, "peak_memory_gb": 18.526900994, "active_memory_gb": 15.764484912157059, "cache_memory_gb": 0.059776218608021736}}
{"model": "mlx-community/InternVL3-14B-8bit", "success": false, "error_stage": "Processor Error", "error_message": "Model loading failed: Received a InternVLImageProcessor for argument image_processor, but a ImageProcessingMixin was expected.", "error_package": "mlx-vlm", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Kimi-VL-A3B-Thinking-2506-bf16", "success": false, "error_stage": "Lib Version", "error_message": "Model loading failed: cannot import name '_validate_images_text_input_order' from 'transformers.processing_utils' (/opt/homebrew/Caskroom/miniconda/base/envs/mlx-vlm/lib/python3.13/site-packages/transformers/processing_utils.py)", "error_package": "mlx-vlm", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Kimi-VL-A3B-Thinking-8bit", "success": false, "error_stage": "Lib Version", "error_message": "Model loading failed: cannot import name '_validate_images_text_input_order' from 'transformers.processing_utils' (/opt/homebrew/Caskroom/miniconda/base/envs/mlx-vlm/lib/python3.13/site-packages/transformers/processing_utils.py)", "error_package": "mlx-vlm", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/LFM2-VL-1.6B-8bit", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": [], "metrics": {"prompt_tokens": 2064, "generation_tokens": 14, "generation_tps": 310.4737775187, "peak_memory_gb": 52.70406, "active_memory_gb": 1.9180225189775229, "cache_memory_gb": 0.07538518123328686}}
{"model": "mlx-community/LFM2.5-VL-1.6B-bf16", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": [], "metrics": {"prompt_tokens": 2064, "generation_tokens": 12, "generation_tps": 195.28111324161583, "peak_memory_gb": 4.44078759, "active_memory_gb": 2.9767780657857656, "cache_memory_gb": 0.03958147205412388}}
{"model": "mlx-community/Llama-3.2-11B-Vision-Instruct-8bit", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 261, "generation_tokens": 118, "generation_tps": 8.539525946663582, "peak_memory_gb": 14.948971263, "active_memory_gb": 10.569925557821989, "cache_memory_gb": 0.44262065552175045}}
{"model": "mlx-community/Ministral-3-3B-Instruct-2512-4bit", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": ["bullets(18)"], "metrics": {"prompt_tokens": 3044, "generation_tokens": 288, "generation_tps": 154.40418152867704, "peak_memory_gb": 7.906974074, "active_memory_gb": 2.5624471362680197, "cache_memory_gb": 0.2958163321018219}}
{"model": "mlx-community/Molmo-7B-D-0924-8bit", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": [], "metrics": {"prompt_tokens": 1476, "generation_tokens": 366, "generation_tps": 44.17297153863524, "peak_memory_gb": 40.694573658, "active_memory_gb": 8.419486112892628, "cache_memory_gb": 0.05527016706764698}}
{"model": "mlx-community/Molmo-7B-D-0924-bf16", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": [], "metrics": {"prompt_tokens": 1476, "generation_tokens": 349, "generation_tps": 27.893919555611646, "peak_memory_gb": 47.506451004, "active_memory_gb": 14.94388491846621, "cache_memory_gb": 0.03461191989481449}}
{"model": "mlx-community/Phi-3.5-vision-instruct-bf16", "success": false, "error_stage": "Config Missing", "error_message": "Model loading failed: /Users/jrp/.cache/huggingface/hub/models--mlx-community--Phi-3.5-vision-instruct-bf16/snapshots/d8da684308c275a86659e2b36a9189b2f4aec8ea does not appear to have a file named image_processing_phi3_v.py. Checkout 'https://huggingface.co//Users/jrp/.cache/huggingface/hub/models--mlx-community--Phi-3.5-vision-instruct-bf16/snapshots/d8da684308c275a86659e2b36a9189b2f4aec8ea/tree/main' for available files.", "error_package": "mlx-vlm", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Qwen2-VL-2B-Instruct-4bit", "success": false, "error_stage": "OOM", "error_message": "Model runtime error during generation for mlx-community/Qwen2-VL-2B-Instruct-4bit: [metal::malloc] Attempting to allocate 135433060352 bytes which is greater than the maximum allowed buffer size of 86586540032 bytes.", "error_package": "mlx", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Qwen3-VL-2B-Thinking-bf16", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": [], "metrics": {"prompt_tokens": 16543, "generation_tokens": 500, "generation_tps": 71.54316911342846, "peak_memory_gb": 12.408241544, "active_memory_gb": 3.966674091294408, "cache_memory_gb": 0.7029577810317278}}
{"model": "mlx-community/SmolVLM-Instruct-bf16", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": [], "metrics": {"prompt_tokens": 1461, "generation_tokens": 279, "generation_tps": 113.49678478130019, "peak_memory_gb": 5.524218403, "active_memory_gb": 4.188583897426724, "cache_memory_gb": 0.1252927016466856}}
{"model": "mlx-community/SmolVLM2-2.2B-Instruct-mlx", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": [], "metrics": {"prompt_tokens": 1361, "generation_tokens": 87, "generation_tps": 116.24115514130186, "peak_memory_gb": 5.483960544, "active_memory_gb": 4.189636753872037, "cache_memory_gb": 0.11166559346020222}}
{"model": "mlx-community/deepseek-vl2-8bit", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": ["refusal(uncertainty)", "repetitive(phrase: \"theuser:and theuser:and theuse...\")", "context-ignored"], "metrics": {"prompt_tokens": 2302, "generation_tokens": 500, "generation_tps": 61.12311679486209, "peak_memory_gb": 32.164834657, "active_memory_gb": 27.326643021777272, "cache_memory_gb": 0.16792610567063093}}
{"model": "mlx-community/gemma-3-12b-pt-8bit", "success": false, "error_stage": "No Chat Template", "error_message": "Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating", "error_package": "mlx-vlm", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/gemma-3-27b-it-qat-4bit", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": ["bullets(26)"], "metrics": {"prompt_tokens": 531, "generation_tokens": 378, "generation_tps": 27.804232630625048, "peak_memory_gb": 19.384191478, "active_memory_gb": 15.685932453721762, "cache_memory_gb": 0.08402124792337418}}
{"model": "mlx-community/gemma-3-27b-it-qat-8bit", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": ["bullets(25)"], "metrics": {"prompt_tokens": 531, "generation_tokens": 372, "generation_tps": 15.214226715507209, "peak_memory_gb": 33.593526784, "active_memory_gb": 28.919407185167074, "cache_memory_gb": 0.04412085376679897}}
{"model": "mlx-community/gemma-3n-E4B-it-bf16", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": ["verbose"], "metrics": {"prompt_tokens": 531, "generation_tokens": 310, "generation_tps": 39.88465315067922, "peak_memory_gb": 17.198963187, "active_memory_gb": 14.627560578286648, "cache_memory_gb": 0.014253638684749603}}
{"model": "mlx-community/llava-v1.6-mistral-7b-8bit", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 2565, "generation_tokens": 8, "generation_tps": 39.38770978171572, "peak_memory_gb": 11.954194696, "active_memory_gb": 7.494569901376963, "cache_memory_gb": 0.47387705370783806}}
{"model": "mlx-community/paligemma2-10b-ft-docci-448-6bit", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 1286, "generation_tokens": 84, "generation_tps": 41.565808757475544, "peak_memory_gb": 11.517690654, "active_memory_gb": 7.326543023809791, "cache_memory_gb": 0.4877695646136999}}
{"model": "mlx-community/paligemma2-10b-ft-docci-448-bf16", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 1286, "generation_tokens": 94, "generation_tps": 4.963415011168605, "peak_memory_gb": 26.906409914, "active_memory_gb": 18.008181234821677, "cache_memory_gb": 4.775907773524523}}
{"model": "mlx-community/paligemma2-3b-ft-docci-448-bf16", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 1286, "generation_tokens": 127, "generation_tps": 17.71370756966241, "peak_memory_gb": 11.206770654, "active_memory_gb": 5.659066816791892, "cache_memory_gb": 3.0543234795331955}}
{"model": "mlx-community/paligemma2-3b-pt-896-4bit", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 4358, "generation_tokens": 9, "generation_tps": 115.29272558524613, "peak_memory_gb": 8.692451634, "active_memory_gb": 1.6090509202331305, "cache_memory_gb": 0.5266574379056692}}
{"model": "mlx-community/pixtral-12b-8bit", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": ["verbose", "bullets(23)"], "metrics": {"prompt_tokens": 3223, "generation_tokens": 341, "generation_tps": 32.7813222740287, "peak_memory_gb": 15.587541326, "active_memory_gb": 12.562042411416769, "cache_memory_gb": 0.09255347587168217}}
{"model": "mlx-community/pixtral-12b-bf16", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": ["verbose", "bullets(27)"], "metrics": {"prompt_tokens": 3223, "generation_tokens": 308, "generation_tps": 18.308601192102408, "peak_memory_gb": 27.443675534, "active_memory_gb": 23.63467424735427, "cache_memory_gb": 0.037416936829686165}}
{"model": "prince-canuma/Florence-2-large-ft", "success": false, "error_stage": "Model Error", "error_message": "Model loading failed: RobertaTokenizer has no attribute additional_special_tokens", "error_package": "mlx-vlm", "quality_issues": [], "metrics": {}}
{"model": "qnguyen3/nanoLLaVA", "success": true, "error_stage": null, "error_message": null, "error_package": null, "quality_issues": [], "metrics": {"prompt_tokens": 284, "generation_tokens": 109, "generation_tps": 100.94573727465307, "peak_memory_gb": 4.491424742, "active_memory_gb": 1.9678422790020704, "cache_memory_gb": 1.3828876167535782}}
