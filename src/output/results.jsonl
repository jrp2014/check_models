{"model": "HuggingFaceTB/SmolVLM-Instruct", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 1429, "generation_tokens": 16, "generation_tps": 123.78000876179712, "peak_memory_gb": 5.486390792, "active_memory_gb": 4.184753889217973, "cache_memory_gb": 0.11880782805383205}}
{"model": "Qwen/Qwen3-VL-2B-Instruct", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["bullets(17)"], "metrics": {"prompt_tokens": 16567, "generation_tokens": 295, "generation_tps": 71.3996658011657, "peak_memory_gb": 12.414516572, "active_memory_gb": 3.963240822777152, "cache_memory_gb": 0.6994482595473528}}
{"model": "meta-llama/Llama-3.2-11B-Vision-Instruct", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 226, "generation_tokens": 215, "generation_tps": 3.849612405565255, "peak_memory_gb": 25.15464327, "active_memory_gb": 19.875752087682486, "cache_memory_gb": 3.0080916304141283}}
{"model": "microsoft/Florence-2-large-ft", "success": false, "error_stage": "Model Error", "error_message": "Model loading failed: Missing 1 parameters: \nlanguage_model.lm_head.weight.", "quality_issues": [], "metrics": {}}
{"model": "microsoft/Phi-3.5-vision-instruct", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["lang_mixing", "hallucination"], "metrics": {"prompt_tokens": 1035, "generation_tokens": 500, "generation_tps": 11.00415836415613, "peak_memory_gb": 11.291679808, "active_memory_gb": 7.724683305248618, "cache_memory_gb": 1.3822036031633615}}
{"model": "mlx-community/Apriel-1.5-15b-Thinker-6bit-MLX", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 3191, "generation_tokens": 500, "generation_tps": 35.9759933994785, "peak_memory_gb": 14.75166229, "active_memory_gb": 11.69644170999527, "cache_memory_gb": 0.14247226063162088}}
{"model": "mlx-community/GLM-4.6V-Flash-6bit", "success": false, "error_stage": "Error", "error_message": "Model generation failed for mlx-community/GLM-4.6V-Flash-6bit: Failed to process inputs with error: PreTrainedTokenizerFast._batch_encode_plus() got an unexpected keyword argument 'images'", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Idefics3-8B-Llama3-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["formatting"], "metrics": {"prompt_tokens": 2536, "generation_tokens": 237, "generation_tps": 29.35649691809119, "peak_memory_gb": 18.482254594, "active_memory_gb": 15.763950854539871, "cache_memory_gb": 0.06618230976164341}}
{"model": "mlx-community/InternVL3-14B-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 2033, "generation_tokens": 233, "generation_tps": 28.94364260145069, "peak_memory_gb": 17.85286069, "active_memory_gb": 15.226991884410381, "cache_memory_gb": 0.07979772053658962}}
{"model": "mlx-community/Kimi-VL-A3B-Thinking-2506-bf16", "success": false, "error_stage": "Lib Version", "error_message": "Model loading failed: cannot import name '_validate_images_text_input_order' from 'transformers.processing_utils' (/opt/homebrew/Caskroom/miniconda/base/envs/mlx-vlm/lib/python3.13/site-packages/transformers/processing_utils.py)", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Kimi-VL-A3B-Thinking-8bit", "success": false, "error_stage": "Lib Version", "error_message": "Model loading failed: cannot import name '_validate_images_text_input_order' from 'transformers.processing_utils' (/opt/homebrew/Caskroom/miniconda/base/envs/mlx-vlm/lib/python3.13/site-packages/transformers/processing_utils.py)", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/LFM2-VL-1.6B-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 2025, "generation_tokens": 14, "generation_tps": 315.06517909689165, "peak_memory_gb": 3.30410637, "active_memory_gb": 1.9181767757982016, "cache_memory_gb": 0.06944773904979229}}
{"model": "mlx-community/Llama-3.2-11B-Vision-Instruct-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 225, "generation_tokens": 179, "generation_tps": 8.611593700800483, "peak_memory_gb": 14.995181263, "active_memory_gb": 10.569544088095427, "cache_memory_gb": 0.4427258390933275}}
{"model": "mlx-community/Ministral-3-3B-Instruct-2512-4bit", "success": false, "error_stage": "Error", "error_message": "Model loading failed: Tokenizer class TokenizersBackend does not exist or is not currently imported.", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Molmo-7B-D-0924-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 1434, "generation_tokens": 234, "generation_tps": 45.55833824305902, "peak_memory_gb": 43.41936659, "active_memory_gb": 10.975442882627249, "cache_memory_gb": 0.05087532289326191}}
{"model": "mlx-community/Molmo-7B-D-0924-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 1434, "generation_tokens": 273, "generation_tps": 28.027526624413436, "peak_memory_gb": 47.491524844, "active_memory_gb": 14.943259308114648, "cache_memory_gb": 0.03271951712667942}}
{"model": "mlx-community/Phi-3.5-vision-instruct-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 1035, "generation_tokens": 224, "generation_tps": 11.063866179872575, "peak_memory_gb": 11.152733524, "active_memory_gb": 7.72669274546206, "cache_memory_gb": 1.3704962972551584}}
{"model": "mlx-community/Qwen2-VL-2B-Instruct-4bit", "success": false, "error_stage": "OOM", "error_message": "Model runtime error during generation for mlx-community/Qwen2-VL-2B-Instruct-4bit: [metal::malloc] Attempting to allocate 136434163712 bytes which is greater than the maximum allowed buffer size of 86586540032 bytes.", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Qwen3-VL-2B-Thinking-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 16569, "generation_tokens": 500, "generation_tps": 60.07395374037492, "peak_memory_gb": 12.418334084, "active_memory_gb": 3.966109512373805, "cache_memory_gb": 0.7043313402682543}}
{"model": "mlx-community/SmolVLM-Instruct-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 1429, "generation_tokens": 16, "generation_tps": 121.49365053240686, "peak_memory_gb": 5.489897024, "active_memory_gb": 4.188019322231412, "cache_memory_gb": 0.11880782805383205}}
{"model": "mlx-community/SmolVLM2-2.2B-Instruct-mlx", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 1329, "generation_tokens": 92, "generation_tps": 117.77726143579521, "peak_memory_gb": 5.491021536, "active_memory_gb": 4.189072178676724, "cache_memory_gb": 0.11880622990429401}}
{"model": "mlx-community/deepseek-vl2-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["repetitive(.)", "context-ignored"], "metrics": {"prompt_tokens": 2238, "generation_tokens": 11, "generation_tps": 69.71119759809618, "peak_memory_gb": 32.115926689, "active_memory_gb": 27.32607844658196, "cache_memory_gb": 0.13853951077908278}}
{"model": "mlx-community/gemma-3-12b-pt-8bit", "success": false, "error_stage": "Error", "error_message": "Cannot use apply_chat_template because this processor does not have a chat template.", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/gemma-3-27b-it-qat-4bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["bullets(27)"], "metrics": {"prompt_tokens": 507, "generation_tokens": 324, "generation_tps": 27.79557915666506, "peak_memory_gb": 18.678761398, "active_memory_gb": 15.028950031846762, "cache_memory_gb": 0.0838070884346962}}
{"model": "mlx-community/gemma-3-27b-it-qat-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["bullets(26)"], "metrics": {"prompt_tokens": 507, "generation_tokens": 359, "generation_tps": 15.353862697767902, "peak_memory_gb": 32.183289792, "active_memory_gb": 27.60602217540145, "cache_memory_gb": 0.044028764590620995}}
{"model": "mlx-community/gemma-3n-E4B-it-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 507, "generation_tokens": 217, "generation_tps": 41.72642941733219, "peak_memory_gb": 17.823221867, "active_memory_gb": 14.626996236853302, "cache_memory_gb": 0.013789854943752289}}
{"model": "mlx-community/llava-v1.6-mistral-7b-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 2416, "generation_tokens": 8, "generation_tps": 47.880956164369536, "peak_memory_gb": 11.939727624, "active_memory_gb": 7.49400532618165, "cache_memory_gb": 0.47387705370783806}}
{"model": "mlx-community/paligemma2-10b-ft-docci-448-6bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 1263, "generation_tokens": 87, "generation_tps": 41.62254571173621, "peak_memory_gb": 11.293568334, "active_memory_gb": 7.325978448614478, "cache_memory_gb": 0.48775005899369717}}
{"model": "mlx-community/paligemma2-10b-ft-docci-448-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["repetitive(phrase: \"the windows have a...\")", "context-ignored"], "metrics": {"prompt_tokens": 1263, "generation_tokens": 500, "generation_tps": 4.882269758715601, "peak_memory_gb": 27.102366198, "active_memory_gb": 18.007616659626365, "cache_memory_gb": 4.779817406088114}}
{"model": "mlx-community/paligemma2-3b-ft-docci-448-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["repetitive(phrase: \"the window has a...\")", "context-ignored"], "metrics": {"prompt_tokens": 1263, "generation_tokens": 500, "generation_tps": 17.510164997131834, "peak_memory_gb": 11.366035494, "active_memory_gb": 5.6585022415965796, "cache_memory_gb": 3.0582407414913177}}
{"model": "mlx-community/paligemma2-3b-pt-896-4bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 4335, "generation_tokens": 8, "generation_tps": 122.76733031450087, "peak_memory_gb": 8.601733426, "active_memory_gb": 1.608486345037818, "cache_memory_gb": 0.5070355031639338}}
{"model": "mlx-community/pixtral-12b-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["bullets(25)"], "metrics": {"prompt_tokens": 3100, "generation_tokens": 247, "generation_tps": 34.08257445417101, "peak_memory_gb": 15.527723342, "active_memory_gb": 12.561477836221457, "cache_memory_gb": 0.09236822463572025}}
{"model": "mlx-community/pixtral-12b-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["bullets(21)"], "metrics": {"prompt_tokens": 3100, "generation_tokens": 225, "generation_tps": 19.296168881653767, "peak_memory_gb": 27.385119118, "active_memory_gb": 23.634109672158957, "cache_memory_gb": 0.03527648188173771}}
{"model": "prince-canuma/Florence-2-large-ft", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["lang_mixing", "formatting", "context-ignored"], "metrics": {"prompt_tokens": 221, "generation_tokens": 500, "generation_tps": 327.5559649868597, "peak_memory_gb": 5.136254164, "active_memory_gb": 3.076001353561878, "cache_memory_gb": 0.17286105826497078}}
{"model": "qnguyen3/nanoLLaVA", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 250, "generation_tokens": 85, "generation_tps": 101.51382491411559, "peak_memory_gb": 4.469667762, "active_memory_gb": 1.9669466707855463, "cache_memory_gb": 1.3827698975801468}}
