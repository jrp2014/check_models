{"model": "HuggingFaceTB/SmolVLM-Instruct", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 1442, "generation_tokens": 140, "generation_tps": 115.81127973104176, "peak_memory_gb": 5.546143432, "active_memory_gb": 4.184753889217973, "cache_memory_gb": 0.11881124414503574}}
{"model": "Qwen/Qwen3-VL-2B-Instruct", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["verbose", "bullets(26)"], "metrics": {"prompt_tokens": 16519, "generation_tokens": 341, "generation_tps": 73.02380793121144, "peak_memory_gb": 12.599082336, "active_memory_gb": 3.9632408265024424, "cache_memory_gb": 0.6981322709470987}}
{"model": "meta-llama/Llama-3.2-11B-Vision-Instruct", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 240, "generation_tokens": 202, "generation_tps": 3.9217363950082182, "peak_memory_gb": 25.161820134, "active_memory_gb": 19.875752087682486, "cache_memory_gb": 3.0080922562628984}}
{"model": "microsoft/Florence-2-large-ft", "success": false, "error_stage": "Model Error", "error_message": "Model loading failed: Missing 1 parameters: \nlanguage_model.lm_head.weight.", "quality_issues": [], "metrics": {}}
{"model": "microsoft/Phi-3.5-vision-instruct", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["lang_mixing"], "metrics": {"prompt_tokens": 1054, "generation_tokens": 500, "generation_tps": 11.279121192844865, "peak_memory_gb": 11.506785648, "active_memory_gb": 7.724683305248618, "cache_memory_gb": 1.3939226362854242}}
{"model": "mlx-community/Apriel-1.5-15b-Thinker-6bit-MLX", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 3271, "generation_tokens": 500, "generation_tps": 34.803893648051954, "peak_memory_gb": 14.850490578, "active_memory_gb": 11.69644170999527, "cache_memory_gb": 0.14293057564646006}}
{"model": "mlx-community/Idefics3-8B-Llama3-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["formatting"], "metrics": {"prompt_tokens": 2550, "generation_tokens": 283, "generation_tps": 29.154487915427737, "peak_memory_gb": 18.520183522, "active_memory_gb": 15.763950854539871, "cache_memory_gb": 0.06920356303453445}}
{"model": "mlx-community/InternVL3-14B-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["bullets(16)"], "metrics": {"prompt_tokens": 2045, "generation_tokens": 212, "generation_tps": 28.58217160436331, "peak_memory_gb": 17.873930594, "active_memory_gb": 15.226991884410381, "cache_memory_gb": 0.08001143299043179}}
{"model": "mlx-community/Kimi-VL-A3B-Thinking-2506-bf16", "success": false, "error_stage": "Lib Version", "error_message": "Model loading failed: cannot import name '_validate_images_text_input_order' from 'transformers.processing_utils' (/opt/homebrew/Caskroom/miniconda/base/envs/mlx-vlm/lib/python3.13/site-packages/transformers/processing_utils.py)", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Kimi-VL-A3B-Thinking-8bit", "success": false, "error_stage": "Lib Version", "error_message": "Model loading failed: cannot import name '_validate_images_text_input_order' from 'transformers.processing_utils' (/opt/homebrew/Caskroom/miniconda/base/envs/mlx-vlm/lib/python3.13/site-packages/transformers/processing_utils.py)", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/LFM2-VL-1.6B-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 2042, "generation_tokens": 85, "generation_tps": 290.2891450395495, "peak_memory_gb": 21.206139964, "active_memory_gb": 18.52197322808206, "cache_memory_gb": 0.07099924422800541}}
{"model": "mlx-community/Llama-3.2-11B-Vision-Instruct-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["refusal(uncertainty)", "context-ignored"], "metrics": {"prompt_tokens": 239, "generation_tokens": 117, "generation_tps": 8.582475915530415, "peak_memory_gb": 17.830612142, "active_memory_gb": 10.569638829678297, "cache_memory_gb": 0.4426251519471407}}
{"model": "mlx-community/Ministral-3-3B-Instruct-2512-4bit", "success": false, "error_stage": "Error", "error_message": "Model loading failed: Tokenizer class TokenizersBackend does not exist or is not currently imported.", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Molmo-7B-D-0924-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 1446, "generation_tokens": 182, "generation_tps": 44.9587157990178, "peak_memory_gb": 40.67979505, "active_memory_gb": 8.418860502541065, "cache_memory_gb": 0.05101274140179157}}
{"model": "mlx-community/Molmo-7B-D-0924-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 1446, "generation_tokens": 273, "generation_tps": 27.780727314682842, "peak_memory_gb": 47.49553902, "active_memory_gb": 14.943259308114648, "cache_memory_gb": 0.032658571377396584}}
{"model": "mlx-community/Phi-3.5-vision-instruct-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 1054, "generation_tokens": 140, "generation_tps": 10.787231096500124, "peak_memory_gb": 11.096434472, "active_memory_gb": 7.72669274546206, "cache_memory_gb": 1.370496580377221}}
{"model": "mlx-community/Qwen2-VL-2B-Instruct-4bit", "success": false, "error_stage": "OOM", "error_message": "Model runtime error during generation for mlx-community/Qwen2-VL-2B-Instruct-4bit: [metal::malloc] Attempting to allocate 135433060352 bytes which is greater than the maximum allowed buffer size of 86586540032 bytes.", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Qwen3-VL-2B-Thinking-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 16521, "generation_tokens": 500, "generation_tps": 72.34448054029032, "peak_memory_gb": 12.602916232, "active_memory_gb": 3.9661095160990953, "cache_memory_gb": 0.7030150834470987}}
{"model": "mlx-community/SmolVLM-Instruct-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 1442, "generation_tokens": 366, "generation_tps": 115.21313657613459, "peak_memory_gb": 5.549649664, "active_memory_gb": 4.188019322231412, "cache_memory_gb": 0.14621221832931042}}
{"model": "mlx-community/SmolVLM2-2.2B-Instruct-mlx", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 1342, "generation_tokens": 70, "generation_tps": 118.39762354276772, "peak_memory_gb": 5.586770196, "active_memory_gb": 4.189072178676724, "cache_memory_gb": 0.11892849393188953}}
{"model": "mlx-community/deepseek-vl2-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 2253, "generation_tokens": 26, "generation_tps": 67.08901212343697, "peak_memory_gb": 32.153859578, "active_memory_gb": 27.32607846520841, "cache_memory_gb": 0.15102470852434635}}
{"model": "mlx-community/gemma-3-12b-pt-8bit", "success": false, "error_stage": "Error", "error_message": "Cannot use apply_chat_template because this processor does not have a chat template.", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/gemma-3-27b-it-qat-4bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["bullets(29)"], "metrics": {"prompt_tokens": 513, "generation_tokens": 338, "generation_tps": 28.30268968798081, "peak_memory_gb": 18.111088618, "active_memory_gb": 15.028950050473213, "cache_memory_gb": 0.08443044871091843}}
{"model": "mlx-community/gemma-3-27b-it-qat-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["bullets(24)"], "metrics": {"prompt_tokens": 513, "generation_tokens": 319, "generation_tps": 15.555658071943286, "peak_memory_gb": 31.644158026, "active_memory_gb": 27.6060221940279, "cache_memory_gb": 0.04434933327138424}}
{"model": "mlx-community/gemma-3n-E4B-it-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["verbose"], "metrics": {"prompt_tokens": 513, "generation_tokens": 311, "generation_tps": 42.17548330342841, "peak_memory_gb": 17.773060653, "active_memory_gb": 14.626996021717787, "cache_memory_gb": 0.014898397028446198}}
{"model": "mlx-community/llava-v1.6-mistral-7b-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 2531, "generation_tokens": 8, "generation_tps": 49.22096291218761, "peak_memory_gb": 12.293015836, "active_memory_gb": 7.494005344808102, "cache_memory_gb": 0.47430429980158806}}
{"model": "mlx-community/paligemma2-10b-ft-docci-448-6bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["repetitive(phrase: \"of the building. there...\")", "context-ignored"], "metrics": {"prompt_tokens": 1266, "generation_tokens": 500, "generation_tps": 41.62905596956145, "peak_memory_gb": 11.263624514, "active_memory_gb": 7.325978448614478, "cache_memory_gb": 0.5196104552596807}}
{"model": "mlx-community/paligemma2-10b-ft-docci-448-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 1266, "generation_tokens": 142, "generation_tps": 5.0013842299316025, "peak_memory_gb": 26.923084326, "active_memory_gb": 18.007616659626365, "cache_memory_gb": 4.776010382920504}}
{"model": "mlx-community/paligemma2-3b-ft-docci-448-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["repetitive(phrase: \"windows on the left...\")", "context-ignored"], "metrics": {"prompt_tokens": 1266, "generation_tokens": 500, "generation_tps": 17.853590099199074, "peak_memory_gb": 11.369310294, "active_memory_gb": 5.6585022415965796, "cache_memory_gb": 3.058378115296364}}
{"model": "mlx-community/paligemma2-3b-pt-896-4bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 4338, "generation_tokens": 169, "generation_tps": 111.55963254011998, "peak_memory_gb": 8.766310706, "active_memory_gb": 1.608486345037818, "cache_memory_gb": 0.527023097500205}}
{"model": "mlx-community/pixtral-12b-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["bullets(17)"], "metrics": {"prompt_tokens": 3180, "generation_tokens": 254, "generation_tps": 34.450341966605166, "peak_memory_gb": 15.624864078, "active_memory_gb": 12.561477836221457, "cache_memory_gb": 0.09276519156992435}}
{"model": "mlx-community/pixtral-12b-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["bullets(17)"], "metrics": {"prompt_tokens": 3180, "generation_tokens": 236, "generation_tps": 19.260621008506828, "peak_memory_gb": 27.42283507, "active_memory_gb": 23.634109672158957, "cache_memory_gb": 0.03750474192202091}}
{"model": "prince-canuma/Florence-2-large-ft", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["lang_mixing", "formatting", "context-ignored"], "metrics": {"prompt_tokens": 232, "generation_tokens": 500, "generation_tps": 321.01043814053975, "peak_memory_gb": 5.150475444, "active_memory_gb": 3.076001353561878, "cache_memory_gb": 0.17414287850260735}}
{"model": "qnguyen3/nanoLLaVA", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 262, "generation_tokens": 76, "generation_tps": 103.49549807904216, "peak_memory_gb": 4.497917134, "active_memory_gb": 1.9669466707855463, "cache_memory_gb": 1.382916808128357}}
