{"model": "HuggingFaceTB/SmolVLM-Instruct", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 1442, "generation_tokens": 48, "generation_tps": 118.53432310488266, "peak_memory_gb": 5.546143432, "active_memory_gb": 4.184753889217973, "cache_memory_gb": 0.11893009208142757}}
{"model": "Qwen/Qwen3-VL-2B-Instruct", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["verbose", "formatting", "bullets(34)"], "metrics": {"prompt_tokens": 16519, "generation_tokens": 500, "generation_tps": 71.91886803773468, "peak_memory_gb": 12.599082336, "active_memory_gb": 3.9632408265024424, "cache_memory_gb": 0.7030150834470987}}
{"model": "meta-llama/Llama-3.2-11B-Vision-Instruct", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 240, "generation_tokens": 166, "generation_tps": 3.793874284875667, "peak_memory_gb": 25.161820134, "active_memory_gb": 19.875752087682486, "cache_memory_gb": 3.0080922562628984}}
{"model": "microsoft/Florence-2-large-ft", "success": false, "error_stage": "Model Error", "error_message": "Model loading failed: Missing 1 parameters: \nlanguage_model.lm_head.weight.", "quality_issues": [], "metrics": {}}
{"model": "microsoft/Phi-3.5-vision-instruct", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["lang_mixing"], "metrics": {"prompt_tokens": 1054, "generation_tokens": 500, "generation_tps": 10.998209540966986, "peak_memory_gb": 11.506785648, "active_memory_gb": 7.724683305248618, "cache_memory_gb": 1.3939226362854242}}
{"model": "mlx-community/Apriel-1.5-15b-Thinker-6bit-MLX", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 3271, "generation_tokens": 500, "generation_tps": 31.768304435590004, "peak_memory_gb": 14.850490578, "active_memory_gb": 11.69644170999527, "cache_memory_gb": 0.14293057564646006}}
{"model": "mlx-community/Idefics3-8B-Llama3-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["formatting"], "metrics": {"prompt_tokens": 2550, "generation_tokens": 323, "generation_tps": 26.719963297068695, "peak_memory_gb": 18.593305314, "active_memory_gb": 15.763950854539871, "cache_memory_gb": 0.06920356303453445}}
{"model": "mlx-community/InternVL3-14B-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["bullets(17)"], "metrics": {"prompt_tokens": 2045, "generation_tokens": 213, "generation_tps": 26.555947760570216, "peak_memory_gb": 17.930471778, "active_memory_gb": 15.227007143199444, "cache_memory_gb": 0.07999617420136929}}
{"model": "mlx-community/Kimi-VL-A3B-Thinking-2506-bf16", "success": false, "error_stage": "Lib Version", "error_message": "Model loading failed: cannot import name '_validate_images_text_input_order' from 'transformers.processing_utils' (/opt/homebrew/Caskroom/miniconda/base/envs/mlx-vlm/lib/python3.13/site-packages/transformers/processing_utils.py)", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Kimi-VL-A3B-Thinking-8bit", "success": false, "error_stage": "Lib Version", "error_message": "Model loading failed: cannot import name '_validate_images_text_input_order' from 'transformers.processing_utils' (/opt/homebrew/Caskroom/miniconda/base/envs/mlx-vlm/lib/python3.13/site-packages/transformers/processing_utils.py)", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/LFM2-VL-1.6B-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 2042, "generation_tokens": 22, "generation_tps": 302.64366107208406, "peak_memory_gb": 21.206156348, "active_memory_gb": 18.521988486871123, "cache_memory_gb": 0.07548854500055313}}
{"model": "mlx-community/Llama-3.2-11B-Vision-Instruct-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 239, "generation_tokens": 153, "generation_tps": 7.151522103289395, "peak_memory_gb": 17.830628526, "active_memory_gb": 10.56965408846736, "cache_memory_gb": 0.4426251519471407}}
{"model": "mlx-community/Ministral-3-3B-Instruct-2512-4bit", "success": false, "error_stage": "Error", "error_message": "Model loading failed: Tokenizer class TokenizersBackend does not exist or is not currently imported.", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Molmo-7B-D-0924-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 1446, "generation_tokens": 171, "generation_tps": 44.82146950501791, "peak_memory_gb": 40.679811434, "active_memory_gb": 8.418875761330128, "cache_memory_gb": 0.05101274140179157}}
{"model": "mlx-community/Molmo-7B-D-0924-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 1446, "generation_tokens": 179, "generation_tps": 28.45389219621517, "peak_memory_gb": 47.495555404, "active_memory_gb": 14.94327456690371, "cache_memory_gb": 0.03265886940062046}}
{"model": "mlx-community/Phi-3.5-vision-instruct-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 1054, "generation_tokens": 163, "generation_tps": 11.29713759900177, "peak_memory_gb": 11.16795062, "active_memory_gb": 7.7267080042511225, "cache_memory_gb": 1.3704813215881586}}
{"model": "mlx-community/Qwen2-VL-2B-Instruct-4bit", "success": false, "error_stage": "OOM", "error_message": "Model runtime error during generation for mlx-community/Qwen2-VL-2B-Instruct-4bit: [metal::malloc] Attempting to allocate 135433060352 bytes which is greater than the maximum allowed buffer size of 86586540032 bytes.", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Qwen3-VL-2B-Thinking-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 16521, "generation_tokens": 500, "generation_tps": 72.55105018340234, "peak_memory_gb": 12.602932616, "active_memory_gb": 3.966124774888158, "cache_memory_gb": 0.7030150834470987}}
{"model": "mlx-community/SmolVLM-Instruct-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 1442, "generation_tokens": 227, "generation_tps": 115.80619591036341, "peak_memory_gb": 5.549666048, "active_memory_gb": 4.188034581020474, "cache_memory_gb": 0.13254034332931042}}
{"model": "mlx-community/SmolVLM2-2.2B-Instruct-mlx", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 1342, "generation_tokens": 20, "generation_tps": 122.7188980331641, "peak_memory_gb": 5.58678658, "active_memory_gb": 4.189087437465787, "cache_memory_gb": 0.11892849393188953}}
{"model": "mlx-community/deepseek-vl2-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["repetitive(phrase: \"and the answer", "and...\")", "context-ignored"], "metrics": {"prompt_tokens": 2253, "generation_tokens": 500, "generation_tps": 62.36075362836756, "peak_memory_gb": 32.140601396, "active_memory_gb": 27.326093705371022, "cache_memory_gb": 0.11391740664839745}}
{"model": "mlx-community/gemma-3-12b-pt-8bit", "success": false, "error_stage": "Error", "error_message": "Cannot use apply_chat_template because this processor does not have a chat template.", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/gemma-3-27b-it-qat-4bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["bullets(27)"], "metrics": {"prompt_tokens": 513, "generation_tokens": 325, "generation_tps": 26.618441783993994, "peak_memory_gb": 18.111104982, "active_memory_gb": 15.028965290635824, "cache_memory_gb": 0.08443283289670944}}
{"model": "mlx-community/gemma-3-27b-it-qat-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["bullets(26)"], "metrics": {"prompt_tokens": 513, "generation_tokens": 328, "generation_tps": 15.086141840902965, "peak_memory_gb": 31.615633376, "active_memory_gb": 27.60603743419051, "cache_memory_gb": 0.04434933327138424}}
{"model": "mlx-community/gemma-3n-E4B-it-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 513, "generation_tokens": 287, "generation_tps": 41.997125757502936, "peak_memory_gb": 17.773077017, "active_memory_gb": 14.627011261880398, "cache_memory_gb": 0.014896489679813385}}
{"model": "mlx-community/llava-v1.6-mistral-7b-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 2531, "generation_tokens": 8, "generation_tps": 46.4501781248115, "peak_memory_gb": 12.2930322, "active_memory_gb": 7.494020584970713, "cache_memory_gb": 0.47430429980158806}}
{"model": "mlx-community/paligemma2-10b-ft-docci-448-6bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 1266, "generation_tokens": 185, "generation_tps": 40.12080156491823, "peak_memory_gb": 11.263640898, "active_memory_gb": 7.325993707403541, "cache_memory_gb": 0.48823838494718075}}
{"model": "mlx-community/paligemma2-10b-ft-docci-448-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 1266, "generation_tokens": 129, "generation_tps": 4.824079501334192, "peak_memory_gb": 26.92310071, "active_memory_gb": 18.007631918415427, "cache_memory_gb": 4.776010382920504}}
{"model": "mlx-community/paligemma2-3b-ft-docci-448-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 1266, "generation_tokens": 113, "generation_tps": 16.65673343099787, "peak_memory_gb": 11.308461654, "active_memory_gb": 5.658517500385642, "cache_memory_gb": 3.054441347718239}}
{"model": "mlx-community/paligemma2-3b-pt-896-4bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["repetitive(phrase: \"walking along the sidewalk....\")", "context-ignored"], "metrics": {"prompt_tokens": 4338, "generation_tokens": 500, "generation_tps": 105.62661908993131, "peak_memory_gb": 8.76632709, "active_memory_gb": 1.6085016038268805, "cache_memory_gb": 0.5467074122279882}}
{"model": "mlx-community/pixtral-12b-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["bullets(19)"], "metrics": {"prompt_tokens": 3180, "generation_tokens": 246, "generation_tps": 29.66750535555033, "peak_memory_gb": 15.624880462, "active_memory_gb": 12.561493095010519, "cache_memory_gb": 0.09276519156992435}}
{"model": "mlx-community/pixtral-12b-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["bullets(18)"], "metrics": {"prompt_tokens": 3180, "generation_tokens": 226, "generation_tps": 16.444387501136283, "peak_memory_gb": 27.422851454, "active_memory_gb": 23.63412493094802, "cache_memory_gb": 0.03750474192202091}}
{"model": "prince-canuma/Florence-2-large-ft", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["lang_mixing", "formatting", "context-ignored"], "metrics": {"prompt_tokens": 232, "generation_tokens": 500, "generation_tps": 326.81243565767454, "peak_memory_gb": 5.150491828, "active_memory_gb": 3.0760166123509407, "cache_memory_gb": 0.17414287850260735}}
{"model": "qnguyen3/nanoLLaVA", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 262, "generation_tokens": 61, "generation_tps": 103.11424753739216, "peak_memory_gb": 4.477166562, "active_memory_gb": 1.9669619295746088, "cache_memory_gb": 1.3829296827316284}}
