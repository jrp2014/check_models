{"model": "HuggingFaceTB/SmolVLM-Instruct", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 1430, "generation_tokens": 9, "generation_tps": 130.63136012454515, "peak_memory_gb": 5.54614324, "active_memory_gb": 4.184753889217973, "cache_memory_gb": 0.11892991326749325}}
{"model": "Qwen/Qwen3-VL-2B-Instruct", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["verbose", "bullets(27)"], "metrics": {"prompt_tokens": 16503, "generation_tokens": 433, "generation_tps": 71.69346144239857, "peak_memory_gb": 12.593184072, "active_memory_gb": 3.9632408041507006, "cache_memory_gb": 0.7029540035873652}}
{"model": "meta-llama/Llama-3.2-11B-Vision-Instruct", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 237, "generation_tokens": 254, "generation_tps": 3.8483439539052102, "peak_memory_gb": 25.160296278, "active_memory_gb": 19.875752087682486, "cache_memory_gb": 3.0080921221524477}}
{"model": "microsoft/Florence-2-large-ft", "success": false, "error_stage": "Model Error", "error_message": "Model loading failed: Missing 1 parameters: \nlanguage_model.lm_head.weight.", "quality_issues": [], "metrics": {}}
{"model": "microsoft/Phi-3.5-vision-instruct", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["lang_mixing"], "metrics": {"prompt_tokens": 1042, "generation_tokens": 500, "generation_tps": 11.011348477419828, "peak_memory_gb": 11.50599394, "active_memory_gb": 7.724663311615586, "cache_memory_gb": 1.39392245747149}}
{"model": "mlx-community/Apriel-1.5-15b-Thinker-6bit-MLX", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 2933, "generation_tokens": 500, "generation_tps": 35.633726553284276, "peak_memory_gb": 14.715060434, "active_memory_gb": 11.69644170999527, "cache_memory_gb": 0.1332194721326232}}
{"model": "mlx-community/Idefics3-8B-Llama3-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["formatting"], "metrics": {"prompt_tokens": 2547, "generation_tokens": 227, "generation_tps": 28.87928985636131, "peak_memory_gb": 18.59189629, "active_memory_gb": 15.763950854539871, "cache_memory_gb": 0.06627387274056673}}
{"model": "mlx-community/InternVL3-14B-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["bullets(18)"], "metrics": {"prompt_tokens": 2032, "generation_tokens": 283, "generation_tps": 28.519285976209737, "peak_memory_gb": 17.922951418, "active_memory_gb": 15.226991884410381, "cache_memory_gb": 0.08683201484382153}}
{"model": "mlx-community/Kimi-VL-A3B-Thinking-2506-bf16", "success": false, "error_stage": "Lib Version", "error_message": "Model loading failed: cannot import name '_validate_images_text_input_order' from 'transformers.processing_utils' (/opt/homebrew/Caskroom/miniconda/base/envs/mlx-vlm/lib/python3.13/site-packages/transformers/processing_utils.py)", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Kimi-VL-A3B-Thinking-8bit", "success": false, "error_stage": "Lib Version", "error_message": "Model loading failed: cannot import name '_validate_images_text_input_order' from 'transformers.processing_utils' (/opt/homebrew/Caskroom/miniconda/base/envs/mlx-vlm/lib/python3.13/site-packages/transformers/processing_utils.py)", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/LFM2-VL-1.6B-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 2029, "generation_tokens": 141, "generation_tps": 289.7349329624166, "peak_memory_gb": 50.646456592, "active_memory_gb": 1.9181767757982016, "cache_memory_gb": 0.07556013576686382}}
{"model": "mlx-community/Llama-3.2-11B-Vision-Instruct-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 236, "generation_tokens": 171, "generation_tps": 9.205882661291799, "peak_memory_gb": 15.027003892, "active_memory_gb": 10.569544088095427, "cache_memory_gb": 0.4426023531705141}}
{"model": "mlx-community/Molmo-7B-D-0924-8bit", "success": false, "error_stage": "Missing Dep", "error_message": "Model loading failed: This modeling file requires the following packages that were not found in your environment: tensorflow. Run `pip install tensorflow`", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Molmo-7B-D-0924-bf16", "success": false, "error_stage": "Missing Dep", "error_message": "Model loading failed: This modeling file requires the following packages that were not found in your environment: tensorflow. Run `pip install tensorflow`", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Phi-3.5-vision-instruct-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 1042, "generation_tokens": 262, "generation_tps": 10.843460993123147, "peak_memory_gb": 33.374723014, "active_memory_gb": 7.726112907752395, "cache_memory_gb": 1.3822053465992212}}
{"model": "mlx-community/Qwen2-VL-2B-Instruct-4bit", "success": false, "error_stage": "OOM", "error_message": "Model runtime error during generation for mlx-community/Qwen2-VL-2B-Instruct-4bit: [metal::malloc] Attempting to allocate 135383101952 bytes which is greater than the maximum allowed buffer size of 86586540032 bytes.", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Qwen3-VL-2B-Thinking-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["verbose"], "metrics": {"prompt_tokens": 16505, "generation_tokens": 500, "generation_tps": 71.52985770747075, "peak_memory_gb": 12.596395372, "active_memory_gb": 3.9655296560376883, "cache_memory_gb": 0.7029542904347181}}
{"model": "mlx-community/SmolVLM-Instruct-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 1430, "generation_tokens": 9, "generation_tps": 129.72404159310642, "peak_memory_gb": 5.549026876, "active_memory_gb": 4.187439484521747, "cache_memory_gb": 0.11892991326749325}}
{"model": "mlx-community/SmolVLM2-2.2B-Instruct-mlx", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 1330, "generation_tokens": 26, "generation_tps": 122.0097590112811, "peak_memory_gb": 5.579462736, "active_memory_gb": 4.188492340967059, "cache_memory_gb": 0.11892831511795521}}
{"model": "mlx-community/deepseek-vl2-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 0, "generation_tokens": 0, "generation_tps": 0.0, "peak_memory_gb": 31.824548216, "active_memory_gb": 27.325254468247294, "cache_memory_gb": 0.5758320279419422}}
{"model": "mlx-community/gemma-3-12b-pt-8bit", "success": false, "error_stage": "Error", "error_message": "Cannot use apply_chat_template because this processor does not have a chat template.", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/gemma-3-27b-it-qat-4bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["bullets(31)"], "metrics": {"prompt_tokens": 504, "generation_tokens": 308, "generation_tps": 26.211945858085265, "peak_memory_gb": 18.110203642, "active_memory_gb": 15.028126053512096, "cache_memory_gb": 0.08400300145149231}}
{"model": "mlx-community/gemma-3-27b-it-qat-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["bullets(60)"], "metrics": {"prompt_tokens": 504, "generation_tokens": 500, "generation_tps": 14.980846631615144, "peak_memory_gb": 31.64327305, "active_memory_gb": 27.605198197066784, "cache_memory_gb": 0.04416602663695812}}
{"model": "mlx-community/gemma-3n-E4B-it-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 504, "generation_tokens": 168, "generation_tps": 42.0268113509495, "peak_memory_gb": 17.772175668, "active_memory_gb": 14.626172717660666, "cache_memory_gb": 0.014388933777809143}}
{"model": "mlx-community/llava-v1.6-mistral-7b-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 2321, "generation_tokens": 8, "generation_tps": 47.858840350825965, "peak_memory_gb": 12.28608538, "active_memory_gb": 7.493181347846985, "cache_memory_gb": 0.47430429980158806}}
{"model": "mlx-community/paligemma2-10b-ft-docci-448-6bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["repetitive(phrase: \"the tiles are wet...\")", "context-ignored"], "metrics": {"prompt_tokens": 1262, "generation_tokens": 500, "generation_tps": 39.73273839953034, "peak_memory_gb": 11.25282739, "active_memory_gb": 7.325154470279813, "cache_memory_gb": 0.519610395655036}}
{"model": "mlx-community/paligemma2-10b-ft-docci-448-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 1262, "generation_tokens": 115, "generation_tps": 5.019443487387072, "peak_memory_gb": 26.918087138, "active_memory_gb": 18.0067926812917, "cache_memory_gb": 4.776010323315859}}
{"model": "mlx-community/paligemma2-3b-ft-docci-448-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 1262, "generation_tokens": 177, "generation_tps": 17.895279403672863, "peak_memory_gb": 11.303447058, "active_memory_gb": 5.657678263261914, "cache_memory_gb": 3.054441288113594}}
{"model": "mlx-community/paligemma2-3b-pt-896-4bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 4334, "generation_tokens": 14, "generation_tps": 117.63474379157994, "peak_memory_gb": 8.75905259, "active_memory_gb": 1.6076623667031527, "cache_memory_gb": 0.507492309436202}}
{"model": "mlx-community/pixtral-12b-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["bullets(17)"], "metrics": {"prompt_tokens": 2842, "generation_tokens": 291, "generation_tps": 34.33336451375261, "peak_memory_gb": 15.494414666, "active_memory_gb": 12.560653857886791, "cache_memory_gb": 0.08598909713327885}}
{"model": "mlx-community/pixtral-12b-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["bullets(18)"], "metrics": {"prompt_tokens": 2842, "generation_tokens": 296, "generation_tps": 19.2308707450608, "peak_memory_gb": 27.302068618, "active_memory_gb": 23.63328569382429, "cache_memory_gb": 0.0346337053924799}}
{"model": "prince-canuma/Florence-2-large-ft", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["lang_mixing", "formatting", "context-ignored"], "metrics": {"prompt_tokens": 233, "generation_tokens": 500, "generation_tps": 328.27881544049035, "peak_memory_gb": 5.149590712, "active_memory_gb": 3.075177375227213, "cache_memory_gb": 0.17414288595318794}}
{"model": "qnguyen3/nanoLLaVA", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 249, "generation_tokens": 84, "generation_tps": 102.27234852149978, "peak_memory_gb": 4.468364202, "active_memory_gb": 1.966122692450881, "cache_memory_gb": 1.382916759699583}}
