{"model": "HuggingFaceTB/SmolVLM-Instruct", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 1404, "generation_tokens": 24, "generation_tps": 122.09269441038653, "peak_memory_gb": 5.546142818, "active_memory_gb": 4.184753889217973, "cache_memory_gb": 0.11892952583730221}}
{"model": "Qwen/Qwen3-VL-2B-Instruct", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["verbose", "bullets(55)"], "metrics": {"prompt_tokens": 16481, "generation_tokens": 500, "generation_tps": 71.67889931424077, "peak_memory_gb": 12.585172296, "active_memory_gb": 3.9632408041507006, "cache_memory_gb": 0.7029540035873652}}
{"model": "meta-llama/Llama-3.2-11B-Vision-Instruct", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["repetitive(phrase: \"* scenic * scenic...\")", "bullets(96)"], "metrics": {"prompt_tokens": 214, "generation_tokens": 500, "generation_tps": 3.822438761037201, "peak_memory_gb": 25.21454055, "active_memory_gb": 19.875752087682486, "cache_memory_gb": 3.0100423116236925}}
{"model": "microsoft/Florence-2-large-ft", "success": false, "error_stage": "Model Error", "error_message": "Model loading failed: Missing 1 parameters: \nlanguage_model.lm_head.weight.", "quality_issues": [], "metrics": {}}
{"model": "microsoft/Phi-3.5-vision-instruct", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["lang_mixing", "hallucination"], "metrics": {"prompt_tokens": 1009, "generation_tokens": 500, "generation_tps": 10.82674289952802, "peak_memory_gb": 11.28999184, "active_memory_gb": 7.724683305248618, "cache_memory_gb": 1.3822032157331705}}
{"model": "mlx-community/Apriel-1.5-15b-Thinker-6bit-MLX", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["hallucination"], "metrics": {"prompt_tokens": 2975, "generation_tokens": 500, "generation_tps": 34.46465526681031, "peak_memory_gb": 14.652063954, "active_memory_gb": 11.69644170999527, "cache_memory_gb": 0.13340142462402582}}
{"model": "mlx-community/Idefics3-8B-Llama3-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["formatting"], "metrics": {"prompt_tokens": 2524, "generation_tokens": 342, "generation_tps": 29.464769553273726, "peak_memory_gb": 18.50768253, "active_memory_gb": 15.763950854539871, "cache_memory_gb": 0.06920353882014751}}
{"model": "mlx-community/InternVL3-14B-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["bullets(21)"], "metrics": {"prompt_tokens": 2010, "generation_tokens": 219, "generation_tps": 29.26107110550086, "peak_memory_gb": 17.91061409, "active_memory_gb": 15.227007143199444, "cache_memory_gb": 0.0799959134310484}}
{"model": "mlx-community/Kimi-VL-A3B-Thinking-2506-bf16", "success": false, "error_stage": "Lib Version", "error_message": "Model loading failed: cannot import name '_validate_images_text_input_order' from 'transformers.processing_utils' (/opt/homebrew/Caskroom/miniconda/base/envs/mlx-vlm/lib/python3.13/site-packages/transformers/processing_utils.py)", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Kimi-VL-A3B-Thinking-8bit", "success": false, "error_stage": "Lib Version", "error_message": "Model loading failed: cannot import name '_validate_images_text_input_order' from 'transformers.processing_utils' (/opt/homebrew/Caskroom/miniconda/base/envs/mlx-vlm/lib/python3.13/site-packages/transformers/processing_utils.py)", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/LFM2-VL-1.6B-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 2007, "generation_tokens": 19, "generation_tps": 308.9150459798307, "peak_memory_gb": 21.206090252, "active_memory_gb": 18.521988486871123, "cache_memory_gb": 0.06962861586362123}}
{"model": "mlx-community/Llama-3.2-11B-Vision-Instruct-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 213, "generation_tokens": 325, "generation_tps": 8.640409709461299, "peak_memory_gb": 17.830628526, "active_memory_gb": 10.56965408846736, "cache_memory_gb": 0.4523743558675051}}
{"model": "mlx-community/Ministral-3-3B-Instruct-2512-4bit", "success": false, "error_stage": "Error", "error_message": "Model loading failed: Tokenizer class TokenizersBackend does not exist or is not currently imported.", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Molmo-7B-D-0924-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 1411, "generation_tokens": 238, "generation_tps": 42.00303811412677, "peak_memory_gb": 41.140939058, "active_memory_gb": 8.418875761330128, "cache_memory_gb": 0.05101248063147068}}
{"model": "mlx-community/Molmo-7B-D-0924-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["repetitive(phrase: \"uk", "uk", "uk", "uk", "...\")"], "metrics": {"prompt_tokens": 1411, "generation_tokens": 500, "generation_tps": 27.429567920763656, "peak_memory_gb": 47.732549844, "active_memory_gb": 14.94327456690371, "cache_memory_gb": 0.03448936529457569}}
{"model": "mlx-community/Phi-3.5-vision-instruct-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 1009, "generation_tokens": 156, "generation_tps": 10.82373494217055, "peak_memory_gb": 11.081524148, "active_memory_gb": 7.7267080042511225, "cache_memory_gb": 1.3704959098249674}}
{"model": "mlx-community/Qwen2-VL-2B-Instruct-4bit", "success": false, "error_stage": "OOM", "error_message": "Model runtime error during generation for mlx-community/Qwen2-VL-2B-Instruct-4bit: [metal::malloc] Attempting to allocate 135383101952 bytes which is greater than the maximum allowed buffer size of 86586540032 bytes.", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Qwen3-VL-2B-Thinking-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 16483, "generation_tokens": 500, "generation_tps": 72.5310972092728, "peak_memory_gb": 12.588973424, "active_memory_gb": 3.966124752536416, "cache_memory_gb": 0.7029539663344622}}
{"model": "mlx-community/SmolVLM-Instruct-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 1404, "generation_tokens": 15, "generation_tps": 126.13614510840036, "peak_memory_gb": 5.54966544, "active_memory_gb": 4.188034581020474, "cache_memory_gb": 0.11892952583730221}}
{"model": "mlx-community/SmolVLM2-2.2B-Instruct-mlx", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 1304, "generation_tokens": 24, "generation_tps": 123.20997913093572, "peak_memory_gb": 5.56519186, "active_memory_gb": 4.189087437465787, "cache_memory_gb": 0.11892792768776417}}
{"model": "mlx-community/deepseek-vl2-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 2015, "generation_tokens": 70, "generation_tps": 65.53882665762666, "peak_memory_gb": 31.818295508, "active_memory_gb": 27.326093723997474, "cache_memory_gb": 0.14966695755720139}}
{"model": "mlx-community/gemma-3-12b-pt-8bit", "success": false, "error_stage": "Error", "error_message": "Cannot use apply_chat_template because this processor does not have a chat template.", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/gemma-3-27b-it-qat-4bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["bullets(37)"], "metrics": {"prompt_tokens": 477, "generation_tokens": 352, "generation_tps": 27.862931277974052, "peak_memory_gb": 18.111104138, "active_memory_gb": 15.028965309262276, "cache_memory_gb": 0.08272304385900497}}
{"model": "mlx-community/gemma-3-27b-it-qat-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["bullets(33)"], "metrics": {"prompt_tokens": 477, "generation_tokens": 319, "generation_tps": 15.34096982606317, "peak_memory_gb": 31.615632532, "active_memory_gb": 27.606037452816963, "cache_memory_gb": 0.04361610673367977}}
{"model": "mlx-community/gemma-3n-E4B-it-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 477, "generation_tokens": 296, "generation_tps": 41.596165732911615, "peak_memory_gb": 17.773076137, "active_memory_gb": 14.627011897973716, "cache_memory_gb": 0.014868028461933136}}
{"model": "mlx-community/llava-v1.6-mistral-7b-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 2295, "generation_tokens": 8, "generation_tps": 47.21795275468559, "peak_memory_gb": 12.27582902, "active_memory_gb": 7.494020603597164, "cache_memory_gb": 0.47430429980158806}}
{"model": "mlx-community/paligemma2-10b-ft-docci-448-6bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["repetitive(phrase: \"the building has a...\")", "context-ignored"], "metrics": {"prompt_tokens": 1233, "generation_tokens": 500, "generation_tps": 36.706667738969095, "peak_memory_gb": 11.194778418, "active_memory_gb": 7.325993707403541, "cache_memory_gb": 0.5196099635213614}}
{"model": "mlx-community/paligemma2-10b-ft-docci-448-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 1233, "generation_tokens": 149, "generation_tps": 4.692560162851497, "peak_memory_gb": 26.889152534, "active_memory_gb": 18.007631918415427, "cache_memory_gb": 4.776009891182184}}
{"model": "mlx-community/paligemma2-3b-ft-docci-448-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 1233, "generation_tokens": 346, "generation_tps": 17.582923637355595, "peak_memory_gb": 11.33538055, "active_memory_gb": 5.658517500385642, "cache_memory_gb": 3.0583776235580444}}
{"model": "mlx-community/paligemma2-3b-pt-896-4bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["repetitive(phrase: \"wisteria hotel", "oakham", "englan...\")"], "metrics": {"prompt_tokens": 4305, "generation_tokens": 500, "generation_tps": 107.421080620326, "peak_memory_gb": 8.71429153, "active_memory_gb": 1.6085016038268805, "cache_memory_gb": 0.5467085251584649}}
{"model": "mlx-community/pixtral-12b-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 2884, "generation_tokens": 229, "generation_tps": 34.146806440663696, "peak_memory_gb": 15.517532494, "active_memory_gb": 12.561493095010519, "cache_memory_gb": 0.08617244102060795}}
{"model": "mlx-community/pixtral-12b-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["verbose", "formatting", "bullets(25)"], "metrics": {"prompt_tokens": 2884, "generation_tokens": 362, "generation_tps": 19.150503966913497, "peak_memory_gb": 27.323990414, "active_memory_gb": 23.63412493094802, "cache_memory_gb": 0.0348172876983881}}
{"model": "prince-canuma/Florence-2-large-ft", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["lang_mixing", "formatting", "context-ignored"], "metrics": {"prompt_tokens": 207, "generation_tokens": 500, "generation_tps": 319.2597006244754, "peak_memory_gb": 5.150491628, "active_memory_gb": 3.0760166123509407, "cache_memory_gb": 0.1715792417526245}}
{"model": "qnguyen3/nanoLLaVA", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 227, "generation_tokens": 85, "generation_tps": 103.0557130667721, "peak_memory_gb": 4.488074898, "active_memory_gb": 1.9669619295746088, "cache_memory_gb": 1.3829195387661457}}
