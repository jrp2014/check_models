{"model": "HuggingFaceTB/SmolVLM-Instruct", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 1404, "generation_tokens": 15, "generation_tps": 125.59811709745203, "peak_memory_gb": 5.486390392, "active_memory_gb": 4.184753889217973, "cache_memory_gb": 0.11880745552480221}}
{"model": "Qwen/Qwen3-VL-2B-Instruct", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["verbose", "bullets(44)"], "metrics": {"prompt_tokens": 16481, "generation_tokens": 500, "generation_tps": 71.5522878875657, "peak_memory_gb": 12.382649672, "active_memory_gb": 3.9632408041507006, "cache_memory_gb": 0.7028968203812838}}
{"model": "meta-llama/Llama-3.2-11B-Vision-Instruct", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 214, "generation_tokens": 275, "generation_tps": 3.726499792425126, "peak_memory_gb": 25.14848231, "active_memory_gb": 19.875752087682486, "cache_memory_gb": 3.0080891866236925}}
{"model": "microsoft/Florence-2-large-ft", "success": false, "error_stage": "Model Error", "error_message": "Model loading failed: Missing 1 parameters: \nlanguage_model.lm_head.weight.", "quality_issues": [], "metrics": {}}
{"model": "microsoft/Phi-3.5-vision-instruct", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["lang_mixing"], "metrics": {"prompt_tokens": 1009, "generation_tokens": 500, "generation_tps": 10.991305433905335, "peak_memory_gb": 11.28999184, "active_memory_gb": 7.724683305248618, "cache_memory_gb": 1.3822032157331705}}
{"model": "mlx-community/Apriel-1.5-15b-Thinker-6bit-MLX", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 2975, "generation_tokens": 500, "generation_tps": 34.373560438082826, "peak_memory_gb": 14.597226706, "active_memory_gb": 11.69644170999527, "cache_memory_gb": 0.13312843535095453}}
{"model": "mlx-community/Idefics3-8B-Llama3-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["formatting"], "metrics": {"prompt_tokens": 2524, "generation_tokens": 356, "generation_tps": 29.126610362888297, "peak_memory_gb": 18.47666765, "active_memory_gb": 15.763950854539871, "cache_memory_gb": 0.06911198608577251}}
{"model": "mlx-community/InternVL3-14B-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 2010, "generation_tokens": 206, "generation_tps": 28.95004357774018, "peak_memory_gb": 17.840654426, "active_memory_gb": 15.226991884410381, "cache_memory_gb": 0.0797975491732359}}
{"model": "mlx-community/Kimi-VL-A3B-Thinking-2506-bf16", "success": false, "error_stage": "Lib Version", "error_message": "Model loading failed: cannot import name '_validate_images_text_input_order' from 'transformers.processing_utils' (/opt/homebrew/Caskroom/miniconda/base/envs/mlx-vlm/lib/python3.13/site-packages/transformers/processing_utils.py)", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Kimi-VL-A3B-Thinking-8bit", "success": false, "error_stage": "Lib Version", "error_message": "Model loading failed: cannot import name '_validate_images_text_input_order' from 'transformers.processing_utils' (/opt/homebrew/Caskroom/miniconda/base/envs/mlx-vlm/lib/python3.13/site-packages/transformers/processing_utils.py)", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/LFM2-VL-1.6B-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 2007, "generation_tokens": 26, "generation_tps": 307.8116077399163, "peak_memory_gb": 21.132214836, "active_memory_gb": 18.52197322808206, "cache_memory_gb": 0.06939500384032726}}
{"model": "mlx-community/Llama-3.2-11B-Vision-Instruct-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 213, "generation_tokens": 121, "generation_tps": 8.46812852613176, "peak_memory_gb": 17.830612142, "active_memory_gb": 10.569638829678297, "cache_memory_gb": 0.4426244664937258}}
{"model": "mlx-community/Ministral-3-3B-Instruct-2512-4bit", "success": false, "error_stage": "Error", "error_message": "Model loading failed: Tokenizer class TokenizersBackend does not exist or is not currently imported.", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Molmo-7B-D-0924-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["repetitive(phrase: \"uk", "uk", "uk", "uk", "...\")"], "metrics": {"prompt_tokens": 1411, "generation_tokens": 500, "generation_tps": 44.074151378842906, "peak_memory_gb": 41.140922674, "active_memory_gb": 8.418860502541065, "cache_memory_gb": 0.05537649430334568}}
{"model": "mlx-community/Molmo-7B-D-0924-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 1411, "generation_tokens": 211, "generation_tps": 27.96303456833604, "peak_memory_gb": 47.73253346, "active_memory_gb": 14.943259308114648, "cache_memory_gb": 0.03265860863029957}}
{"model": "mlx-community/Phi-3.5-vision-instruct-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 1009, "generation_tokens": 252, "generation_tps": 10.97378967282886, "peak_memory_gb": 11.08151186, "active_memory_gb": 7.72669274546206, "cache_memory_gb": 1.3704806510359049}}
{"model": "mlx-community/Qwen2-VL-2B-Instruct-4bit", "success": false, "error_stage": "OOM", "error_message": "Model runtime error during generation for mlx-community/Qwen2-VL-2B-Instruct-4bit: [metal::malloc] Attempting to allocate 135383101952 bytes which is greater than the maximum allowed buffer size of 86586540032 bytes.", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/Qwen3-VL-2B-Thinking-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 16483, "generation_tokens": 500, "generation_tps": 71.37667082082885, "peak_memory_gb": 12.386401648, "active_memory_gb": 3.9661094937473536, "cache_memory_gb": 0.7028967831283808}}
{"model": "mlx-community/SmolVLM-Instruct-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 1404, "generation_tokens": 96, "generation_tps": 115.60050962000214, "peak_memory_gb": 5.489896624, "active_memory_gb": 4.188019322231412, "cache_memory_gb": 0.11880745552480221}}
{"model": "mlx-community/SmolVLM2-2.2B-Instruct-mlx", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 1304, "generation_tokens": 14, "generation_tps": 125.92792238382933, "peak_memory_gb": 5.491021136, "active_memory_gb": 4.189072178676724, "cache_memory_gb": 0.11880585737526417}}
{"model": "mlx-community/deepseek-vl2-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 2015, "generation_tokens": 102, "generation_tps": 62.98374044611374, "peak_memory_gb": 31.830947542, "active_memory_gb": 27.32607846520841, "cache_memory_gb": 0.14949148148298264}}
{"model": "mlx-community/gemma-3-12b-pt-8bit", "success": false, "error_stage": "Error", "error_message": "Cannot use apply_chat_template because this processor does not have a chat template.", "quality_issues": [], "metrics": {}}
{"model": "mlx-community/gemma-3-27b-it-qat-4bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["bullets(32)"], "metrics": {"prompt_tokens": 477, "generation_tokens": 308, "generation_tps": 26.86607067408986, "peak_memory_gb": 18.678760698, "active_memory_gb": 15.028950050473213, "cache_memory_gb": 0.08231105655431747}}
{"model": "mlx-community/gemma-3-27b-it-qat-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["bullets(41)"], "metrics": {"prompt_tokens": 477, "generation_tokens": 368, "generation_tps": 14.822756088468553, "peak_memory_gb": 32.183289092, "active_memory_gb": 27.6060221940279, "cache_memory_gb": 0.04338722489774227}}
{"model": "mlx-community/gemma-3n-E4B-it-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["verbose"], "metrics": {"prompt_tokens": 477, "generation_tokens": 311, "generation_tps": 41.388797319587674, "peak_memory_gb": 17.823221137, "active_memory_gb": 14.626996639184654, "cache_memory_gb": 0.014206655323505402}}
{"model": "mlx-community/llava-v1.6-mistral-7b-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 2295, "generation_tokens": 8, "generation_tps": 47.40953778131743, "peak_memory_gb": 11.931224348, "active_memory_gb": 7.494005344808102, "cache_memory_gb": 0.47387705370783806}}
{"model": "mlx-community/paligemma2-10b-ft-docci-448-6bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 1233, "generation_tokens": 148, "generation_tps": 40.70821288027127, "peak_memory_gb": 11.23176843, "active_memory_gb": 7.325978448614478, "cache_memory_gb": 0.48774961195886135}}
{"model": "mlx-community/paligemma2-10b-ft-docci-448-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["context-ignored"], "metrics": {"prompt_tokens": 1233, "generation_tokens": 122, "generation_tps": 4.886600681596038, "peak_memory_gb": 26.889005078, "active_memory_gb": 18.007616659626365, "cache_memory_gb": 4.775887820869684}}
{"model": "mlx-community/paligemma2-3b-ft-docci-448-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["repetitive(phrase: \"the windows have a...\")", "context-ignored"], "metrics": {"prompt_tokens": 1233, "generation_tokens": 500, "generation_tps": 17.17091124762843, "peak_memory_gb": 11.335167046, "active_memory_gb": 5.6585022415965796, "cache_memory_gb": 3.058240294456482}}
{"model": "mlx-community/paligemma2-3b-pt-896-4bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["repetitive(phrase: \"wisteria hotel wisteria hotel...\")", "context-ignored"], "metrics": {"prompt_tokens": 4305, "generation_tokens": 500, "generation_tps": 106.71411698793352, "peak_memory_gb": 8.555563338, "active_memory_gb": 1.608486345037818, "cache_memory_gb": 0.5462500024586916}}
{"model": "mlx-community/pixtral-12b-8bit", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 2884, "generation_tokens": 224, "generation_tps": 33.270866088580036, "peak_memory_gb": 15.411216734, "active_memory_gb": 12.561477836221457, "cache_memory_gb": 0.08595881797373295}}
{"model": "mlx-community/pixtral-12b-bf16", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 2884, "generation_tokens": 221, "generation_tps": 18.744634706192087, "peak_memory_gb": 27.270824334, "active_memory_gb": 23.634109672158957, "cache_memory_gb": 0.0347257349640131}}
{"model": "prince-canuma/Florence-2-large-ft", "success": true, "error_stage": null, "error_message": null, "quality_issues": ["lang_mixing", "formatting", "context-ignored"], "metrics": {"prompt_tokens": 207, "generation_tokens": 500, "generation_tps": 322.20382258261014, "peak_memory_gb": 5.136254052, "active_memory_gb": 3.076001353561878, "cache_memory_gb": 0.17139611020684242}}
{"model": "qnguyen3/nanoLLaVA", "success": true, "error_stage": null, "error_message": null, "quality_issues": [], "metrics": {"prompt_tokens": 227, "generation_tokens": 117, "generation_tps": 100.52876048641097, "peak_memory_gb": 4.455692118, "active_memory_gb": 1.9669466707855463, "cache_memory_gb": 1.3827702887356281}}
